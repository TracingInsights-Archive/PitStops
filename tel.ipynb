{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, lxml, pandas\n",
      "Successfully installed lxml-5.4.0 numpy-2.2.5 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install requests pandas lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.8.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "astroid                   3.3.8\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.4\n",
      "attrs                     25.1.0\n",
      "autopep8                  2.3.2\n",
      "babel                     2.17.0\n",
      "bandit                    1.8.2\n",
      "beautifulsoup4            4.13.3\n",
      "bleach                    6.2.0\n",
      "certifi                   2025.1.31\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.1\n",
      "comm                      0.2.2\n",
      "cryptography              44.0.0\n",
      "debugpy                   1.8.12\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.9\n",
      "distlib                   0.3.9\n",
      "docutils                  0.21.2\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.17.0\n",
      "flake8                    7.1.1\n",
      "fqdn                      1.5.1\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.1\n",
      "id                        1.5.0\n",
      "idna                      3.10\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.32.0\n",
      "isoduration               20.11.0\n",
      "isort                     6.0.0\n",
      "jaraco.classes            3.4.0\n",
      "jaraco.context            6.0.1\n",
      "jaraco.functools          4.1.0\n",
      "jedi                      0.19.2\n",
      "jeepney                   0.8.0\n",
      "Jinja2                    3.1.5\n",
      "json5                     0.10.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.15.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.5\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "keyring                   25.6.0\n",
      "lxml                      5.4.0\n",
      "markdown-it-py            3.0.0\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib-inline         0.1.7\n",
      "mccabe                    0.7.0\n",
      "mdurl                     0.1.2\n",
      "mistune                   3.1.1\n",
      "more-itertools            10.6.0\n",
      "mypy                      1.15.0\n",
      "mypy-extensions           1.0.0\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "nh3                       0.2.20\n",
      "notebook                  7.3.2\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.2.5\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pbr                       6.1.1\n",
      "pep8                      1.7.1\n",
      "pexpect                   4.9.0\n",
      "pip                       25.0.1\n",
      "pipenv                    2024.4.1\n",
      "platformdirs              4.3.6\n",
      "prometheus_client         0.21.1\n",
      "prompt_toolkit            3.0.50\n",
      "psutil                    6.1.1\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pycodestyle               2.12.1\n",
      "pycparser                 2.22\n",
      "pydocstyle                6.3.0\n",
      "pyflakes                  3.2.0\n",
      "Pygments                  2.19.1\n",
      "pylama                    8.4.1\n",
      "pylint                    3.3.4\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.2.1\n",
      "pytoolconfig              1.3.1\n",
      "pytz                      2025.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.1\n",
      "readme_renderer           44.0\n",
      "referencing               0.36.2\n",
      "requests                  2.32.3\n",
      "requests-toolbelt         1.0.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986                   2.0.0\n",
      "rfc3986-validator         0.1.1\n",
      "rich                      13.9.4\n",
      "rope                      1.13.0\n",
      "rpds-py                   0.22.3\n",
      "SecretStorage             3.3.3\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.8.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "snowballstemmer           2.2.0\n",
      "soupsieve                 2.6\n",
      "stack-data                0.6.3\n",
      "stevedore                 5.4.0\n",
      "terminado                 0.18.1\n",
      "tinycss2                  1.4.0\n",
      "tomlkit                   0.13.2\n",
      "tornado                   6.4.2\n",
      "traitlets                 5.14.3\n",
      "twine                     6.1.0\n",
      "types-python-dateutil     2.9.0.20241206\n",
      "typing_extensions         4.12.2\n",
      "tzdata                    2025.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.3.0\n",
      "virtualenv                20.29.1\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wheel                     0.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6367\n",
      "Successfully fetched data (Status Code: 200)\n",
      "\n",
      "Successfully extracted events data:\n",
      "[\n",
      "    {\n",
      "        \"id\": 1086,\n",
      "        \"title\": \"FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Albert Park Grand Prix Circuit\",\n",
      "        \"abbr\": \"AUS\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-03-15 23:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1087,\n",
      "        \"title\": \"FORMULA 1 HEINEKEN CHINESE GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Shanghai International Circuit\",\n",
      "        \"abbr\": \"CHI\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-03-23 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1088,\n",
      "        \"title\": \"FORMULA 1 LENOVO JAPANESE GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Suzuka Circuit\",\n",
      "        \"abbr\": \"JAP\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-04-05 22:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1089,\n",
      "        \"title\": \"FORMULA 1 GULF AIR BAHRAIN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Bahrain International Circuit\",\n",
      "        \"abbr\": \"BAH\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-04-13 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1090,\n",
      "        \"title\": \"FORMULA 1 STC SAUDI ARABIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Jeddah Corniche Circuit\",\n",
      "        \"abbr\": \"SAU\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-04-20 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1091,\n",
      "        \"title\": \"FORMULA 1 CRYPTO.COM MIAMI GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Miami International Autodrome\",\n",
      "        \"abbr\": \"MIA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-05-04 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1092,\n",
      "        \"title\": \"FORMULA 1 AWS GRAN PREMIO DEL MADE IN ITALY E DELL'EMILIA-ROMAGNA 2025\",\n",
      "        \"short_title\": \"Autodromo Internazionale Enzo e Dino Ferrari\",\n",
      "        \"abbr\": \"ITA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-05-18 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1093,\n",
      "        \"title\": \"FORMULA 1 TAG HEUER GRAND PRIX DE MONACO 2025\",\n",
      "        \"short_title\": \"Circuit de Monaco\",\n",
      "        \"abbr\": \"MON\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-05-24 22:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1094,\n",
      "        \"title\": \"FORMULA 1 ARAMCO GRAN PREMIO DE ESPA\\u00d1A 2025\",\n",
      "        \"short_title\": \"Circuit de Barcelona-Catalunya\",\n",
      "        \"abbr\": \"ESP\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-06-01 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1095,\n",
      "        \"title\": \"FORMULA 1 PIRELLI GRAND PRIX DU CANADA 2025\",\n",
      "        \"short_title\": \"Circuit Gilles-Villeneuve\",\n",
      "        \"abbr\": \"CAN\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-06-15 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1096,\n",
      "        \"title\": \"FORMULA 1 MSC CRUISES AUSTRIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Red Bull Ring\",\n",
      "        \"abbr\": \"AUT\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-06-29 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1097,\n",
      "        \"title\": \"FORMULA 1 QATAR AIRWAYS BRITISH GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Silverstone Circuit\",\n",
      "        \"abbr\": \"GBR\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-07-06 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1098,\n",
      "        \"title\": \"FORMULA 1 MO\\u00cbT & CHANDON BELGIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Circuit de Spa-Francorchamps\",\n",
      "        \"abbr\": \"BEL\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-07-27 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1099,\n",
      "        \"title\": \"FORMULA 1 LENOVO HUNGARIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Hungaroring\",\n",
      "        \"abbr\": \"HUN\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-08-03 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1100,\n",
      "        \"title\": \"FORMULA 1 HEINEKEN DUTCH GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Circuit Zandvoort\",\n",
      "        \"abbr\": \"NED\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-08-31 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1101,\n",
      "        \"title\": \"FORMULA 1 PIRELLI GRAN PREMIO D\\u2019ITALIA 2025\",\n",
      "        \"short_title\": \"Autodromo Nazionale Monza\",\n",
      "        \"abbr\": \"ITA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-09-07 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1102,\n",
      "        \"title\": \"FORMULA 1 QATAR AIRWAYS AZERBAIJAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Baku City Circuit\",\n",
      "        \"abbr\": \"AZE\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-09-20 22:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1103,\n",
      "        \"title\": \"FORMULA 1 SINGAPORE AIRLINES SINGAPORE GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Marina Bay Street Circuit\",\n",
      "        \"abbr\": \"SIN\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-10-05 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1104,\n",
      "        \"title\": \"FORMULA 1 MSC CRUISES UNITED STATES GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Circuit of The Americas\",\n",
      "        \"abbr\": \"USA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-10-19 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1105,\n",
      "        \"title\": \"FORMULA 1 GRAN PREMIO DE LA CIUDAD DE M\\u00c9XICO 2025\",\n",
      "        \"short_title\": \"Aut\\u00f3dromo Hermanos Rodr\\u00edguez\",\n",
      "        \"abbr\": \"MEX\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-10-26 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1106,\n",
      "        \"title\": \"FORMULA 1 MSC CRUISES GRANDE PR\\u00caMIO DE S\\u00c3O PAULO 2025\",\n",
      "        \"short_title\": \"Aut\\u00f3dromo Jos\\u00e9 Carlos Pace\",\n",
      "        \"abbr\": \"BRA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-11-09 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1107,\n",
      "        \"title\": \"FORMULA 1 HEINEKEN LAS VEGAS GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Las Vegas Strip Circuit\",\n",
      "        \"abbr\": \"VEG\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-11-22 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1108,\n",
      "        \"title\": \"FORMULA 1 QATAR AIRWAYS QATAR GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Lusail International Circuit\",\n",
      "        \"abbr\": \"QAT\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-11-30 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1109,\n",
      "        \"title\": \"FORMULA 1 ETIHAD AIRWAYS ABU DHABI GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Yas Marina Circuit\",\n",
      "        \"abbr\": \"UAE\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-12-07 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL to fetch data from\n",
    "url = \"https://inmotion.dhl/api/f1-award-element-data/6367\"\n",
    "\n",
    "print(f\"Attempting to fetch data from: {url}\")\n",
    "\n",
    "try:\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, timeout=10) # Added a timeout\n",
    "\n",
    "    # Check for HTTP errors (like 404 Not Found, 500 Internal Server Error)\n",
    "    response.raise_for_status()\n",
    "    print(f\"Successfully fetched data (Status Code: {response.status_code})\")\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        parsed_data = response.json()\n",
    "\n",
    "        # Navigate through the dictionary to extract the 'events' list\n",
    "        # Path: root -> 'data' -> 'chart' -> 'events'\n",
    "        data_section = parsed_data.get('data')\n",
    "        chart_section = data_section.get('chart') if data_section else None\n",
    "        events_data = chart_section.get('events') if chart_section else None\n",
    "\n",
    "        # Check if the data was found and print it\n",
    "        if events_data is not None:\n",
    "            print(\"\\nSuccessfully extracted events data:\")\n",
    "            # Pretty print the extracted list\n",
    "            print(json.dumps(events_data, indent=4))\n",
    "        else:\n",
    "            print(\"\\nError: Could not find the 'events' data at the expected path ('data' -> 'chart' -> 'events') in the JSON response.\")\n",
    "            # Optional: Print the structure if keys are missing\n",
    "            # print(\"\\nReceived JSON structure:\")\n",
    "            # print(json.dumps(parsed_data, indent=4))\n",
    "\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\nError: Failed to decode JSON from the response.\")\n",
    "        print(\"Response text was:\")\n",
    "        print(response.text[:500] + \"...\" if len(response.text) > 500 else response.text) # Print beginning of text\n",
    "\n",
    "except requests.exceptions.Timeout:\n",
    "    print(f\"\\nError: The request to {url} timed out.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\nError during request to {url}: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 events to process.\n",
      "\n",
      "Attempting to fetch data for Event ID: 1086 (FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1086\n",
      "Successfully fetched and parsed data for Event ID: 1086\n",
      "\n",
      "Attempting to fetch data for Event ID: 1087 (FORMULA 1 HEINEKEN CHINESE GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1087\n",
      "Successfully fetched and parsed data for Event ID: 1087\n",
      "\n",
      "Attempting to fetch data for Event ID: 1088 (FORMULA 1 LENOVO JAPANESE GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1088\n",
      "Successfully fetched and parsed data for Event ID: 1088\n",
      "\n",
      "Attempting to fetch data for Event ID: 1089 (FORMULA 1 GULF AIR BAHRAIN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1089\n",
      "Successfully fetched and parsed data for Event ID: 1089\n",
      "\n",
      "Attempting to fetch data for Event ID: 1090 (FORMULA 1 STC SAUDI ARABIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1090\n",
      "Successfully fetched and parsed data for Event ID: 1090\n",
      "\n",
      "Attempting to fetch data for Event ID: 1091 (FORMULA 1 CRYPTO.COM MIAMI GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1091\n",
      "Successfully fetched and parsed data for Event ID: 1091\n",
      "\n",
      "Attempting to fetch data for Event ID: 1092 (FORMULA 1 AWS GRAN PREMIO DEL MADE IN ITALY E DELL'EMILIA-ROMAGNA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1092\n",
      "Successfully fetched and parsed data for Event ID: 1092\n",
      "\n",
      "Attempting to fetch data for Event ID: 1093 (FORMULA 1 TAG HEUER GRAND PRIX DE MONACO 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1093\n",
      "Successfully fetched and parsed data for Event ID: 1093\n",
      "\n",
      "Attempting to fetch data for Event ID: 1094 (FORMULA 1 ARAMCO GRAN PREMIO DE ESPAÑA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1094\n",
      "Successfully fetched and parsed data for Event ID: 1094\n",
      "\n",
      "Attempting to fetch data for Event ID: 1095 (FORMULA 1 PIRELLI GRAND PRIX DU CANADA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1095\n",
      "Successfully fetched and parsed data for Event ID: 1095\n",
      "\n",
      "Attempting to fetch data for Event ID: 1096 (FORMULA 1 MSC CRUISES AUSTRIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1096\n",
      "Successfully fetched and parsed data for Event ID: 1096\n",
      "\n",
      "Attempting to fetch data for Event ID: 1097 (FORMULA 1 QATAR AIRWAYS BRITISH GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1097\n",
      "Successfully fetched and parsed data for Event ID: 1097\n",
      "\n",
      "Attempting to fetch data for Event ID: 1098 (FORMULA 1 MOËT & CHANDON BELGIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1098\n",
      "Successfully fetched and parsed data for Event ID: 1098\n",
      "\n",
      "Attempting to fetch data for Event ID: 1099 (FORMULA 1 LENOVO HUNGARIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1099\n",
      "Successfully fetched and parsed data for Event ID: 1099\n",
      "\n",
      "Attempting to fetch data for Event ID: 1100 (FORMULA 1 HEINEKEN DUTCH GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1100\n",
      "Successfully fetched and parsed data for Event ID: 1100\n",
      "\n",
      "Attempting to fetch data for Event ID: 1101 (FORMULA 1 PIRELLI GRAN PREMIO D’ITALIA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1101\n",
      "Successfully fetched and parsed data for Event ID: 1101\n",
      "\n",
      "Attempting to fetch data for Event ID: 1102 (FORMULA 1 QATAR AIRWAYS AZERBAIJAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1102\n",
      "Successfully fetched and parsed data for Event ID: 1102\n",
      "\n",
      "Attempting to fetch data for Event ID: 1103 (FORMULA 1 SINGAPORE AIRLINES SINGAPORE GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1103\n",
      "Successfully fetched and parsed data for Event ID: 1103\n",
      "\n",
      "Attempting to fetch data for Event ID: 1104 (FORMULA 1 MSC CRUISES UNITED STATES GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1104\n",
      "Successfully fetched and parsed data for Event ID: 1104\n",
      "\n",
      "Attempting to fetch data for Event ID: 1105 (FORMULA 1 GRAN PREMIO DE LA CIUDAD DE MÉXICO 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1105\n",
      "Successfully fetched and parsed data for Event ID: 1105\n",
      "\n",
      "Attempting to fetch data for Event ID: 1106 (FORMULA 1 MSC CRUISES GRANDE PRÊMIO DE SÃO PAULO 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1106\n",
      "Successfully fetched and parsed data for Event ID: 1106\n",
      "\n",
      "Attempting to fetch data for Event ID: 1107 (FORMULA 1 HEINEKEN LAS VEGAS GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1107\n",
      "Successfully fetched and parsed data for Event ID: 1107\n",
      "\n",
      "Attempting to fetch data for Event ID: 1108 (FORMULA 1 QATAR AIRWAYS QATAR GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1108\n",
      "Successfully fetched and parsed data for Event ID: 1108\n",
      "\n",
      "Attempting to fetch data for Event ID: 1109 (FORMULA 1 ETIHAD AIRWAYS ABU DHABI GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1109\n",
      "Successfully fetched and parsed data for Event ID: 1109\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully fetched data for 24 events.\n",
      "Failed to fetch data for 0 events.\n",
      "\n",
      "--- Sample Data for Event ID 1086 ---\n",
      "{\n",
      "    \"data\": {\n",
      "        \"chart\": [\n",
      "            {\n",
      "                \"id\": 7240,\n",
      "                \"driverNr\": 16,\n",
      "                \"tla\": \"LEC\",\n",
      "                \"firstName\": \"Charles\",\n",
      "                \"lastName\": \"Leclerc\",\n",
      "                \"team\": \"Ferrari\",\n",
      "                \"duration\": 2.32,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:17:28.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 34,\n",
      "                \"points\": 25,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7241,\n",
      "                \"driverNr\": 44,\n",
      "                \"tla\": \"HAM\",\n",
      "                \"firstName\": \"Lewis\",\n",
      "                \"lastName\": \"Hamilton\",\n",
      "                \"team\": \"Ferrari\",\n",
      "                \"duration\": 2.38,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:42:38.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 47,\n",
      "                \"points\": 18,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7242,\n",
      "                \"driverNr\": 63,\n",
      "                \"tla\": \"RUS\",\n",
      "                \"firstName\": \"George\",\n",
      "                \"lastName\": \"Russell\",\n",
      "                \"team\": \"Mercedes\",\n",
      "                \"duration\": 2.43,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:17:17.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 34,\n",
      "                \"points\": 15,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7243,\n",
      "                \"driverNr\": 22,\n",
      "                \"tla\": \"TSU\",\n",
      "                \"firstName\": \"Yuki\",\n",
      "                \"lastName\": \"Tsunoda\",\n",
      "                \"team\": \"Racing Bulls\",\n",
      "                \"duration\": 2.47,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:42:42.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 47,\n",
      "                \"points\": 12,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7244,\n",
      "                \"driverNr\": 87,\n",
      "                \"tla\": \"BEA\",\n",
      "                \"firstName\": \"Oliver\",\n",
      "                \"lastName\": \"Bearman\",\n",
      "                \"team\": \"Haas\",\n",
      "                \"duration\": 2.49,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 15:29:03.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 4,\n",
      "                \"points\": 10,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7245,\n",
      "                \"driverNr\": 31,\n",
      "                \"tla\": \"OCO\",\n",
      "                \"firstName\": \"Esteban\",\n",
      "                \"lastName\": \"Ocon\",\n",
      "                \"team\": \"Haas\",\n",
      "                \"duration\": 2.54,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 15:28:55.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 4,\n",
      "                \"points\": 8,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7247,\n",
      "                \"driverNr\": 1,\n",
      "                \"tla\": \"VER\",\n",
      "                \"firstName\": \"Max\",\n",
      "                \"lastName\": \"Verstappen\",\n",
      "                \"team\": \"Red Bull\",\n",
      "                \"duration\": 2.56,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:40:15.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 46,\n",
      "                \"points\": 6,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7248,\n",
      "                \"driverNr\": 30,\n",
      "                \"tla\": \"LAW\",\n",
      "                \"firstName\": \"Liam\",\n",
      "                \"lastName\": \"Lawson\",\n",
      "                \"team\": \"Red Bull\",\n",
      "                \"duration\": 2.58,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 15:29:00.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 4,\n",
      "                \"points\": 4,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7251,\n",
      "                \"driverNr\": 12,\n",
      "                \"tla\": \"ANT\",\n",
      "                \"firstName\": \"Andrea Kimi\",\n",
      "                \"lastName\": \"Antonelli\",\n",
      "                \"team\": \"Mercedes\",\n",
      "                \"duration\": 2.73,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:15:54.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 33,\n",
      "                \"points\": 2,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7252,\n",
      "                \"driverNr\": 23,\n",
      "                \"tla\": \"ALB\",\n",
      "                \"firstName\": \"Alexander\",\n",
      "                \"lastName\": \"Albon\",\n",
      "                \"team\": \"Williams\",\n",
      "                \"duration\": 2.75,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:15:41.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 33,\n",
      "                \"points\": 1,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            }\n",
      "        ],\n",
      "        \"sort\": \"1\",\n",
      "        \"event_id\": 1086,\n",
      "        \"headline\": {\n",
      "            \"style\": {\n",
      "                \"size\": \"3\",\n",
      "                \"align\": \"c\"\n",
      "            },\n",
      "            \"headline\": {\n",
      "                \"style\": {\n",
      "                    \"color\": \"b\",\n",
      "                    \"uppercase\": false\n",
      "                }\n",
      "            },\n",
      "            \"headline_type\": \"1\"\n",
      "        },\n",
      "        \"list_item_title\": \"Australia\"\n",
      "    },\n",
      "    \"htmlList\": {\n",
      "        \"table\": \"\\n<table class=\\\"f1-award-table\\\">\\n  <tr>\\n    <th class=\\\"align-center\\\">Pos.</th>\\n    <th>Team</th>\\n    <th>Driver</th>\\n    <th>Time (sec)</th>\\n    <th>Lap</th>\\n    <th>Points</th>\\n  </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>1</strong></td>\\n      <td>Ferrari</td>\\n      <td>Leclerc</td>\\n      <td>2.32</td>\\n      <td>34</td>\\n      <td><strong>25</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>2</strong></td>\\n      <td>Ferrari</td>\\n      <td>Hamilton</td>\\n      <td>2.38</td>\\n      <td>47</td>\\n      <td><strong>18</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>3</strong></td>\\n      <td>Mercedes</td>\\n      <td>Russell</td>\\n      <td>2.43</td>\\n      <td>34</td>\\n      <td><strong>15</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>4</strong></td>\\n      <td>Racing Bulls</td>\\n      <td>Tsunoda</td>\\n      <td>2.47</td>\\n      <td>47</td>\\n      <td><strong>12</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>5</strong></td>\\n      <td>Haas</td>\\n      <td>Bearman</td>\\n      <td>2.49</td>\\n      <td>4</td>\\n      <td><strong>10</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>6</strong></td>\\n      <td>Haas</td>\\n      <td>Ocon</td>\\n      <td>2.54</td>\\n      <td>4</td>\\n      <td><strong>8</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>7</strong></td>\\n      <td>Ferrari</td>\\n      <td>Hamilton</td>\\n      <td>2.55</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>8</strong></td>\\n      <td>Red Bull</td>\\n      <td>Verstappen</td>\\n      <td>2.56</td>\\n      <td>46</td>\\n      <td><strong>6</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>9</strong></td>\\n      <td>Red Bull</td>\\n      <td>Lawson</td>\\n      <td>2.58</td>\\n      <td>4</td>\\n      <td><strong>4</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>10</strong></td>\\n      <td>Racing Bulls</td>\\n      <td>Tsunoda</td>\\n      <td>2.67</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>11</strong></td>\\n      <td>Haas</td>\\n      <td>Ocon</td>\\n      <td>2.69</td>\\n      <td>46</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>12</strong></td>\\n      <td>Mercedes</td>\\n      <td>Antonelli</td>\\n      <td>2.73</td>\\n      <td>33</td>\\n      <td><strong>2</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>13</strong></td>\\n      <td>Williams</td>\\n      <td>Albon</td>\\n      <td>2.75</td>\\n      <td>33</td>\\n      <td><strong>1</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>14</strong></td>\\n      <td>Mercedes</td>\\n      <td>Russell</td>\\n      <td>2.76</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>15</strong></td>\\n      <td>Red Bull</td>\\n      <td>Verstappen</td>\\n      <td>2.77</td>\\n      <td>34</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>16</strong></td>\\n      <td>McLaren</td>\\n      <td>Norris</td>\\n      <td>2.79</td>\\n      <td>34</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>17</strong></td>\\n      <td>Haas</td>\\n      <td>Bearman</td>\\n      <td>2.80</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>18</strong></td>\\n      <td>Haas</td>\\n      <td>Ocon</td>\\n      <td>2.82</td>\\n      <td>39</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>19</strong></td>\\n      <td>Aston Martin</td>\\n      <td>Stroll</td>\\n      <td>2.82</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>20</strong></td>\\n      <td>Aston Martin</td>\\n      <td>Stroll</td>\\n      <td>2.87</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>21</strong></td>\\n      <td>Red Bull</td>\\n      <td>Lawson</td>\\n      <td>2.94</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>22</strong></td>\\n      <td>Mercedes</td>\\n      <td>Antonelli</td>\\n      <td>3.05</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>23</strong></td>\\n      <td>McLaren</td>\\n      <td>Norris</td>\\n      <td>3.19</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>24</strong></td>\\n      <td>Ferrari</td>\\n      <td>Leclerc</td>\\n      <td>3.28</td>\\n      <td>47</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>25</strong></td>\\n      <td>Alpine</td>\\n      <td>Gasly</td>\\n      <td>3.32</td>\\n      <td>46</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>26</strong></td>\\n      <td>McLaren</td>\\n      <td>Piastri</td>\\n      <td>3.45</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>27</strong></td>\\n      <td>Williams</td>\\n      <td>Albon</td>\\n      <td>3.54</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>28</strong></td>\\n      <td>Alpine</td>\\n      <td>Gasly</td>\\n      <td>3.66</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>29</strong></td>\\n      <td>McLaren</td>\\n      <td>Piastri</td>\\n      <td>3.72</td>\\n      <td>34</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>30</strong></td>\\n      <td>Sauber</td>\\n      <td>Hulkenberg</td>\\n      <td>4.01</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>31</strong></td>\\n      <td>Haas</td>\\n      <td>Bearman</td>\\n      <td>4.95</td>\\n      <td>39</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>32</strong></td>\\n      <td>Sauber</td>\\n      <td>Hulkenberg</td>\\n      <td>5.13</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>33</strong></td>\\n      <td>Sauber</td>\\n      <td>Bortoleto</td>\\n      <td>5.59</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>34</strong></td>\\n      <td>Sauber</td>\\n      <td>Bortoleto</td>\\n      <td>9.33</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n  </table>\\n\\n\",\n",
      "        \"content\": \"<div class=\\\"f1-award-content\\\"><html-content-element\\n        class=\\\"html html--v3\\\"        id=\\\"element_event_html\\\"        \\n    ><div class=\\\"height-toggleable box-gradient\\\"><div class=\\\"running-text\\\"><h3>Ferrari perform DHL Fastest Pit Stop in Australia, but get the timing wrong</h3><p>Towards the end of last year, the Ferrari, Red Bull and McLaren mechanics were taking turns to chalk up the DHL Fastest Pit Stop. With a one-two in the first race of 2025, the Italian team have carried on where they left off. Unfortunately for them, though, it was only the stops themselves that they got right in Australia, not the timing. </p><p>The crew excelled during their second stop of the day, when they swapped intermediates for slicks on Charles Leclerc\\u2019s car on lap 34. Despite the difficulties caused by the wet conditions, they managed a superb tire change of 2.32 seconds. </p><p>Almost equally impressive was the second fastest stop of 2.38 seconds for Lewis Hamilton\\u2019s car on lap 47 in the rain. Yet the achievement brings only limited satisfaction, as they should actually have called the seven-time champion into the pits on lap 44, when another shower hit the track, and switched him back to intermediates. </p><p>However, Ferrari failed to make the right decision with both cars and stayed out far too long in the false hope that the rain would stop quickly. Ultimately, Leclerc and Hamilton had to settle for eighth and tenth place respectively. \\u201cIt was a tough race and there are things we have to review and work on,\\u201d said Leclerc afterwards. \\u201cWe weren\\u2019t the fastest out there, but in such weather conditions, there was a chance of scoring some big points which we didn\\u2019t capitalize on today.\\u201d </p></div></div><read-more-element\\n        class=\\\"height-toggle-new js-toggle-height state-closed\\\"\\n        data-target=\\\"#element_event_html .height-toggleable\\\"\\n        data-scroll-target=\\\"#element_event_html\\\"\\n        data-scroll-offset=\\\"-15\\\"\\n    ><div class=\\\"text\\\"><span class=\\\"open-text\\\">Read more</span><span class=\\\"close-text\\\">Close</span></div></read-more-element></html-content-element></div>\",\n",
      "        \"video\": \"<div class='f1-award-video'><full-player\\n  id=\\\"p192095696_player\\\"\\n  class=\\\"player player--video-js\\\" data-analytics=\\\"\\\"      data-aspectratio-desktop=\\\"16:9\\\"\\n            data-aspectratio-mobile=\\\"1:1\\\"\\n      ><div class=\\\"player__inner\\\"><video-js\\n      id=\\\"p192095696_player_video\\\"\\n      class=\\\"player__video\\\"\\n       preload=\\\"metadata\\\"  data-poster-desktop=\\\"https://images.inmotion.dhl/1920/q_0/2f7a66732f64686c6d656469612f766964656f2f726567697374657265642f323032352f30332f31353835353661386531313233613765613331326461643730653561343337392e6a7067.jpg\\\"\\n      data-poster-mobile=\\\"https://images.inmotion.dhl/1920/q_0/2f7a66732f64686c6d656469612f766964656f2f726567697374657265642f323032352f30332f35376633653438353537656631636230326164303631633063636136303665302e6a7067.jpg\\\"\\n      playsinline\\n    ><source\\n          type=\\\"video/mp4\\\"\\n          label=\\\"1080p\\\"\\n          data-src-desktop=\\\"https://media.inmotion.dhl/video/registered/2025/03/158556a8e1123a7ea312dad70e5a4379_1080p.mp4\\\"/><source\\n          type=\\\"video/mp4\\\"\\n          label=\\\"1080p\\\"\\n          data-src-mobile=\\\"https://media.inmotion.dhl/video/registered/2025/03/57f3e48557ef1cb02ad061c0cca606e0_1080p.mp4\\\"/></video-js></div></full-player></div>\",\n",
      "        \"header\": \"\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time # Import time for potential delays\n",
    "\n",
    "# --- 1. Sample events_data (Replace with your actual data) ---\n",
    "# Since the tool environment couldn't fetch the initial list,\n",
    "# we'll use the sample data from your first prompt here.\n",
    "\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "base_event_url = \"https://inmotion.dhl/api/f1-award-element-data/6365\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "}\n",
    "timeout_seconds = 15\n",
    "# Optional: Add a small delay between requests to avoid overwhelming the server\n",
    "delay_between_requests = 0.5 # seconds\n",
    "\n",
    "# --- 3. Data Fetching Loop ---\n",
    "all_event_specific_data = {} # Dictionary to store results {event_id: data}\n",
    "\n",
    "print(f\"Found {len(events_data)} events to process.\")\n",
    "\n",
    "for event in events_data:\n",
    "    event_id = event.get('id')\n",
    "    event_title = event.get('title', 'Unknown Title') # Get title for logging\n",
    "\n",
    "    if not event_id:\n",
    "        print(f\"Warning: Skipping event with missing ID: {event_title}\")\n",
    "        continue\n",
    "\n",
    "    # Construct the specific URL for this event\n",
    "    specific_url = f\"{base_event_url}?event={event_id}\"\n",
    "    print(f\"\\nAttempting to fetch data for Event ID: {event_id} ({event_title})\")\n",
    "    print(f\"URL: {specific_url}\")\n",
    "\n",
    "    try:\n",
    "        # Make the GET request for the specific event\n",
    "        response = requests.get(specific_url, headers=headers, timeout=timeout_seconds)\n",
    "        response.raise_for_status() # Check for HTTP errors\n",
    "\n",
    "        # Parse the JSON response\n",
    "        try:\n",
    "            event_specific_data = response.json()\n",
    "            all_event_specific_data[event_id] = event_specific_data # Store the data\n",
    "            print(f\"Successfully fetched and parsed data for Event ID: {event_id}\")\n",
    "            # Optional: Print a snippet of the fetched data\n",
    "            # print(json.dumps(event_specific_data, indent=2)[:200] + \"...\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Failed to decode JSON for Event ID: {event_id}\")\n",
    "            print(f\"Response text (first 500 chars): {response.text[:500]}\")\n",
    "            all_event_specific_data[event_id] = {\"error\": \"JSONDecodeError\", \"response_text\": response.text[:500]}\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Error: Request timed out for Event ID: {event_id} at {specific_url}\")\n",
    "        all_event_specific_data[event_id] = {\"error\": \"Timeout\"}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request for Event ID: {event_id} at {specific_url}: {e}\")\n",
    "        all_event_specific_data[event_id] = {\"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for Event ID: {event_id}: {e}\")\n",
    "        all_event_specific_data[event_id] = {\"error\": f\"Unexpected: {str(e)}\"}\n",
    "\n",
    "    # Optional delay\n",
    "    if delay_between_requests > 0:\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "# --- 4. Final Output ---\n",
    "print(\"\\n--- Processing Complete ---\")\n",
    "successful_fetches = sum(1 for data in all_event_specific_data.values() if isinstance(data, dict) and 'error' not in data)\n",
    "failed_fetches = len(events_data) - successful_fetches\n",
    "print(f\"Successfully fetched data for {successful_fetches} events.\")\n",
    "print(f\"Failed to fetch data for {failed_fetches} events.\")\n",
    "\n",
    "# Optional: Print all collected data (can be very large)\n",
    "# print(\"\\n--- Collected Data ---\")\n",
    "# print(json.dumps(all_event_specific_data, indent=4))\n",
    "\n",
    "# Example: Print data for a specific event ID if it exists and wasn't an error\n",
    "target_id_to_show = 1086\n",
    "if target_id_to_show in all_event_specific_data and isinstance(all_event_specific_data[target_id_to_show], dict) and 'error' not in all_event_specific_data[target_id_to_show]:\n",
    "     print(f\"\\n--- Sample Data for Event ID {target_id_to_show} ---\")\n",
    "     print(json.dumps(all_event_specific_data[target_id_to_show], indent=4))\n",
    "elif target_id_to_show in all_event_specific_data:\n",
    "     print(f\"\\n--- Error Data for Event ID {target_id_to_show} ---\")\n",
    "     print(json.dumps(all_event_specific_data[target_id_to_show], indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "\n",
      "--- DataFrame for Event ID 1086 ---\n",
      "    Pos.          Team      Driver  Time (sec)  Lap  Points\n",
      "0      1       Ferrari     Leclerc        2.32   34    25.0\n",
      "1      2       Ferrari    Hamilton        2.38   47    18.0\n",
      "2      3      Mercedes     Russell        2.43   34    15.0\n",
      "3      4  Racing Bulls     Tsunoda        2.47   47    12.0\n",
      "4      5          Haas     Bearman        2.49    4    10.0\n",
      "5      6          Haas        Ocon        2.54    4     8.0\n",
      "6      7       Ferrari    Hamilton        2.55   33     NaN\n",
      "7      8      Red Bull  Verstappen        2.56   46     6.0\n",
      "8      9      Red Bull      Lawson        2.58    4     4.0\n",
      "9     10  Racing Bulls     Tsunoda        2.67   33     NaN\n",
      "10    11          Haas        Ocon        2.69   46     NaN\n",
      "11    12      Mercedes   Antonelli        2.73   33     2.0\n",
      "12    13      Williams       Albon        2.75   33     1.0\n",
      "13    14      Mercedes     Russell        2.76   44     NaN\n",
      "14    15      Red Bull  Verstappen        2.77   34     NaN\n",
      "15    16       McLaren      Norris        2.79   34     NaN\n",
      "16    17          Haas     Bearman        2.80   44     NaN\n",
      "17    18          Haas        Ocon        2.82   39     NaN\n",
      "18    19  Aston Martin      Stroll        2.82   44     NaN\n",
      "19    20  Aston Martin      Stroll        2.87   33     NaN\n",
      "20    21      Red Bull      Lawson        2.94   33     NaN\n",
      "21    22      Mercedes   Antonelli        3.05   44     NaN\n",
      "22    23       McLaren      Norris        3.19   44     NaN\n",
      "23    24       Ferrari     Leclerc        3.28   47     NaN\n",
      "24    25        Alpine       Gasly        3.32   46     NaN\n",
      "25    26       McLaren     Piastri        3.45   44     NaN\n",
      "26    27      Williams       Albon        3.54   44     NaN\n",
      "27    28        Alpine       Gasly        3.66   33     NaN\n",
      "28    29       McLaren     Piastri        3.72   34     NaN\n",
      "29    30        Sauber  Hulkenberg        4.01   44     NaN\n",
      "30    31          Haas     Bearman        4.95   39     NaN\n",
      "31    32        Sauber  Hulkenberg        5.13   33     NaN\n",
      "32    33        Sauber   Bortoleto        5.59   33     NaN\n",
      "33    34        Sauber   Bortoleto        9.33   44     NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io # Needed to treat the string as a file for read_html\n",
    "import json\n",
    "\n",
    "# --- Sample Data (as provided in the prompt) ---\n",
    "# In a real scenario, this would come from your 'all_event_specific_data[1086]'\n",
    "sample_event_data  = all_event_specific_data[target_id_to_show]\n",
    "\n",
    "def html_table_to_dataframe(event_json_data):\n",
    "    \"\"\"\n",
    "    Extracts an HTML table string from event JSON data and converts it to a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        event_json_data (dict): The JSON data dictionary for a specific event,\n",
    "                                expected to contain ['htmlList']['table'].\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame created from the HTML table, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    if not isinstance(event_json_data, dict):\n",
    "        print(\"Error: Input must be a dictionary.\")\n",
    "        return None\n",
    "\n",
    "    # Safely extract the HTML table string\n",
    "    html_table_str = event_json_data.get('htmlList', {}).get('table')\n",
    "\n",
    "    if not html_table_str:\n",
    "        print(\"Error: Could not find 'htmlList' -> 'table' in the provided JSON data or it's empty.\")\n",
    "        return None\n",
    "\n",
    "    if not isinstance(html_table_str, str):\n",
    "        print(\"Error: The value at ['htmlList']['table'] is not a string.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Found HTML table string. Attempting to parse...\")\n",
    "    try:\n",
    "        # pd.read_html returns a list of DataFrames. We expect only one table.\n",
    "        list_of_dfs = pd.read_html(io.StringIO(html_table_str))\n",
    "\n",
    "        if list_of_dfs:\n",
    "            print(\"Successfully parsed HTML table into DataFrame.\")\n",
    "            return list_of_dfs[0] # Return the first DataFrame found\n",
    "        else:\n",
    "            # This case is unlikely if the input contains a <table> tag,\n",
    "            # pd.read_html usually raises ValueError if no tables are found.\n",
    "            print(\"Warning: No tables found by pd.read_html, although HTML string was present.\")\n",
    "            return None\n",
    "\n",
    "    except ValueError as ve:\n",
    "        # This error often means no tables were found in the string\n",
    "        print(f\"Error parsing HTML with pandas (ValueError): {ve}\")\n",
    "        print(\"Check if the HTML string actually contains a <table> tag.\")\n",
    "        return None\n",
    "    except ImportError:\n",
    "        print(\"Error: The 'lxml' library might be required by pd.read_html. Please install it (`pip install lxml`).\")\n",
    "        # Note: The tool environment usually has common libraries like lxml.\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during HTML parsing: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assuming 'sample_event_data' holds the JSON for Event ID 1086\n",
    "event_dataframe = html_table_to_dataframe(sample_event_data)\n",
    "\n",
    "if event_dataframe is not None:\n",
    "    print(\"\\n--- DataFrame for Event ID 1086 ---\")\n",
    "    # Display the DataFrame. print() works, but display() might be nicer in some environments.\n",
    "    # Using print() for compatibility here.\n",
    "    print(event_dataframe.to_string()) # .to_string() ensures all rows/cols are printed\n",
    "\n",
    "    # You can also print specific info, e.g., the first 5 rows:\n",
    "    # print(\"\\n--- First 5 Rows ---\")\n",
    "    # print(event_dataframe.head())\n",
    "else:\n",
    "    print(\"\\nFailed to create DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos.</th>\n",
       "      <th>Team</th>\n",
       "      <th>Driver</th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>Lap</th>\n",
       "      <th>Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ferrari</td>\n",
       "      <td>Leclerc</td>\n",
       "      <td>2.32</td>\n",
       "      <td>34</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ferrari</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>2.38</td>\n",
       "      <td>47</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>Russell</td>\n",
       "      <td>2.43</td>\n",
       "      <td>34</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Racing Bulls</td>\n",
       "      <td>Tsunoda</td>\n",
       "      <td>2.47</td>\n",
       "      <td>47</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Haas</td>\n",
       "      <td>Bearman</td>\n",
       "      <td>2.49</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Haas</td>\n",
       "      <td>Ocon</td>\n",
       "      <td>2.54</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Ferrari</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>2.55</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Red Bull</td>\n",
       "      <td>Verstappen</td>\n",
       "      <td>2.56</td>\n",
       "      <td>46</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Red Bull</td>\n",
       "      <td>Lawson</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Racing Bulls</td>\n",
       "      <td>Tsunoda</td>\n",
       "      <td>2.67</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Haas</td>\n",
       "      <td>Ocon</td>\n",
       "      <td>2.69</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>Antonelli</td>\n",
       "      <td>2.73</td>\n",
       "      <td>33</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Williams</td>\n",
       "      <td>Albon</td>\n",
       "      <td>2.75</td>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>Russell</td>\n",
       "      <td>2.76</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Red Bull</td>\n",
       "      <td>Verstappen</td>\n",
       "      <td>2.77</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>McLaren</td>\n",
       "      <td>Norris</td>\n",
       "      <td>2.79</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Haas</td>\n",
       "      <td>Bearman</td>\n",
       "      <td>2.80</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Haas</td>\n",
       "      <td>Ocon</td>\n",
       "      <td>2.82</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Aston Martin</td>\n",
       "      <td>Stroll</td>\n",
       "      <td>2.82</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Aston Martin</td>\n",
       "      <td>Stroll</td>\n",
       "      <td>2.87</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Red Bull</td>\n",
       "      <td>Lawson</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>Antonelli</td>\n",
       "      <td>3.05</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>McLaren</td>\n",
       "      <td>Norris</td>\n",
       "      <td>3.19</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Ferrari</td>\n",
       "      <td>Leclerc</td>\n",
       "      <td>3.28</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Alpine</td>\n",
       "      <td>Gasly</td>\n",
       "      <td>3.32</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>McLaren</td>\n",
       "      <td>Piastri</td>\n",
       "      <td>3.45</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Williams</td>\n",
       "      <td>Albon</td>\n",
       "      <td>3.54</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Alpine</td>\n",
       "      <td>Gasly</td>\n",
       "      <td>3.66</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>McLaren</td>\n",
       "      <td>Piastri</td>\n",
       "      <td>3.72</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Sauber</td>\n",
       "      <td>Hulkenberg</td>\n",
       "      <td>4.01</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Haas</td>\n",
       "      <td>Bearman</td>\n",
       "      <td>4.95</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Sauber</td>\n",
       "      <td>Hulkenberg</td>\n",
       "      <td>5.13</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Sauber</td>\n",
       "      <td>Bortoleto</td>\n",
       "      <td>5.59</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Sauber</td>\n",
       "      <td>Bortoleto</td>\n",
       "      <td>9.33</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pos.          Team      Driver  Time (sec)  Lap  Points\n",
       "0      1       Ferrari     Leclerc        2.32   34    25.0\n",
       "1      2       Ferrari    Hamilton        2.38   47    18.0\n",
       "2      3      Mercedes     Russell        2.43   34    15.0\n",
       "3      4  Racing Bulls     Tsunoda        2.47   47    12.0\n",
       "4      5          Haas     Bearman        2.49    4    10.0\n",
       "5      6          Haas        Ocon        2.54    4     8.0\n",
       "6      7       Ferrari    Hamilton        2.55   33     NaN\n",
       "7      8      Red Bull  Verstappen        2.56   46     6.0\n",
       "8      9      Red Bull      Lawson        2.58    4     4.0\n",
       "9     10  Racing Bulls     Tsunoda        2.67   33     NaN\n",
       "10    11          Haas        Ocon        2.69   46     NaN\n",
       "11    12      Mercedes   Antonelli        2.73   33     2.0\n",
       "12    13      Williams       Albon        2.75   33     1.0\n",
       "13    14      Mercedes     Russell        2.76   44     NaN\n",
       "14    15      Red Bull  Verstappen        2.77   34     NaN\n",
       "15    16       McLaren      Norris        2.79   34     NaN\n",
       "16    17          Haas     Bearman        2.80   44     NaN\n",
       "17    18          Haas        Ocon        2.82   39     NaN\n",
       "18    19  Aston Martin      Stroll        2.82   44     NaN\n",
       "19    20  Aston Martin      Stroll        2.87   33     NaN\n",
       "20    21      Red Bull      Lawson        2.94   33     NaN\n",
       "21    22      Mercedes   Antonelli        3.05   44     NaN\n",
       "22    23       McLaren      Norris        3.19   44     NaN\n",
       "23    24       Ferrari     Leclerc        3.28   47     NaN\n",
       "24    25        Alpine       Gasly        3.32   46     NaN\n",
       "25    26       McLaren     Piastri        3.45   44     NaN\n",
       "26    27      Williams       Albon        3.54   44     NaN\n",
       "27    28        Alpine       Gasly        3.66   33     NaN\n",
       "28    29       McLaren     Piastri        3.72   34     NaN\n",
       "29    30        Sauber  Hulkenberg        4.01   44     NaN\n",
       "30    31          Haas     Bearman        4.95   39     NaN\n",
       "31    32        Sauber  Hulkenberg        5.13   33     NaN\n",
       "32    33        Sauber   Bortoleto        5.59   33     NaN\n",
       "33    34        Sauber   Bortoleto        9.33   44     NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6367\n",
      "Successfully fetched data (Status Code: 200)\n",
      "\n",
      "Successfully extracted events data:\n",
      "[\n",
      "    {\n",
      "        \"id\": 1086,\n",
      "        \"title\": \"FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Albert Park Grand Prix Circuit\",\n",
      "        \"abbr\": \"AUS\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-03-15 23:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1087,\n",
      "        \"title\": \"FORMULA 1 HEINEKEN CHINESE GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Shanghai International Circuit\",\n",
      "        \"abbr\": \"CHI\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-03-23 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1088,\n",
      "        \"title\": \"FORMULA 1 LENOVO JAPANESE GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Suzuka Circuit\",\n",
      "        \"abbr\": \"JAP\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-04-05 22:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1089,\n",
      "        \"title\": \"FORMULA 1 GULF AIR BAHRAIN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Bahrain International Circuit\",\n",
      "        \"abbr\": \"BAH\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-04-13 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1090,\n",
      "        \"title\": \"FORMULA 1 STC SAUDI ARABIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Jeddah Corniche Circuit\",\n",
      "        \"abbr\": \"SAU\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-04-20 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1091,\n",
      "        \"title\": \"FORMULA 1 CRYPTO.COM MIAMI GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Miami International Autodrome\",\n",
      "        \"abbr\": \"MIA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-05-04 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1092,\n",
      "        \"title\": \"FORMULA 1 AWS GRAN PREMIO DEL MADE IN ITALY E DELL'EMILIA-ROMAGNA 2025\",\n",
      "        \"short_title\": \"Autodromo Internazionale Enzo e Dino Ferrari\",\n",
      "        \"abbr\": \"ITA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-05-18 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1093,\n",
      "        \"title\": \"FORMULA 1 TAG HEUER GRAND PRIX DE MONACO 2025\",\n",
      "        \"short_title\": \"Circuit de Monaco\",\n",
      "        \"abbr\": \"MON\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-05-24 22:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1094,\n",
      "        \"title\": \"FORMULA 1 ARAMCO GRAN PREMIO DE ESPA\\u00d1A 2025\",\n",
      "        \"short_title\": \"Circuit de Barcelona-Catalunya\",\n",
      "        \"abbr\": \"ESP\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-06-01 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1095,\n",
      "        \"title\": \"FORMULA 1 PIRELLI GRAND PRIX DU CANADA 2025\",\n",
      "        \"short_title\": \"Circuit Gilles-Villeneuve\",\n",
      "        \"abbr\": \"CAN\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-06-15 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1096,\n",
      "        \"title\": \"FORMULA 1 MSC CRUISES AUSTRIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Red Bull Ring\",\n",
      "        \"abbr\": \"AUT\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-06-29 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1097,\n",
      "        \"title\": \"FORMULA 1 QATAR AIRWAYS BRITISH GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Silverstone Circuit\",\n",
      "        \"abbr\": \"GBR\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-07-06 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1098,\n",
      "        \"title\": \"FORMULA 1 MO\\u00cbT & CHANDON BELGIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Circuit de Spa-Francorchamps\",\n",
      "        \"abbr\": \"BEL\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-07-27 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1099,\n",
      "        \"title\": \"FORMULA 1 LENOVO HUNGARIAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Hungaroring\",\n",
      "        \"abbr\": \"HUN\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-08-03 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1100,\n",
      "        \"title\": \"FORMULA 1 HEINEKEN DUTCH GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Circuit Zandvoort\",\n",
      "        \"abbr\": \"NED\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-08-31 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1101,\n",
      "        \"title\": \"FORMULA 1 PIRELLI GRAN PREMIO D\\u2019ITALIA 2025\",\n",
      "        \"short_title\": \"Autodromo Nazionale Monza\",\n",
      "        \"abbr\": \"ITA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-09-07 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1102,\n",
      "        \"title\": \"FORMULA 1 QATAR AIRWAYS AZERBAIJAN GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Baku City Circuit\",\n",
      "        \"abbr\": \"AZE\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-09-20 22:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1103,\n",
      "        \"title\": \"FORMULA 1 SINGAPORE AIRLINES SINGAPORE GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Marina Bay Street Circuit\",\n",
      "        \"abbr\": \"SIN\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-10-05 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1104,\n",
      "        \"title\": \"FORMULA 1 MSC CRUISES UNITED STATES GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Circuit of The Americas\",\n",
      "        \"abbr\": \"USA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-10-19 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1105,\n",
      "        \"title\": \"FORMULA 1 GRAN PREMIO DE LA CIUDAD DE M\\u00c9XICO 2025\",\n",
      "        \"short_title\": \"Aut\\u00f3dromo Hermanos Rodr\\u00edguez\",\n",
      "        \"abbr\": \"MEX\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-10-26 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1106,\n",
      "        \"title\": \"FORMULA 1 MSC CRUISES GRANDE PR\\u00caMIO DE S\\u00c3O PAULO 2025\",\n",
      "        \"short_title\": \"Aut\\u00f3dromo Jos\\u00e9 Carlos Pace\",\n",
      "        \"abbr\": \"BRA\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-11-09 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1107,\n",
      "        \"title\": \"FORMULA 1 HEINEKEN LAS VEGAS GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Las Vegas Strip Circuit\",\n",
      "        \"abbr\": \"VEG\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-11-22 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1108,\n",
      "        \"title\": \"FORMULA 1 QATAR AIRWAYS QATAR GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Lusail International Circuit\",\n",
      "        \"abbr\": \"QAT\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-11-30 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": 1109,\n",
      "        \"title\": \"FORMULA 1 ETIHAD AIRWAYS ABU DHABI GRAND PRIX 2025\",\n",
      "        \"short_title\": \"Yas Marina Circuit\",\n",
      "        \"abbr\": \"UAE\",\n",
      "        \"date\": {\n",
      "            \"date\": \"2025-12-07 00:00:00.000000\",\n",
      "            \"timezone_type\": 3,\n",
      "            \"timezone\": \"UTC\"\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "\n",
    "def fetch_data(url: str, timeout: int = 10) -> Tuple[bool, Any]:\n",
    "    \"\"\"\n",
    "    Fetch data from the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url: The URL to fetch data from\n",
    "        timeout: Request timeout in seconds\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - Success status (True/False)\n",
    "            - Response data if successful, error message if not\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to fetch data from: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Successfully fetched data (Status Code: {response.status_code})\")\n",
    "\n",
    "        try:\n",
    "            return True, response.json()\n",
    "        except json.JSONDecodeError:\n",
    "            error_msg = \"Failed to decode JSON from the response.\"\n",
    "            print(f\"\\nError: {error_msg}\")\n",
    "            print(\"Response text was:\")\n",
    "            print(response.text[:500] + \"...\" if len(response.text) > 500 else response.text)\n",
    "            return False, error_msg\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        error_msg = f\"The request to {url} timed out.\"\n",
    "        print(f\"\\nError: {error_msg}\")\n",
    "        return False, error_msg\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_msg = f\"Request error: {e}\"\n",
    "        print(f\"\\nError during request to {url}: {error_msg}\")\n",
    "        return False, error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error: {e}\"\n",
    "        print(f\"\\nAn unexpected error occurred: {error_msg}\")\n",
    "        return False, error_msg\n",
    "\n",
    "\n",
    "def extract_events_data(data: Dict[str, Any]) -> Tuple[bool, Optional[List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Extract events data from the parsed JSON response.\n",
    "\n",
    "    Args:\n",
    "        data: Parsed JSON data\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - Success status (True/False)\n",
    "            - Events data if successful, None if not\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Navigate through the dictionary to extract the 'events' list\n",
    "        # Path: root -> 'data' -> 'chart' -> 'events'\n",
    "        data_section = data.get('data')\n",
    "        chart_section = data_section.get('chart') if data_section else None\n",
    "        events_data = chart_section.get('events') if chart_section else None\n",
    "\n",
    "        if events_data is not None:\n",
    "            return True, events_data\n",
    "        else:\n",
    "            error_msg = \"Could not find the 'events' data at the expected path ('data' -> 'chart' -> 'events') in the JSON response.\"\n",
    "            print(f\"\\nError: {error_msg}\")\n",
    "            return False, None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error extracting events data: {e}\"\n",
    "        print(f\"\\nError: {error_msg}\")\n",
    "        return False, None\n",
    "\n",
    "\n",
    "def process_f1_data(url: str = \"https://inmotion.dhl/api/f1-award-element-data/6367\") -> None:\n",
    "    \"\"\"\n",
    "    Main function to fetch and process F1 data.\n",
    "\n",
    "    Args:\n",
    "        url: The API URL to fetch data from\n",
    "    \"\"\"\n",
    "    # Fetch data\n",
    "    success, response_data = fetch_data(url)\n",
    "\n",
    "    if not success:\n",
    "        return\n",
    "\n",
    "    # Extract events data\n",
    "    success, events_data = extract_events_data(response_data)\n",
    "\n",
    "    if success and events_data is not None:\n",
    "        print(\"\\nSuccessfully extracted events data:\")\n",
    "        # Pretty print the extracted list\n",
    "        print(json.dumps(events_data, indent=4))\n",
    "\n",
    "\n",
    "# Execute the code if this cell is run\n",
    "if __name__ == \"__main__\" or 'get_ipython' in globals():\n",
    "    process_f1_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 events to process.\n",
      "\n",
      "Attempting to fetch data for Event ID: 1086 (Sample Event 1)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1086\n",
      "Successfully fetched and parsed data for Event ID: 1086\n",
      "\n",
      "Attempting to fetch data for Event ID: 1087 (Sample Event 2)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1087\n",
      "Successfully fetched and parsed data for Event ID: 1087\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully fetched data for 2 events.\n",
      "Failed to fetch data for 0 events.\n",
      "\n",
      "--- Sample Data for Event ID 1086 ---\n",
      "{\n",
      "    \"data\": {\n",
      "        \"chart\": [\n",
      "            {\n",
      "                \"id\": 7240,\n",
      "                \"driverNr\": 16,\n",
      "                \"tla\": \"LEC\",\n",
      "                \"firstName\": \"Charles\",\n",
      "                \"lastName\": \"Leclerc\",\n",
      "                \"team\": \"Ferrari\",\n",
      "                \"duration\": 2.32,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:17:28.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 34,\n",
      "                \"points\": 25,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7241,\n",
      "                \"driverNr\": 44,\n",
      "                \"tla\": \"HAM\",\n",
      "                \"firstName\": \"Lewis\",\n",
      "                \"lastName\": \"Hamilton\",\n",
      "                \"team\": \"Ferrari\",\n",
      "                \"duration\": 2.38,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:42:38.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 47,\n",
      "                \"points\": 18,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7242,\n",
      "                \"driverNr\": 63,\n",
      "                \"tla\": \"RUS\",\n",
      "                \"firstName\": \"George\",\n",
      "                \"lastName\": \"Russell\",\n",
      "                \"team\": \"Mercedes\",\n",
      "                \"duration\": 2.43,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:17:17.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 34,\n",
      "                \"points\": 15,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7243,\n",
      "                \"driverNr\": 22,\n",
      "                \"tla\": \"TSU\",\n",
      "                \"firstName\": \"Yuki\",\n",
      "                \"lastName\": \"Tsunoda\",\n",
      "                \"team\": \"Racing Bulls\",\n",
      "                \"duration\": 2.47,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:42:42.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 47,\n",
      "                \"points\": 12,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7244,\n",
      "                \"driverNr\": 87,\n",
      "                \"tla\": \"BEA\",\n",
      "                \"firstName\": \"Oliver\",\n",
      "                \"lastName\": \"Bearman\",\n",
      "                \"team\": \"Haas\",\n",
      "                \"duration\": 2.49,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 15:29:03.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 4,\n",
      "                \"points\": 10,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7245,\n",
      "                \"driverNr\": 31,\n",
      "                \"tla\": \"OCO\",\n",
      "                \"firstName\": \"Esteban\",\n",
      "                \"lastName\": \"Ocon\",\n",
      "                \"team\": \"Haas\",\n",
      "                \"duration\": 2.54,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 15:28:55.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 4,\n",
      "                \"points\": 8,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7247,\n",
      "                \"driverNr\": 1,\n",
      "                \"tla\": \"VER\",\n",
      "                \"firstName\": \"Max\",\n",
      "                \"lastName\": \"Verstappen\",\n",
      "                \"team\": \"Red Bull\",\n",
      "                \"duration\": 2.56,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:40:15.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 46,\n",
      "                \"points\": 6,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7248,\n",
      "                \"driverNr\": 30,\n",
      "                \"tla\": \"LAW\",\n",
      "                \"firstName\": \"Liam\",\n",
      "                \"lastName\": \"Lawson\",\n",
      "                \"team\": \"Red Bull\",\n",
      "                \"duration\": 2.58,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 15:29:00.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 4,\n",
      "                \"points\": 4,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7251,\n",
      "                \"driverNr\": 12,\n",
      "                \"tla\": \"ANT\",\n",
      "                \"firstName\": \"Andrea Kimi\",\n",
      "                \"lastName\": \"Antonelli\",\n",
      "                \"team\": \"Mercedes\",\n",
      "                \"duration\": 2.73,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:15:54.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 33,\n",
      "                \"points\": 2,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 7252,\n",
      "                \"driverNr\": 23,\n",
      "                \"tla\": \"ALB\",\n",
      "                \"firstName\": \"Alexander\",\n",
      "                \"lastName\": \"Albon\",\n",
      "                \"team\": \"Williams\",\n",
      "                \"duration\": 2.75,\n",
      "                \"startTime\": {\n",
      "                    \"date\": \"2025-03-16 16:15:41.000000\",\n",
      "                    \"timezone_type\": 3,\n",
      "                    \"timezone\": \"UTC\"\n",
      "                },\n",
      "                \"lap\": 33,\n",
      "                \"points\": 1,\n",
      "                \"irregular\": false,\n",
      "                \"notes\": \"\"\n",
      "            }\n",
      "        ],\n",
      "        \"sort\": \"1\",\n",
      "        \"event_id\": 1086,\n",
      "        \"headline\": {\n",
      "            \"style\": {\n",
      "                \"size\": \"3\",\n",
      "                \"align\": \"c\"\n",
      "            },\n",
      "            \"headline\": {\n",
      "                \"style\": {\n",
      "                    \"color\": \"b\",\n",
      "                    \"uppercase\": false\n",
      "                }\n",
      "            },\n",
      "            \"headline_type\": \"1\"\n",
      "        },\n",
      "        \"list_item_title\": \"Australia\"\n",
      "    },\n",
      "    \"htmlList\": {\n",
      "        \"table\": \"\\n<table class=\\\"f1-award-table\\\">\\n  <tr>\\n    <th class=\\\"align-center\\\">Pos.</th>\\n    <th>Team</th>\\n    <th>Driver</th>\\n    <th>Time (sec)</th>\\n    <th>Lap</th>\\n    <th>Points</th>\\n  </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>1</strong></td>\\n      <td>Ferrari</td>\\n      <td>Leclerc</td>\\n      <td>2.32</td>\\n      <td>34</td>\\n      <td><strong>25</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>2</strong></td>\\n      <td>Ferrari</td>\\n      <td>Hamilton</td>\\n      <td>2.38</td>\\n      <td>47</td>\\n      <td><strong>18</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>3</strong></td>\\n      <td>Mercedes</td>\\n      <td>Russell</td>\\n      <td>2.43</td>\\n      <td>34</td>\\n      <td><strong>15</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>4</strong></td>\\n      <td>Racing Bulls</td>\\n      <td>Tsunoda</td>\\n      <td>2.47</td>\\n      <td>47</td>\\n      <td><strong>12</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>5</strong></td>\\n      <td>Haas</td>\\n      <td>Bearman</td>\\n      <td>2.49</td>\\n      <td>4</td>\\n      <td><strong>10</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>6</strong></td>\\n      <td>Haas</td>\\n      <td>Ocon</td>\\n      <td>2.54</td>\\n      <td>4</td>\\n      <td><strong>8</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>7</strong></td>\\n      <td>Ferrari</td>\\n      <td>Hamilton</td>\\n      <td>2.55</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>8</strong></td>\\n      <td>Red Bull</td>\\n      <td>Verstappen</td>\\n      <td>2.56</td>\\n      <td>46</td>\\n      <td><strong>6</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>9</strong></td>\\n      <td>Red Bull</td>\\n      <td>Lawson</td>\\n      <td>2.58</td>\\n      <td>4</td>\\n      <td><strong>4</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>10</strong></td>\\n      <td>Racing Bulls</td>\\n      <td>Tsunoda</td>\\n      <td>2.67</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>11</strong></td>\\n      <td>Haas</td>\\n      <td>Ocon</td>\\n      <td>2.69</td>\\n      <td>46</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>12</strong></td>\\n      <td>Mercedes</td>\\n      <td>Antonelli</td>\\n      <td>2.73</td>\\n      <td>33</td>\\n      <td><strong>2</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>13</strong></td>\\n      <td>Williams</td>\\n      <td>Albon</td>\\n      <td>2.75</td>\\n      <td>33</td>\\n      <td><strong>1</strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>14</strong></td>\\n      <td>Mercedes</td>\\n      <td>Russell</td>\\n      <td>2.76</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>15</strong></td>\\n      <td>Red Bull</td>\\n      <td>Verstappen</td>\\n      <td>2.77</td>\\n      <td>34</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>16</strong></td>\\n      <td>McLaren</td>\\n      <td>Norris</td>\\n      <td>2.79</td>\\n      <td>34</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>17</strong></td>\\n      <td>Haas</td>\\n      <td>Bearman</td>\\n      <td>2.80</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>18</strong></td>\\n      <td>Haas</td>\\n      <td>Ocon</td>\\n      <td>2.82</td>\\n      <td>39</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>19</strong></td>\\n      <td>Aston Martin</td>\\n      <td>Stroll</td>\\n      <td>2.82</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>20</strong></td>\\n      <td>Aston Martin</td>\\n      <td>Stroll</td>\\n      <td>2.87</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>21</strong></td>\\n      <td>Red Bull</td>\\n      <td>Lawson</td>\\n      <td>2.94</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>22</strong></td>\\n      <td>Mercedes</td>\\n      <td>Antonelli</td>\\n      <td>3.05</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>23</strong></td>\\n      <td>McLaren</td>\\n      <td>Norris</td>\\n      <td>3.19</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>24</strong></td>\\n      <td>Ferrari</td>\\n      <td>Leclerc</td>\\n      <td>3.28</td>\\n      <td>47</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>25</strong></td>\\n      <td>Alpine</td>\\n      <td>Gasly</td>\\n      <td>3.32</td>\\n      <td>46</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>26</strong></td>\\n      <td>McLaren</td>\\n      <td>Piastri</td>\\n      <td>3.45</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>27</strong></td>\\n      <td>Williams</td>\\n      <td>Albon</td>\\n      <td>3.54</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>28</strong></td>\\n      <td>Alpine</td>\\n      <td>Gasly</td>\\n      <td>3.66</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>29</strong></td>\\n      <td>McLaren</td>\\n      <td>Piastri</td>\\n      <td>3.72</td>\\n      <td>34</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>30</strong></td>\\n      <td>Sauber</td>\\n      <td>Hulkenberg</td>\\n      <td>4.01</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>31</strong></td>\\n      <td>Haas</td>\\n      <td>Bearman</td>\\n      <td>4.95</td>\\n      <td>39</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>32</strong></td>\\n      <td>Sauber</td>\\n      <td>Hulkenberg</td>\\n      <td>5.13</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>33</strong></td>\\n      <td>Sauber</td>\\n      <td>Bortoleto</td>\\n      <td>5.59</td>\\n      <td>33</td>\\n      <td><strong></strong></td>\\n    </tr>\\n      <tr>\\n      <td class=\\\"align-center\\\"><strong>34</strong></td>\\n      <td>Sauber</td>\\n      <td>Bortoleto</td>\\n      <td>9.33</td>\\n      <td>44</td>\\n      <td><strong></strong></td>\\n    </tr>\\n  </table>\\n\\n\",\n",
      "        \"content\": \"<div class=\\\"f1-award-content\\\"><html-content-element\\n        class=\\\"html html--v3\\\"        id=\\\"element_event_html\\\"        \\n    ><div class=\\\"height-toggleable box-gradient\\\"><div class=\\\"running-text\\\"><h3>Ferrari perform DHL Fastest Pit Stop in Australia, but get the timing wrong</h3><p>Towards the end of last year, the Ferrari, Red Bull and McLaren mechanics were taking turns to chalk up the DHL Fastest Pit Stop. With a one-two in the first race of 2025, the Italian team have carried on where they left off. Unfortunately for them, though, it was only the stops themselves that they got right in Australia, not the timing. </p><p>The crew excelled during their second stop of the day, when they swapped intermediates for slicks on Charles Leclerc\\u2019s car on lap 34. Despite the difficulties caused by the wet conditions, they managed a superb tire change of 2.32 seconds. </p><p>Almost equally impressive was the second fastest stop of 2.38 seconds for Lewis Hamilton\\u2019s car on lap 47 in the rain. Yet the achievement brings only limited satisfaction, as they should actually have called the seven-time champion into the pits on lap 44, when another shower hit the track, and switched him back to intermediates. </p><p>However, Ferrari failed to make the right decision with both cars and stayed out far too long in the false hope that the rain would stop quickly. Ultimately, Leclerc and Hamilton had to settle for eighth and tenth place respectively. \\u201cIt was a tough race and there are things we have to review and work on,\\u201d said Leclerc afterwards. \\u201cWe weren\\u2019t the fastest out there, but in such weather conditions, there was a chance of scoring some big points which we didn\\u2019t capitalize on today.\\u201d </p></div></div><read-more-element\\n        class=\\\"height-toggle-new js-toggle-height state-closed\\\"\\n        data-target=\\\"#element_event_html .height-toggleable\\\"\\n        data-scroll-target=\\\"#element_event_html\\\"\\n        data-scroll-offset=\\\"-15\\\"\\n    ><div class=\\\"text\\\"><span class=\\\"open-text\\\">Read more</span><span class=\\\"close-text\\\">Close</span></div></read-more-element></html-content-element></div>\",\n",
      "        \"video\": \"<div class='f1-award-video'><full-player\\n  id=\\\"p2130695957_player\\\"\\n  class=\\\"player player--video-js\\\" data-analytics=\\\"\\\"      data-aspectratio-desktop=\\\"16:9\\\"\\n            data-aspectratio-mobile=\\\"1:1\\\"\\n      ><div class=\\\"player__inner\\\"><video-js\\n      id=\\\"p2130695957_player_video\\\"\\n      class=\\\"player__video\\\"\\n       preload=\\\"metadata\\\"  data-poster-desktop=\\\"https://images.inmotion.dhl/MYFjQaB-XB-zewHDACtWVxtWhVuhjBxAu52IdArP9I0/format:jpeg/width:1920/bG9jYWw6Ly8vemZzL2RobG1lZGlhL3ZpZGVvL3JlZ2lzdGVyZWQvMjAyNS8wMy8xNTg1NTZhOGUxMTIzYTdlYTMxMmRhZDcwZTVhNDM3OS5qcGc\\\"\\n      data-poster-mobile=\\\"https://images.inmotion.dhl/xJhbhuWzwYNwHpuqtjqU8XZ8T3IdH_7w0iol8k2lzto/format:jpeg/width:1920/bG9jYWw6Ly8vemZzL2RobG1lZGlhL3ZpZGVvL3JlZ2lzdGVyZWQvMjAyNS8wMy81N2YzZTQ4NTU3ZWYxY2IwMmFkMDYxYzBjY2E2MDZlMC5qcGc\\\"\\n      playsinline\\n    ><source\\n          type=\\\"video/mp4\\\"\\n          label=\\\"1080p\\\"\\n          data-src-desktop=\\\"https://media.inmotion.dhl/video/registered/2025/03/158556a8e1123a7ea312dad70e5a4379_1080p.mp4\\\"/><source\\n          type=\\\"video/mp4\\\"\\n          label=\\\"1080p\\\"\\n          data-src-mobile=\\\"https://media.inmotion.dhl/video/registered/2025/03/57f3e48557ef1cb02ad061c0cca606e0_1080p.mp4\\\"/></video-js></div></full-player></div>\",\n",
      "        \"header\": \"<div class=\\\"f1-award-header\\\"><header\\n        class=\\\"headline-group headline-group--size-3 headline-group--align-c\\\"                \\n    ><h1\\n        class=\\\"headline color-b\\\"                \\n    >\\n\\n    FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\\n\\n            </h1></header></div>\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "\n",
    "\n",
    "class EventDataFetcher:\n",
    "    \"\"\"\n",
    "    A class to fetch event-specific data from the DHL F1 API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_url: str = \"https://inmotion.dhl/api/f1-award-element-data/6365\",\n",
    "        timeout: int = 15,\n",
    "        delay: float = 0.5,\n",
    "        user_agent: str = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the EventDataFetcher with configuration parameters.\n",
    "\n",
    "        Args:\n",
    "            base_url: The base URL for the API endpoint\n",
    "            timeout: Request timeout in seconds\n",
    "            delay: Delay between requests in seconds\n",
    "            user_agent: User agent string for the HTTP requests\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.timeout = timeout\n",
    "        self.delay = delay\n",
    "        self.headers = {\n",
    "            'User-Agent': user_agent\n",
    "        }\n",
    "        self.results = {}  # Dictionary to store results {event_id: data}\n",
    "\n",
    "    def fetch_event_data(self, event_id: Union[int, str], event_title: str = \"Unknown Title\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch data for a specific event.\n",
    "\n",
    "        Args:\n",
    "            event_id: The ID of the event\n",
    "            event_title: The title of the event (for logging purposes)\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the event data or error information\n",
    "        \"\"\"\n",
    "        specific_url = f\"{self.base_url}?event={event_id}\"\n",
    "        print(f\"\\nAttempting to fetch data for Event ID: {event_id} ({event_title})\")\n",
    "        print(f\"URL: {specific_url}\")\n",
    "\n",
    "        try:\n",
    "            # Make the GET request for the specific event\n",
    "            response = requests.get(specific_url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "            # Parse the JSON response\n",
    "            try:\n",
    "                event_data = response.json()\n",
    "                print(f\"Successfully fetched and parsed data for Event ID: {event_id}\")\n",
    "                return event_data\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: Failed to decode JSON for Event ID: {event_id}\")\n",
    "                print(f\"Response text (first 500 chars): {response.text[:500]}\")\n",
    "                return {\"error\": \"JSONDecodeError\", \"response_text\": response.text[:500]}\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Error: Request timed out for Event ID: {event_id}\")\n",
    "            return {\"error\": \"Timeout\"}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during request for Event ID: {event_id}: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred for Event ID: {event_id}: {e}\")\n",
    "            return {\"error\": f\"Unexpected: {str(e)}\"}\n",
    "\n",
    "    def process_events(self, events_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a list of events and fetch data for each one.\n",
    "\n",
    "        Args:\n",
    "            events_data: A list of event dictionaries, each containing at least 'id' and optionally 'title'\n",
    "\n",
    "        Returns:\n",
    "            A dictionary mapping event IDs to their fetched data\n",
    "        \"\"\"\n",
    "        print(f\"Found {len(events_data)} events to process.\")\n",
    "\n",
    "        for event in events_data:\n",
    "            event_id = event.get('id')\n",
    "            event_title = event.get('title', 'Unknown Title')\n",
    "\n",
    "            if not event_id:\n",
    "                print(f\"Warning: Skipping event with missing ID: {event_title}\")\n",
    "                continue\n",
    "\n",
    "            # Fetch data for this event\n",
    "            event_data = self.fetch_event_data(event_id, event_title)\n",
    "            self.results[event_id] = event_data\n",
    "\n",
    "            # Optional delay between requests\n",
    "            if self.delay > 0:\n",
    "                time.sleep(self.delay)\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Print a summary of the fetching results.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Processing Complete ---\")\n",
    "\n",
    "        successful_fetches = sum(1 for data in self.results.values()\n",
    "                                if isinstance(data, dict) and 'error' not in data)\n",
    "        failed_fetches = len(self.results) - successful_fetches\n",
    "\n",
    "        print(f\"Successfully fetched data for {successful_fetches} events.\")\n",
    "        print(f\"Failed to fetch data for {failed_fetches} events.\")\n",
    "\n",
    "    def print_sample_data(self, event_id: Union[int, str]) -> None:\n",
    "        \"\"\"\n",
    "        Print sample data for a specific event ID.\n",
    "\n",
    "        Args:\n",
    "            event_id: The ID of the event to display data for\n",
    "        \"\"\"\n",
    "        if event_id in self.results:\n",
    "            if isinstance(self.results[event_id], dict) and 'error' not in self.results[event_id]:\n",
    "                print(f\"\\n--- Sample Data for Event ID {event_id} ---\")\n",
    "                print(json.dumps(self.results[event_id], indent=4))\n",
    "            else:\n",
    "                print(f\"\\n--- Error Data for Event ID {event_id} ---\")\n",
    "                print(json.dumps(self.results[event_id], indent=4))\n",
    "        else:\n",
    "            print(f\"\\nNo data found for Event ID {event_id}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample events data (replace with your actual data)\n",
    "    sample_events = [\n",
    "        {\"id\": 1086, \"title\": \"Sample Event 1\"},\n",
    "        {\"id\": 1087, \"title\": \"Sample Event 2\"},\n",
    "        # Add more events as needed\n",
    "    ]\n",
    "\n",
    "    # Create the fetcher with default or custom configuration\n",
    "    fetcher = EventDataFetcher(\n",
    "        base_url=\"https://inmotion.dhl/api/f1-award-element-data/6365\",\n",
    "        timeout=15,\n",
    "        delay=0.5\n",
    "    )\n",
    "\n",
    "    # Process all events\n",
    "    fetcher.process_events(sample_events)\n",
    "\n",
    "    # Print summary\n",
    "    fetcher.print_summary()\n",
    "\n",
    "    # Print sample data for a specific event\n",
    "    fetcher.print_sample_data(1086)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6367\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully extracted events data\n",
      "Found 24 events to process.\n",
      "\n",
      "Attempting to fetch data for Event ID: 1086 (FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1086\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1086\n",
      "\n",
      "Attempting to fetch data for Event ID: 1087 (FORMULA 1 HEINEKEN CHINESE GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1087\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1087\n",
      "\n",
      "Attempting to fetch data for Event ID: 1088 (FORMULA 1 LENOVO JAPANESE GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1088\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1088\n",
      "\n",
      "Attempting to fetch data for Event ID: 1089 (FORMULA 1 GULF AIR BAHRAIN GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1089\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1089\n",
      "\n",
      "Attempting to fetch data for Event ID: 1090 (FORMULA 1 STC SAUDI ARABIAN GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1090\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1090\n",
      "\n",
      "Attempting to fetch data for Event ID: 1091 (FORMULA 1 CRYPTO.COM MIAMI GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1091\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1091\n",
      "\n",
      "Attempting to fetch data for Event ID: 1092 (FORMULA 1 AWS GRAN PREMIO DEL MADE IN ITALY E DELL'EMILIA-ROMAGNA 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1092\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1092\n",
      "\n",
      "Attempting to fetch data for Event ID: 1093 (FORMULA 1 TAG HEUER GRAND PRIX DE MONACO 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1093\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1093\n",
      "\n",
      "Attempting to fetch data for Event ID: 1094 (FORMULA 1 ARAMCO GRAN PREMIO DE ESPAÑA 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1094\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1094\n",
      "\n",
      "Attempting to fetch data for Event ID: 1095 (FORMULA 1 PIRELLI GRAND PRIX DU CANADA 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1095\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1095\n",
      "\n",
      "Attempting to fetch data for Event ID: 1096 (FORMULA 1 MSC CRUISES AUSTRIAN GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1096\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1096\n",
      "\n",
      "Attempting to fetch data for Event ID: 1097 (FORMULA 1 QATAR AIRWAYS BRITISH GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1097\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1097\n",
      "\n",
      "Attempting to fetch data for Event ID: 1098 (FORMULA 1 MOËT & CHANDON BELGIAN GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1098\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1098\n",
      "\n",
      "Attempting to fetch data for Event ID: 1099 (FORMULA 1 LENOVO HUNGARIAN GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1099\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1099\n",
      "\n",
      "Attempting to fetch data for Event ID: 1100 (FORMULA 1 HEINEKEN DUTCH GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1100\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1100\n",
      "\n",
      "Attempting to fetch data for Event ID: 1101 (FORMULA 1 PIRELLI GRAN PREMIO D’ITALIA 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1101\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1101\n",
      "\n",
      "Attempting to fetch data for Event ID: 1102 (FORMULA 1 QATAR AIRWAYS AZERBAIJAN GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1102\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1102\n",
      "\n",
      "Attempting to fetch data for Event ID: 1103 (FORMULA 1 SINGAPORE AIRLINES SINGAPORE GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1103\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1103\n",
      "\n",
      "Attempting to fetch data for Event ID: 1104 (FORMULA 1 MSC CRUISES UNITED STATES GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1104\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1104\n",
      "\n",
      "Attempting to fetch data for Event ID: 1105 (FORMULA 1 GRAN PREMIO DE LA CIUDAD DE MÉXICO 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1105\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1105\n",
      "\n",
      "Attempting to fetch data for Event ID: 1106 (FORMULA 1 MSC CRUISES GRANDE PRÊMIO DE SÃO PAULO 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1106\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1106\n",
      "\n",
      "Attempting to fetch data for Event ID: 1107 (FORMULA 1 HEINEKEN LAS VEGAS GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1107\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1107\n",
      "\n",
      "Attempting to fetch data for Event ID: 1108 (FORMULA 1 QATAR AIRWAYS QATAR GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1108\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1108\n",
      "\n",
      "Attempting to fetch data for Event ID: 1109 (FORMULA 1 ETIHAD AIRWAYS ABU DHABI GRAND PRIX 2025)\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6365?event=1109\n",
      "Successfully fetched data (Status Code: 200)\n",
      "Successfully fetched and parsed data for Event ID: 1109\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully fetched data for 24 events.\n",
      "Failed to fetch data for 0 events.\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "\n",
      "--- DataFrame for Event ID 1086 ---\n",
      "    Pos.          Team      Driver  Time (sec)  Lap  Points\n",
      "0      1       Ferrari     Leclerc        2.32   34    25.0\n",
      "1      2       Ferrari    Hamilton        2.38   47    18.0\n",
      "2      3      Mercedes     Russell        2.43   34    15.0\n",
      "3      4  Racing Bulls     Tsunoda        2.47   47    12.0\n",
      "4      5          Haas     Bearman        2.49    4    10.0\n",
      "5      6          Haas        Ocon        2.54    4     8.0\n",
      "6      7       Ferrari    Hamilton        2.55   33     NaN\n",
      "7      8      Red Bull  Verstappen        2.56   46     6.0\n",
      "8      9      Red Bull      Lawson        2.58    4     4.0\n",
      "9     10  Racing Bulls     Tsunoda        2.67   33     NaN\n",
      "10    11          Haas        Ocon        2.69   46     NaN\n",
      "11    12      Mercedes   Antonelli        2.73   33     2.0\n",
      "12    13      Williams       Albon        2.75   33     1.0\n",
      "13    14      Mercedes     Russell        2.76   44     NaN\n",
      "14    15      Red Bull  Verstappen        2.77   34     NaN\n",
      "15    16       McLaren      Norris        2.79   34     NaN\n",
      "16    17          Haas     Bearman        2.80   44     NaN\n",
      "17    18          Haas        Ocon        2.82   39     NaN\n",
      "18    19  Aston Martin      Stroll        2.82   44     NaN\n",
      "19    20  Aston Martin      Stroll        2.87   33     NaN\n",
      "20    21      Red Bull      Lawson        2.94   33     NaN\n",
      "21    22      Mercedes   Antonelli        3.05   44     NaN\n",
      "22    23       McLaren      Norris        3.19   44     NaN\n",
      "23    24       Ferrari     Leclerc        3.28   47     NaN\n",
      "24    25        Alpine       Gasly        3.32   46     NaN\n",
      "25    26       McLaren     Piastri        3.45   44     NaN\n",
      "26    27      Williams       Albon        3.54   44     NaN\n",
      "27    28        Alpine       Gasly        3.66   33     NaN\n",
      "28    29       McLaren     Piastri        3.72   34     NaN\n",
      "29    30        Sauber  Hulkenberg        4.01   44     NaN\n",
      "30    31          Haas     Bearman        4.95   39     NaN\n",
      "31    32        Sauber  Hulkenberg        5.13   33     NaN\n",
      "32    33        Sauber   Bortoleto        5.59   33     NaN\n",
      "33    34        Sauber   Bortoleto        9.33   44     NaN\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import io\n",
    "from typing import Dict, List, Optional, Any, Tuple, Union\n",
    "\n",
    "\n",
    "def fetch_data_from_url(url: str, headers: Optional[Dict] = None,\n",
    "                        timeout: int = 10) -> Tuple[bool, Union[Dict, str]]:\n",
    "    \"\"\"\n",
    "    Fetch data from a URL with error handling.\n",
    "\n",
    "    Args:\n",
    "        url: The URL to fetch data from\n",
    "        headers: Optional HTTP headers\n",
    "        timeout: Request timeout in seconds\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing success status and either the parsed JSON data or error message\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to fetch data from: {url}\")\n",
    "\n",
    "    try:\n",
    "        # Make the GET request\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Successfully fetched data (Status Code: {response.status_code})\")\n",
    "\n",
    "        # Parse the JSON response\n",
    "        try:\n",
    "            return True, response.json()\n",
    "        except json.JSONDecodeError:\n",
    "            error_msg = f\"Failed to decode JSON from the response. Response text: {response.text[:500]}\"\n",
    "            print(f\"Error: {error_msg}\")\n",
    "            return False, error_msg\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        error_msg = f\"The request to {url} timed out.\"\n",
    "        print(f\"Error: {error_msg}\")\n",
    "        return False, error_msg\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_msg = f\"Request error for {url}: {e}\"\n",
    "        print(f\"Error: {error_msg}\")\n",
    "        return False, error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error: {e}\"\n",
    "        print(f\"Error: {error_msg}\")\n",
    "        return False, error_msg\n",
    "\n",
    "\n",
    "def extract_events_data(parsed_data: Dict) -> Tuple[bool, Union[List, str]]:\n",
    "    \"\"\"\n",
    "    Extract events data from parsed JSON.\n",
    "\n",
    "    Args:\n",
    "        parsed_data: The parsed JSON data\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing success status and either the events data or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Navigate through the dictionary to extract the 'events' list\n",
    "        data_section = parsed_data.get(\"data\")\n",
    "        chart_section = data_section.get(\"chart\") if data_section else None\n",
    "        events_data = chart_section.get(\"events\") if chart_section else None\n",
    "\n",
    "        if events_data is not None:\n",
    "            print(\"Successfully extracted events data\")\n",
    "            return True, events_data\n",
    "        else:\n",
    "            error_msg = \"Could not find the 'events' data at the expected path ('data' -> 'chart' -> 'events')\"\n",
    "            print(f\"Error: {error_msg}\")\n",
    "            return False, error_msg\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error extracting events data: {e}\"\n",
    "        print(f\"Error: {error_msg}\")\n",
    "        return False, error_msg\n",
    "\n",
    "\n",
    "def fetch_event_specific_data(events_data: List[Dict], base_url: str,\n",
    "                             headers: Optional[Dict] = None, timeout: int = 15,\n",
    "                             delay: float = 0.5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch specific data for each event.\n",
    "\n",
    "    Args:\n",
    "        events_data: List of events\n",
    "        base_url: Base URL for event-specific data\n",
    "        headers: Optional HTTP headers\n",
    "        timeout: Request timeout in seconds\n",
    "        delay: Delay between requests in seconds\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping event IDs to their specific data\n",
    "    \"\"\"\n",
    "    all_event_specific_data = {}  # Dictionary to store results {event_id: data}\n",
    "\n",
    "    print(f\"Found {len(events_data)} events to process.\")\n",
    "\n",
    "    for event in events_data:\n",
    "        event_id = event.get(\"id\")\n",
    "        event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "        if not event_id:\n",
    "            print(f\"Warning: Skipping event with missing ID: {event_title}\")\n",
    "            continue\n",
    "\n",
    "        # Construct the specific URL for this event\n",
    "        specific_url = f\"{base_url}?event={event_id}\"\n",
    "        print(f\"\\nAttempting to fetch data for Event ID: {event_id} ({event_title})\")\n",
    "\n",
    "        success, result = fetch_data_from_url(specific_url, headers, timeout)\n",
    "\n",
    "        if success:\n",
    "            all_event_specific_data[event_id] = result\n",
    "            print(f\"Successfully fetched and parsed data for Event ID: {event_id}\")\n",
    "        else:\n",
    "            all_event_specific_data[event_id] = {\"error\": result}\n",
    "\n",
    "        # Optional delay\n",
    "        if delay > 0:\n",
    "            time.sleep(delay)\n",
    "\n",
    "    return all_event_specific_data\n",
    "\n",
    "\n",
    "def html_table_to_dataframe(event_json_data: Dict) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Extracts an HTML table string from event JSON data and converts it to a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        event_json_data: The JSON data dictionary for a specific event,\n",
    "                         expected to contain ['htmlList']['table'].\n",
    "\n",
    "    Returns:\n",
    "        The DataFrame created from the HTML table, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    if not isinstance(event_json_data, dict):\n",
    "        print(\"Error: Input must be a dictionary.\")\n",
    "        return None\n",
    "\n",
    "    # Safely extract the HTML table string\n",
    "    html_table_str = event_json_data.get(\"htmlList\", {}).get(\"table\")\n",
    "\n",
    "    if not html_table_str:\n",
    "        print(\"Error: Could not find 'htmlList' -> 'table' in the provided JSON data or it's empty.\")\n",
    "        return None\n",
    "\n",
    "    if not isinstance(html_table_str, str):\n",
    "        print(\"Error: The value at ['htmlList']['table'] is not a string.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Found HTML table string. Attempting to parse...\")\n",
    "    try:\n",
    "        # pd.read_html returns a list of DataFrames. We expect only one table.\n",
    "        list_of_dfs = pd.read_html(io.StringIO(html_table_str))\n",
    "\n",
    "        if list_of_dfs:\n",
    "            print(\"Successfully parsed HTML table into DataFrame.\")\n",
    "            return list_of_dfs[0]  # Return the first DataFrame found\n",
    "        else:\n",
    "            print(\"Warning: No tables found by pd.read_html, although HTML string was present.\")\n",
    "            return None\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error parsing HTML with pandas (ValueError): {ve}\")\n",
    "        return None\n",
    "    except ImportError:\n",
    "        print(\"Error: The 'lxml' library might be required by pd.read_html. Please install it (`pip install lxml`).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during HTML parsing: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_event_data(event_data: Dict, event_id: Any) -> None:\n",
    "    \"\"\"\n",
    "    Process and display data for a specific event.\n",
    "\n",
    "    Args:\n",
    "        event_data: The event data to process\n",
    "        event_id: The ID of the event\n",
    "    \"\"\"\n",
    "    if \"error\" in event_data:\n",
    "        print(f\"\\n--- Error Data for Event ID {event_id} ---\")\n",
    "        print(json.dumps(event_data, indent=4))\n",
    "        return\n",
    "\n",
    "    # Convert HTML table to DataFrame\n",
    "    event_dataframe = html_table_to_dataframe(event_data)\n",
    "\n",
    "    if event_dataframe is not None:\n",
    "        print(f\"\\n--- DataFrame for Event ID {event_id} ---\")\n",
    "        print(event_dataframe.to_string())\n",
    "    else:\n",
    "        print(f\"\\nFailed to create DataFrame for Event ID {event_id}.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the workflow.\"\"\"\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"events_url\": \"https://inmotion.dhl/api/f1-award-element-data/6367\",\n",
    "        \"base_event_url\": \"https://inmotion.dhl/api/f1-award-element-data/6365\",\n",
    "        \"headers\": {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "        },\n",
    "        \"timeout\": 15,\n",
    "        \"delay\": 0.5,\n",
    "        \"target_event_id\": 1086  # Example event ID to display\n",
    "    }\n",
    "\n",
    "    # Step 1: Fetch events data\n",
    "    success, result = fetch_data_from_url(config[\"events_url\"], config[\"headers\"], config[\"timeout\"])\n",
    "    if not success:\n",
    "        print(\"Failed to fetch initial events data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Extract events data\n",
    "    success, events_data = extract_events_data(result)\n",
    "    if not success:\n",
    "        print(\"Failed to extract events data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Fetch specific data for each event\n",
    "    all_event_specific_data = fetch_event_specific_data(\n",
    "        events_data,\n",
    "        config[\"base_event_url\"],\n",
    "        config[\"headers\"],\n",
    "        config[\"timeout\"],\n",
    "        config[\"delay\"]\n",
    "    )\n",
    "\n",
    "    # Step 4: Display summary\n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "    successful_fetches = sum(\n",
    "        1 for data in all_event_specific_data.values()\n",
    "        if isinstance(data, dict) and \"error\" not in data\n",
    "    )\n",
    "    failed_fetches = len(events_data) - successful_fetches\n",
    "    print(f\"Successfully fetched data for {successful_fetches} events.\")\n",
    "    print(f\"Failed to fetch data for {failed_fetches} events.\")\n",
    "\n",
    "    # Step 5: Process and display data for a specific event\n",
    "    target_id = config[\"target_event_id\"]\n",
    "    if target_id in all_event_specific_data:\n",
    "        process_event_data(all_event_specific_data[target_id], target_id)\n",
    "    else:\n",
    "        print(f\"\\nNo data found for Event ID {target_id}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6367\n",
      "\n",
      "Successfully extracted events data\n",
      "Found 24 events to process.\n",
      "\n",
      "Attempting to fetch data for Event ID: 1086 (FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1086\n",
      "Successfully fetched and parsed data for Event ID: 1086\n",
      "\n",
      "Attempting to fetch data for Event ID: 1087 (FORMULA 1 HEINEKEN CHINESE GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1087\n",
      "Successfully fetched and parsed data for Event ID: 1087\n",
      "\n",
      "Attempting to fetch data for Event ID: 1088 (FORMULA 1 LENOVO JAPANESE GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1088\n",
      "Successfully fetched and parsed data for Event ID: 1088\n",
      "\n",
      "Attempting to fetch data for Event ID: 1089 (FORMULA 1 GULF AIR BAHRAIN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1089\n",
      "Successfully fetched and parsed data for Event ID: 1089\n",
      "\n",
      "Attempting to fetch data for Event ID: 1090 (FORMULA 1 STC SAUDI ARABIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1090\n",
      "Successfully fetched and parsed data for Event ID: 1090\n",
      "\n",
      "Attempting to fetch data for Event ID: 1091 (FORMULA 1 CRYPTO.COM MIAMI GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1091\n",
      "Successfully fetched and parsed data for Event ID: 1091\n",
      "\n",
      "Attempting to fetch data for Event ID: 1092 (FORMULA 1 AWS GRAN PREMIO DEL MADE IN ITALY E DELL'EMILIA-ROMAGNA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1092\n",
      "Successfully fetched and parsed data for Event ID: 1092\n",
      "\n",
      "Attempting to fetch data for Event ID: 1093 (FORMULA 1 TAG HEUER GRAND PRIX DE MONACO 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1093\n",
      "Successfully fetched and parsed data for Event ID: 1093\n",
      "\n",
      "Attempting to fetch data for Event ID: 1094 (FORMULA 1 ARAMCO GRAN PREMIO DE ESPAÑA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1094\n",
      "Successfully fetched and parsed data for Event ID: 1094\n",
      "\n",
      "Attempting to fetch data for Event ID: 1095 (FORMULA 1 PIRELLI GRAND PRIX DU CANADA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1095\n",
      "Successfully fetched and parsed data for Event ID: 1095\n",
      "\n",
      "Attempting to fetch data for Event ID: 1096 (FORMULA 1 MSC CRUISES AUSTRIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1096\n",
      "Successfully fetched and parsed data for Event ID: 1096\n",
      "\n",
      "Attempting to fetch data for Event ID: 1097 (FORMULA 1 QATAR AIRWAYS BRITISH GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1097\n",
      "Successfully fetched and parsed data for Event ID: 1097\n",
      "\n",
      "Attempting to fetch data for Event ID: 1098 (FORMULA 1 MOËT & CHANDON BELGIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1098\n",
      "Successfully fetched and parsed data for Event ID: 1098\n",
      "\n",
      "Attempting to fetch data for Event ID: 1099 (FORMULA 1 LENOVO HUNGARIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1099\n",
      "Successfully fetched and parsed data for Event ID: 1099\n",
      "\n",
      "Attempting to fetch data for Event ID: 1100 (FORMULA 1 HEINEKEN DUTCH GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1100\n",
      "Successfully fetched and parsed data for Event ID: 1100\n",
      "\n",
      "Attempting to fetch data for Event ID: 1101 (FORMULA 1 PIRELLI GRAN PREMIO D’ITALIA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1101\n",
      "Successfully fetched and parsed data for Event ID: 1101\n",
      "\n",
      "Attempting to fetch data for Event ID: 1102 (FORMULA 1 QATAR AIRWAYS AZERBAIJAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1102\n",
      "Successfully fetched and parsed data for Event ID: 1102\n",
      "\n",
      "Attempting to fetch data for Event ID: 1103 (FORMULA 1 SINGAPORE AIRLINES SINGAPORE GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1103\n",
      "Successfully fetched and parsed data for Event ID: 1103\n",
      "\n",
      "Attempting to fetch data for Event ID: 1104 (FORMULA 1 MSC CRUISES UNITED STATES GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1104\n",
      "Successfully fetched and parsed data for Event ID: 1104\n",
      "\n",
      "Attempting to fetch data for Event ID: 1105 (FORMULA 1 GRAN PREMIO DE LA CIUDAD DE MÉXICO 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1105\n",
      "Successfully fetched and parsed data for Event ID: 1105\n",
      "\n",
      "Attempting to fetch data for Event ID: 1106 (FORMULA 1 MSC CRUISES GRANDE PRÊMIO DE SÃO PAULO 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1106\n",
      "Successfully fetched and parsed data for Event ID: 1106\n",
      "\n",
      "Attempting to fetch data for Event ID: 1107 (FORMULA 1 HEINEKEN LAS VEGAS GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1107\n",
      "Successfully fetched and parsed data for Event ID: 1107\n",
      "\n",
      "Attempting to fetch data for Event ID: 1108 (FORMULA 1 QATAR AIRWAYS QATAR GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1108\n",
      "Successfully fetched and parsed data for Event ID: 1108\n",
      "\n",
      "Attempting to fetch data for Event ID: 1109 (FORMULA 1 ETIHAD AIRWAYS ABU DHABI GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1109\n",
      "Successfully fetched and parsed data for Event ID: 1109\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully fetched data for 24 events.\n",
      "Failed to fetch data for 0 events.\n",
      "\n",
      "Processing event: FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_LOUIS_VUITTON_AUSTRALIAN.json\n",
      "\n",
      "Processing event: FORMULA 1 HEINEKEN CHINESE GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_HEINEKEN_CHINESE.json\n",
      "\n",
      "Processing event: FORMULA 1 LENOVO JAPANESE GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_LENOVO_JAPANESE.json\n",
      "\n",
      "Processing event: FORMULA 1 GULF AIR BAHRAIN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_GULF_AIR_BAHRAIN.json\n",
      "\n",
      "Processing event: FORMULA 1 STC SAUDI ARABIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_STC_SAUDI_ARABIAN.json\n",
      "\n",
      "Processing event: FORMULA 1 CRYPTO.COM MIAMI GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_COM_MIAMI.json\n",
      "\n",
      "Processing event: FORMULA 1 AWS GRAN PREMIO DEL MADE IN ITALY E DELL'EMILIA-ROMAGNA 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_.json\n",
      "\n",
      "Processing event: FORMULA 1 TAG HEUER GRAND PRIX DE MONACO 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_TAG_HEUER.json\n",
      "\n",
      "Processing event: FORMULA 1 ARAMCO GRAN PREMIO DE ESPAÑA 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_.json\n",
      "\n",
      "Processing event: FORMULA 1 PIRELLI GRAND PRIX DU CANADA 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_PIRELLI.json\n",
      "\n",
      "Processing event: FORMULA 1 MSC CRUISES AUSTRIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_MSC_CRUISES_AUSTRIAN.json\n",
      "\n",
      "Processing event: FORMULA 1 QATAR AIRWAYS BRITISH GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_QATAR_AIRWAYS_BRITISH.json\n",
      "\n",
      "Processing event: FORMULA 1 MOËT & CHANDON BELGIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_CHANDON_BELGIAN.json\n",
      "\n",
      "Processing event: FORMULA 1 LENOVO HUNGARIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_LENOVO_HUNGARIAN.json\n",
      "\n",
      "Processing event: FORMULA 1 HEINEKEN DUTCH GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_HEINEKEN_DUTCH.json\n",
      "\n",
      "Processing event: FORMULA 1 PIRELLI GRAN PREMIO D’ITALIA 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_.json\n",
      "\n",
      "Processing event: FORMULA 1 QATAR AIRWAYS AZERBAIJAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_QATAR_AIRWAYS_AZERBAIJAN.json\n",
      "\n",
      "Processing event: FORMULA 1 SINGAPORE AIRLINES SINGAPORE GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_SINGAPORE_AIRLINES_SINGAPORE.json\n",
      "\n",
      "Processing event: FORMULA 1 MSC CRUISES UNITED STATES GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_MSC_CRUISES_UNITED_STATES.json\n",
      "\n",
      "Processing event: FORMULA 1 GRAN PREMIO DE LA CIUDAD DE MÉXICO 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_.json\n",
      "\n",
      "Processing event: FORMULA 1 MSC CRUISES GRANDE PRÊMIO DE SÃO PAULO 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_.json\n",
      "\n",
      "Processing event: FORMULA 1 HEINEKEN LAS VEGAS GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_HEINEKEN_LAS_VEGAS.json\n",
      "\n",
      "Processing event: FORMULA 1 QATAR AIRWAYS QATAR GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_QATAR_AIRWAYS_QATAR.json\n",
      "\n",
      "Processing event: FORMULA 1 ETIHAD AIRWAYS ABU DHABI GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to race_data/2025_ETIHAD_AIRWAYS_ABU_DHABI.json\n",
      "\n",
      "--- JSON Export Complete ---\n",
      "Successfully saved 24 event data files to the 'race_data' directory.\n",
      "Files saved:\n",
      "  - race_data/2025_LOUIS_VUITTON_AUSTRALIAN.json\n",
      "  - race_data/2025_HEINEKEN_CHINESE.json\n",
      "  - race_data/2025_LENOVO_JAPANESE.json\n",
      "  - race_data/2025_GULF_AIR_BAHRAIN.json\n",
      "  - race_data/2025_STC_SAUDI_ARABIAN.json\n",
      "  - race_data/2025_COM_MIAMI.json\n",
      "  - race_data/2025_.json\n",
      "  - race_data/2025_TAG_HEUER.json\n",
      "  - race_data/2025_.json\n",
      "  - race_data/2025_PIRELLI.json\n",
      "  - race_data/2025_MSC_CRUISES_AUSTRIAN.json\n",
      "  - race_data/2025_QATAR_AIRWAYS_BRITISH.json\n",
      "  - race_data/2025_CHANDON_BELGIAN.json\n",
      "  - race_data/2025_LENOVO_HUNGARIAN.json\n",
      "  - race_data/2025_HEINEKEN_DUTCH.json\n",
      "  - race_data/2025_.json\n",
      "  - race_data/2025_QATAR_AIRWAYS_AZERBAIJAN.json\n",
      "  - race_data/2025_SINGAPORE_AIRLINES_SINGAPORE.json\n",
      "  - race_data/2025_MSC_CRUISES_UNITED_STATES.json\n",
      "  - race_data/2025_.json\n",
      "  - race_data/2025_.json\n",
      "  - race_data/2025_HEINEKEN_LAS_VEGAS.json\n",
      "  - race_data/2025_QATAR_AIRWAYS_QATAR.json\n",
      "  - race_data/2025_ETIHAD_AIRWAYS_ABU_DHABI.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "\n",
    "# Configuration constants\n",
    "DEFAULT_TIMEOUT = 10\n",
    "EVENT_DATA_URL = \"https://inmotion.dhl/api/f1-award-element-data/6367\"\n",
    "EVENT_SPECIFIC_URL = \"https://inmotion.dhl/api/f1-award-element-data/6365\"\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DELAY_BETWEEN_REQUESTS = 0.5  # seconds\n",
    "FUEL_CORRECTION_FACTOR = 0.03\n",
    "INITIAL_FUEL_LOAD = {\"Race\": 100, \"Sprint\": 30}\n",
    "\n",
    "\n",
    "class F1DataFetcher:\n",
    "    \"\"\"Class for fetching and processing Formula 1 data.\"\"\"\n",
    "\n",
    "    def __init__(self, timeout: int = DEFAULT_TIMEOUT, headers: Dict = None):\n",
    "        \"\"\"\n",
    "        Initialize the F1DataFetcher.\n",
    "\n",
    "        Args:\n",
    "            timeout: Request timeout in seconds\n",
    "            headers: HTTP headers for requests\n",
    "        \"\"\"\n",
    "        self.timeout = timeout\n",
    "        self.headers = headers or DEFAULT_HEADERS\n",
    "        self.event_data_cache = {}\n",
    "        self.event_specific_data_cache = {}\n",
    "\n",
    "    def fetch_events_data(self, url: str = EVENT_DATA_URL) -> Dict:\n",
    "        \"\"\"\n",
    "        Fetch events data from the specified URL.\n",
    "\n",
    "        Args:\n",
    "            url: URL to fetch data from\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing events data\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "            json.JSONDecodeError: If response is not valid JSON\n",
    "        \"\"\"\n",
    "        print(f\"Attempting to fetch data from: {url}\")\n",
    "\n",
    "        try:\n",
    "            response = self._make_request(url)\n",
    "            parsed_data = response.json()\n",
    "\n",
    "            # Extract events list from the nested structure\n",
    "            data_section = parsed_data.get(\"data\", {})\n",
    "            chart_section = data_section.get(\"chart\", {})\n",
    "            events_data = chart_section.get(\"events\", [])\n",
    "\n",
    "            if events_data:\n",
    "                print(\"\\nSuccessfully extracted events data\")\n",
    "                return events_data\n",
    "            else:\n",
    "                print(\"\\nError: Could not find the 'events' data at the expected path\")\n",
    "                return []\n",
    "\n",
    "        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
    "            print(f\"\\nError fetching events data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_event_specific_data(self, events_data: List[Dict],\n",
    "                                  base_url: str = EVENT_SPECIFIC_URL) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch specific data for each event.\n",
    "\n",
    "        Args:\n",
    "            events_data: List of event dictionaries\n",
    "            base_url: Base URL for event-specific data\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping event IDs to their specific data\n",
    "        \"\"\"\n",
    "        all_event_specific_data = {}\n",
    "\n",
    "        print(f\"Found {len(events_data)} events to process.\")\n",
    "\n",
    "        for event in events_data:\n",
    "            event_id = event.get(\"id\")\n",
    "            event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "            if not event_id:\n",
    "                print(f\"Warning: Skipping event with missing ID: {event_title}\")\n",
    "                continue\n",
    "\n",
    "            # Check cache first\n",
    "            if event_id in self.event_specific_data_cache:\n",
    "                all_event_specific_data[event_id] = self.event_specific_data_cache[event_id]\n",
    "                print(f\"Using cached data for Event ID: {event_id} ({event_title})\")\n",
    "                continue\n",
    "\n",
    "            specific_url = f\"{base_url}?event={event_id}\"\n",
    "            print(f\"\\nAttempting to fetch data for Event ID: {event_id} ({event_title})\")\n",
    "            print(f\"URL: {specific_url}\")\n",
    "\n",
    "            try:\n",
    "                response = self._make_request(specific_url)\n",
    "                event_specific_data = response.json()\n",
    "\n",
    "                # Cache the result\n",
    "                self.event_specific_data_cache[event_id] = event_specific_data\n",
    "                all_event_specific_data[event_id] = event_specific_data\n",
    "\n",
    "                print(f\"Successfully fetched and parsed data for Event ID: {event_id}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                error_info = {\"error\": str(e)}\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error during request for Event ID: {event_id}: {e}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                error_info = {\n",
    "                    \"error\": \"JSONDecodeError\",\n",
    "                    \"response_text\": response.text[:500] if hasattr(response, 'text') else \"No response text\"\n",
    "                }\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error: Failed to decode JSON for Event ID: {event_id}\")\n",
    "\n",
    "            # Add delay between requests\n",
    "            if DELAY_BETWEEN_REQUESTS > 0:\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        # Print summary\n",
    "        self._print_fetch_summary(events_data, all_event_specific_data)\n",
    "        return all_event_specific_data\n",
    "\n",
    "    def _make_request(self, url: str) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Make an HTTP request with error handling.\n",
    "\n",
    "        Args:\n",
    "            url: URL to request\n",
    "\n",
    "        Returns:\n",
    "            Response object\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Error: The request to {url} timed out.\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Error during request to {url}\")\n",
    "            raise\n",
    "\n",
    "    def _print_fetch_summary(self, events_data: List[Dict],\n",
    "                            all_event_specific_data: Dict[str, Any]) -> None:\n",
    "        \"\"\"Print a summary of the fetch operation.\"\"\"\n",
    "        print(\"\\n--- Processing Complete ---\")\n",
    "        successful_fetches = sum(\n",
    "            1 for data in all_event_specific_data.values()\n",
    "            if isinstance(data, dict) and \"error\" not in data\n",
    "        )\n",
    "        failed_fetches = len(events_data) - successful_fetches\n",
    "        print(f\"Successfully fetched data for {successful_fetches} events.\")\n",
    "        print(f\"Failed to fetch data for {failed_fetches} events.\")\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Class for processing F1 data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def html_table_to_dataframe(event_json_data: Dict) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract HTML table from event JSON data and convert to DataFrame.\n",
    "\n",
    "        Args:\n",
    "            event_json_data: JSON data dictionary for a specific event\n",
    "\n",
    "        Returns:\n",
    "            DataFrame created from HTML table, or None if extraction fails\n",
    "        \"\"\"\n",
    "        if not isinstance(event_json_data, dict):\n",
    "            print(\"Error: Input must be a dictionary.\")\n",
    "            return None\n",
    "\n",
    "        # Extract HTML table string\n",
    "        html_table_str = event_json_data.get(\"htmlList\", {}).get(\"table\")\n",
    "\n",
    "        if not html_table_str:\n",
    "            print(\"Error: Could not find 'htmlList' -> 'table' in the provided JSON data or it's empty.\")\n",
    "            return None\n",
    "\n",
    "        if not isinstance(html_table_str, str):\n",
    "            print(\"Error: The value at ['htmlList']['table'] is not a string.\")\n",
    "            return None\n",
    "\n",
    "        print(\"Found HTML table string. Attempting to parse...\")\n",
    "        try:\n",
    "            # Parse HTML table into DataFrame\n",
    "            list_of_dfs = pd.read_html(io.StringIO(html_table_str))\n",
    "\n",
    "            if list_of_dfs:\n",
    "                print(\"Successfully parsed HTML table into DataFrame.\")\n",
    "                return list_of_dfs[0]\n",
    "            else:\n",
    "                print(\"Warning: No tables found by pd.read_html, although HTML string was present.\")\n",
    "                return None\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error parsing HTML with pandas (ValueError): {ve}\")\n",
    "            print(\"Check if the HTML string actually contains a <table> tag.\")\n",
    "            return None\n",
    "        except ImportError:\n",
    "            print(\"Error: The 'lxml' library might be required by pd.read_html. Please install it (`pip install lxml`).\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during HTML parsing: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dataframe_to_json(df: pd.DataFrame, event_title: str, year: int = 2025, output_dir: str = \"race_data\") -> str:\n",
    "        \"\"\"\n",
    "        Save DataFrame to a JSON file with a standardized filename.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            event_title: Title of the event (e.g., \"FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\")\n",
    "            year: Year of the event\n",
    "            output_dir: Directory to save the JSON file\n",
    "\n",
    "        Returns:\n",
    "            Path to the saved JSON file\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            print(f\"Error: Cannot save None DataFrame for {event_title}\")\n",
    "            return \"\"\n",
    "\n",
    "        # Extract the Grand Prix name from the title\n",
    "        # Example: \"FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\" -> \"Australian_Grand_Prix\"\n",
    "        match = re.search(r'([A-Z]+(?:\\s+[A-Z]+)*)\\s+GRAND\\s+PRIX', event_title, re.IGNORECASE)\n",
    "        if match:\n",
    "            grand_prix_name = match.group(1).strip()\n",
    "        else:\n",
    "            # Fallback: extract words between the sponsor and the year\n",
    "            parts = event_title.split()\n",
    "            if len(parts) >= 3:\n",
    "                # Try to find the Grand Prix name by removing common elements\n",
    "                filtered_parts = [p for p in parts if p not in [\"FORMULA\", \"1\", \"GRAND\", \"PRIX\", str(year)]]\n",
    "                # Remove likely sponsor names (all caps words at the beginning)\n",
    "                while filtered_parts and filtered_parts[0].isupper():\n",
    "                    filtered_parts.pop(0)\n",
    "                grand_prix_name = \" \".join(filtered_parts[:2])  # Take the first two remaining words\n",
    "            else:\n",
    "                grand_prix_name = \"Unknown_Grand_Prix\"\n",
    "\n",
    "        # Clean up the name and replace spaces with underscores\n",
    "        grand_prix_name = re.sub(r'[^\\w\\s]', '', grand_prix_name)  # Remove special characters\n",
    "        grand_prix_name = grand_prix_name.replace(' ', '_')\n",
    "\n",
    "        # Create the filename\n",
    "        filename = f\"{year}_{grand_prix_name}.json\"\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save the DataFrame to a JSON file\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        df.to_json(file_path, orient=\"records\", indent=4)\n",
    "\n",
    "        print(f\"Saved data to {file_path}\")\n",
    "        return file_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate the usage of the classes.\"\"\"\n",
    "    # Initialize the data fetcher\n",
    "    fetcher = F1DataFetcher()\n",
    "\n",
    "    # Fetch events data\n",
    "    events_data = fetcher.fetch_events_data()\n",
    "    if not events_data:\n",
    "        print(\"No events data found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Fetch specific data for each event\n",
    "    all_event_specific_data = fetcher.fetch_event_specific_data(events_data)\n",
    "\n",
    "    # Create a DataProcessor instance\n",
    "    processor = DataProcessor()\n",
    "\n",
    "    # Create a directory to store the JSON files\n",
    "    output_dir = \"race_data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each event and save to JSON\n",
    "    saved_files = []\n",
    "    for event in events_data:\n",
    "        event_id = event.get(\"id\")\n",
    "        event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "        if not event_id or event_id not in all_event_specific_data:\n",
    "            print(f\"Skipping event {event_title}: No data available\")\n",
    "            continue\n",
    "\n",
    "        event_data = all_event_specific_data[event_id]\n",
    "\n",
    "        # Check if there was an error fetching this event's data\n",
    "        if isinstance(event_data, dict) and \"error\" in event_data:\n",
    "            print(f\"Skipping event {event_title}: Error in data - {event_data.get('error')}\")\n",
    "            continue\n",
    "\n",
    "        # Convert HTML table to DataFrame\n",
    "        print(f\"\\nProcessing event: {event_title}\")\n",
    "        event_dataframe = processor.html_table_to_dataframe(event_data)\n",
    "\n",
    "        if event_dataframe is not None:\n",
    "            # Extract year from title or use default\n",
    "            year_match = re.search(r'(\\d{4})', event_title)\n",
    "            year = int(year_match.group(1)) if year_match else 2025\n",
    "\n",
    "            # Save DataFrame to JSON\n",
    "            file_path = processor.save_dataframe_to_json(\n",
    "                event_dataframe,\n",
    "                event_title,\n",
    "                year,\n",
    "                output_dir\n",
    "            )\n",
    "\n",
    "            if file_path:\n",
    "                saved_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Failed to create DataFrame for {event_title}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n--- JSON Export Complete ---\")\n",
    "    print(f\"Successfully saved {len(saved_files)} event data files to the '{output_dir}' directory.\")\n",
    "    if saved_files:\n",
    "        print(\"Files saved:\")\n",
    "        for file_path in saved_files:\n",
    "            print(f\"  - {file_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully extracted events data\n",
      "Found 24 events to process.\n",
      "\n",
      "Attempting to fetch data for Event ID: 1086 (FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1086\n",
      "Successfully fetched and parsed data for Event ID: 1086\n",
      "\n",
      "Attempting to fetch data for Event ID: 1087 (FORMULA 1 HEINEKEN CHINESE GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1087\n",
      "Successfully fetched and parsed data for Event ID: 1087\n",
      "\n",
      "Attempting to fetch data for Event ID: 1088 (FORMULA 1 LENOVO JAPANESE GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1088\n",
      "Successfully fetched and parsed data for Event ID: 1088\n",
      "\n",
      "Attempting to fetch data for Event ID: 1089 (FORMULA 1 GULF AIR BAHRAIN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1089\n",
      "Successfully fetched and parsed data for Event ID: 1089\n",
      "\n",
      "Attempting to fetch data for Event ID: 1090 (FORMULA 1 STC SAUDI ARABIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1090\n",
      "Successfully fetched and parsed data for Event ID: 1090\n",
      "\n",
      "Attempting to fetch data for Event ID: 1091 (FORMULA 1 CRYPTO.COM MIAMI GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1091\n",
      "Successfully fetched and parsed data for Event ID: 1091\n",
      "\n",
      "Attempting to fetch data for Event ID: 1092 (FORMULA 1 AWS GRAN PREMIO DEL MADE IN ITALY E DELL'EMILIA-ROMAGNA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1092\n",
      "Successfully fetched and parsed data for Event ID: 1092\n",
      "\n",
      "Attempting to fetch data for Event ID: 1093 (FORMULA 1 TAG HEUER GRAND PRIX DE MONACO 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1093\n",
      "Successfully fetched and parsed data for Event ID: 1093\n",
      "\n",
      "Attempting to fetch data for Event ID: 1094 (FORMULA 1 ARAMCO GRAN PREMIO DE ESPAÑA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1094\n",
      "Successfully fetched and parsed data for Event ID: 1094\n",
      "\n",
      "Attempting to fetch data for Event ID: 1095 (FORMULA 1 PIRELLI GRAND PRIX DU CANADA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1095\n",
      "Successfully fetched and parsed data for Event ID: 1095\n",
      "\n",
      "Attempting to fetch data for Event ID: 1096 (FORMULA 1 MSC CRUISES AUSTRIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1096\n",
      "Successfully fetched and parsed data for Event ID: 1096\n",
      "\n",
      "Attempting to fetch data for Event ID: 1097 (FORMULA 1 QATAR AIRWAYS BRITISH GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1097\n",
      "Successfully fetched and parsed data for Event ID: 1097\n",
      "\n",
      "Attempting to fetch data for Event ID: 1098 (FORMULA 1 MOËT & CHANDON BELGIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1098\n",
      "Successfully fetched and parsed data for Event ID: 1098\n",
      "\n",
      "Attempting to fetch data for Event ID: 1099 (FORMULA 1 LENOVO HUNGARIAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1099\n",
      "Successfully fetched and parsed data for Event ID: 1099\n",
      "\n",
      "Attempting to fetch data for Event ID: 1100 (FORMULA 1 HEINEKEN DUTCH GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1100\n",
      "Successfully fetched and parsed data for Event ID: 1100\n",
      "\n",
      "Attempting to fetch data for Event ID: 1101 (FORMULA 1 PIRELLI GRAN PREMIO D’ITALIA 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1101\n",
      "Successfully fetched and parsed data for Event ID: 1101\n",
      "\n",
      "Attempting to fetch data for Event ID: 1102 (FORMULA 1 QATAR AIRWAYS AZERBAIJAN GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1102\n",
      "Successfully fetched and parsed data for Event ID: 1102\n",
      "\n",
      "Attempting to fetch data for Event ID: 1103 (FORMULA 1 SINGAPORE AIRLINES SINGAPORE GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1103\n",
      "Successfully fetched and parsed data for Event ID: 1103\n",
      "\n",
      "Attempting to fetch data for Event ID: 1104 (FORMULA 1 MSC CRUISES UNITED STATES GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1104\n",
      "Successfully fetched and parsed data for Event ID: 1104\n",
      "\n",
      "Attempting to fetch data for Event ID: 1105 (FORMULA 1 GRAN PREMIO DE LA CIUDAD DE MÉXICO 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1105\n",
      "Successfully fetched and parsed data for Event ID: 1105\n",
      "\n",
      "Attempting to fetch data for Event ID: 1106 (FORMULA 1 MSC CRUISES GRANDE PRÊMIO DE SÃO PAULO 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1106\n",
      "Successfully fetched and parsed data for Event ID: 1106\n",
      "\n",
      "Attempting to fetch data for Event ID: 1107 (FORMULA 1 HEINEKEN LAS VEGAS GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1107\n",
      "Successfully fetched and parsed data for Event ID: 1107\n",
      "\n",
      "Attempting to fetch data for Event ID: 1108 (FORMULA 1 QATAR AIRWAYS QATAR GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1108\n",
      "Successfully fetched and parsed data for Event ID: 1108\n",
      "\n",
      "Attempting to fetch data for Event ID: 1109 (FORMULA 1 ETIHAD AIRWAYS ABU DHABI GRAND PRIX 2025)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6365?event=1109\n",
      "Successfully fetched and parsed data for Event ID: 1109\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully fetched data for 24 events.\n",
      "Failed to fetch data for 0 events.\n",
      "\n",
      "Processing event: FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Australian Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 HEINEKEN CHINESE GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Chinese Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 LENOVO JAPANESE GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Japanese Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 GULF AIR BAHRAIN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Bahrain Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 STC SAUDI ARABIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Saudi Arabian Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 CRYPTO.COM MIAMI GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Miami Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 AWS GRAN PREMIO DEL MADE IN ITALY E DELL'EMILIA-ROMAGNA 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Emilia Romagna Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 TAG HEUER GRAND PRIX DE MONACO 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Monaco Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 ARAMCO GRAN PREMIO DE ESPAÑA 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Spanish Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 PIRELLI GRAND PRIX DU CANADA 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Canadian Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 MSC CRUISES AUSTRIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Austrian Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 QATAR AIRWAYS BRITISH GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/British Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 MOËT & CHANDON BELGIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Belgian Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 LENOVO HUNGARIAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Hungarian Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 HEINEKEN DUTCH GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Dutch Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 PIRELLI GRAN PREMIO D’ITALIA 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Italian Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 QATAR AIRWAYS AZERBAIJAN GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Azerbaijan Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 SINGAPORE AIRLINES SINGAPORE GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Singapore Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 MSC CRUISES UNITED STATES GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/United States Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 GRAN PREMIO DE LA CIUDAD DE MÉXICO 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Mexico City Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 MSC CRUISES GRANDE PRÊMIO DE SÃO PAULO 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/São Paulo Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 HEINEKEN LAS VEGAS GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Las Vegas Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 QATAR AIRWAYS QATAR GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Qatar Grand Prix.json\n",
      "\n",
      "Processing event: FORMULA 1 ETIHAD AIRWAYS ABU DHABI GRAND PRIX 2025\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2025/Abu Dhabi Grand Prix.json\n",
      "\n",
      "--- JSON Export Complete ---\n",
      "Successfully saved 24 event data files to the '2025' directory.\n",
      "Files saved:\n",
      "  - 2025/Australian Grand Prix.json\n",
      "  - 2025/Chinese Grand Prix.json\n",
      "  - 2025/Japanese Grand Prix.json\n",
      "  - 2025/Bahrain Grand Prix.json\n",
      "  - 2025/Saudi Arabian Grand Prix.json\n",
      "  - 2025/Miami Grand Prix.json\n",
      "  - 2025/Emilia Romagna Grand Prix.json\n",
      "  - 2025/Monaco Grand Prix.json\n",
      "  - 2025/Spanish Grand Prix.json\n",
      "  - 2025/Canadian Grand Prix.json\n",
      "  - 2025/Austrian Grand Prix.json\n",
      "  - 2025/British Grand Prix.json\n",
      "  - 2025/Belgian Grand Prix.json\n",
      "  - 2025/Hungarian Grand Prix.json\n",
      "  - 2025/Dutch Grand Prix.json\n",
      "  - 2025/Italian Grand Prix.json\n",
      "  - 2025/Azerbaijan Grand Prix.json\n",
      "  - 2025/Singapore Grand Prix.json\n",
      "  - 2025/United States Grand Prix.json\n",
      "  - 2025/Mexico City Grand Prix.json\n",
      "  - 2025/São Paulo Grand Prix.json\n",
      "  - 2025/Las Vegas Grand Prix.json\n",
      "  - 2025/Qatar Grand Prix.json\n",
      "  - 2025/Abu Dhabi Grand Prix.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "\n",
    "# Configuration constants\n",
    "DEFAULT_TIMEOUT = 10\n",
    "EVENT_DATA_URL = \"https://inmotion.dhl/api/f1-award-element-data/6367\"\n",
    "EVENT_SPECIFIC_URL = \"https://inmotion.dhl/api/f1-award-element-data/6365\"\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DELAY_BETWEEN_REQUESTS = 0.5  # seconds\n",
    "FUEL_CORRECTION_FACTOR = 0.03\n",
    "INITIAL_FUEL_LOAD = {\"Race\": 100, \"Sprint\": 30}\n",
    "\n",
    "# 2025 F1 Race Calendar - Official race names without sponsors\n",
    "F1_RACES_2025 = {\n",
    "    \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "    \"CHINESE\": \"Chinese Grand Prix\",\n",
    "    \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "    \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "    \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "    \"MIAMI\": \"Miami Grand Prix\",\n",
    "    \"EMILIA-ROMAGNA\": \"Emilia Romagna Grand Prix\",\n",
    "    \"MONACO\": \"Monaco Grand Prix\",\n",
    "    \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "    \"CANADA\": \"Canadian Grand Prix\",\n",
    "    \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "    \"BRITISH\": \"British Grand Prix\",\n",
    "    \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "    \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "    \"DUTCH\": \"Dutch Grand Prix\",\n",
    "    \"ITALIA\": \"Italian Grand Prix\",\n",
    "    \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "    \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "    \"UNITED STATES\": \"United States Grand Prix\",\n",
    "    \"MÉXICO\": \"Mexico City Grand Prix\",\n",
    "    \"SÃO PAULO\": \"São Paulo Grand Prix\",\n",
    "    \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "    \"QATAR\": \"Qatar Grand Prix\",\n",
    "    \"ABU DHABI\": \"Abu Dhabi Grand Prix\"\n",
    "}\n",
    "\n",
    "\n",
    "class F1DataFetcher:\n",
    "    \"\"\"Class for fetching and processing Formula 1 data.\"\"\"\n",
    "\n",
    "    def __init__(self, timeout: int = DEFAULT_TIMEOUT, headers: Dict = None):\n",
    "        \"\"\"\n",
    "        Initialize the F1DataFetcher.\n",
    "\n",
    "        Args:\n",
    "            timeout: Request timeout in seconds\n",
    "            headers: HTTP headers for requests\n",
    "        \"\"\"\n",
    "        self.timeout = timeout\n",
    "        self.headers = headers or DEFAULT_HEADERS\n",
    "        self.event_data_cache = {}\n",
    "        self.event_specific_data_cache = {}\n",
    "\n",
    "    def fetch_events_data(self, url: str = EVENT_DATA_URL) -> Dict:\n",
    "        \"\"\"\n",
    "        Fetch events data from the specified URL.\n",
    "\n",
    "        Args:\n",
    "            url: URL to fetch data from\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing events data\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "            json.JSONDecodeError: If response is not valid JSON\n",
    "        \"\"\"\n",
    "        print(f\"Attempting to fetch data from: {url}\")\n",
    "\n",
    "        try:\n",
    "            response = self._make_request(url)\n",
    "            parsed_data = response.json()\n",
    "\n",
    "            # Extract events list from the nested structure\n",
    "            data_section = parsed_data.get(\"data\", {})\n",
    "            chart_section = data_section.get(\"chart\", {})\n",
    "            events_data = chart_section.get(\"events\", [])\n",
    "\n",
    "            if events_data:\n",
    "                print(\"\\nSuccessfully extracted events data\")\n",
    "                return events_data\n",
    "            else:\n",
    "                print(\"\\nError: Could not find the 'events' data at the expected path\")\n",
    "                return []\n",
    "\n",
    "        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
    "            print(f\"\\nError fetching events data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_event_specific_data(self, events_data: List[Dict],\n",
    "                                  base_url: str = EVENT_SPECIFIC_URL) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch specific data for each event.\n",
    "\n",
    "        Args:\n",
    "            events_data: List of event dictionaries\n",
    "            base_url: Base URL for event-specific data\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping event IDs to their specific data\n",
    "        \"\"\"\n",
    "        all_event_specific_data = {}\n",
    "\n",
    "        print(f\"Found {len(events_data)} events to process.\")\n",
    "\n",
    "        for event in events_data:\n",
    "            event_id = event.get(\"id\")\n",
    "            event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "            if not event_id:\n",
    "                print(f\"Warning: Skipping event with missing ID: {event_title}\")\n",
    "                continue\n",
    "\n",
    "            # Check cache first\n",
    "            if event_id in self.event_specific_data_cache:\n",
    "                all_event_specific_data[event_id] = self.event_specific_data_cache[event_id]\n",
    "                print(f\"Using cached data for Event ID: {event_id} ({event_title})\")\n",
    "                continue\n",
    "\n",
    "            specific_url = f\"{base_url}?event={event_id}\"\n",
    "            print(f\"\\nAttempting to fetch data for Event ID: {event_id} ({event_title})\")\n",
    "            print(f\"URL: {specific_url}\")\n",
    "\n",
    "            try:\n",
    "                response = self._make_request(specific_url)\n",
    "                event_specific_data = response.json()\n",
    "\n",
    "                # Cache the result\n",
    "                self.event_specific_data_cache[event_id] = event_specific_data\n",
    "                all_event_specific_data[event_id] = event_specific_data\n",
    "\n",
    "                print(f\"Successfully fetched and parsed data for Event ID: {event_id}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                error_info = {\"error\": str(e)}\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error during request for Event ID: {event_id}: {e}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                error_info = {\n",
    "                    \"error\": \"JSONDecodeError\",\n",
    "                    \"response_text\": response.text[:500] if hasattr(response, 'text') else \"No response text\"\n",
    "                }\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error: Failed to decode JSON for Event ID: {event_id}\")\n",
    "\n",
    "            # Add delay between requests\n",
    "            if DELAY_BETWEEN_REQUESTS > 0:\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        # Print summary\n",
    "        self._print_fetch_summary(events_data, all_event_specific_data)\n",
    "        return all_event_specific_data\n",
    "\n",
    "    def _make_request(self, url: str) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Make an HTTP request with error handling.\n",
    "\n",
    "        Args:\n",
    "            url: URL to request\n",
    "\n",
    "        Returns:\n",
    "            Response object\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Error: The request to {url} timed out.\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Error during request to {url}\")\n",
    "            raise\n",
    "\n",
    "    def _print_fetch_summary(self, events_data: List[Dict],\n",
    "                            all_event_specific_data: Dict[str, Any]) -> None:\n",
    "        \"\"\"Print a summary of the fetch operation.\"\"\"\n",
    "        print(\"\\n--- Processing Complete ---\")\n",
    "        successful_fetches = sum(\n",
    "            1 for data in all_event_specific_data.values()\n",
    "            if isinstance(data, dict) and \"error\" not in data\n",
    "        )\n",
    "        failed_fetches = len(events_data) - successful_fetches\n",
    "        print(f\"Successfully fetched data for {successful_fetches} events.\")\n",
    "        print(f\"Failed to fetch data for {failed_fetches} events.\")\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Class for processing F1 data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def html_table_to_dataframe(event_json_data: Dict) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract HTML table from event JSON data and convert to DataFrame.\n",
    "\n",
    "        Args:\n",
    "            event_json_data: JSON data dictionary for a specific event\n",
    "\n",
    "        Returns:\n",
    "            DataFrame created from HTML table, or None if extraction fails\n",
    "        \"\"\"\n",
    "        if not isinstance(event_json_data, dict):\n",
    "            print(\"Error: Input must be a dictionary.\")\n",
    "            return None\n",
    "\n",
    "        # Extract HTML table string\n",
    "        html_table_str = event_json_data.get(\"htmlList\", {}).get(\"table\")\n",
    "\n",
    "        if not html_table_str:\n",
    "            print(\"Error: Could not find 'htmlList' -> 'table' in the provided JSON data or it's empty.\")\n",
    "            return None\n",
    "\n",
    "        if not isinstance(html_table_str, str):\n",
    "            print(\"Error: The value at ['htmlList']['table'] is not a string.\")\n",
    "            return None\n",
    "\n",
    "        print(\"Found HTML table string. Attempting to parse...\")\n",
    "        try:\n",
    "            # Parse HTML table into DataFrame\n",
    "            list_of_dfs = pd.read_html(io.StringIO(html_table_str))\n",
    "\n",
    "            if list_of_dfs:\n",
    "                print(\"Successfully parsed HTML table into DataFrame.\")\n",
    "                return list_of_dfs[0]\n",
    "            else:\n",
    "                print(\"Warning: No tables found by pd.read_html, although HTML string was present.\")\n",
    "                return None\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error parsing HTML with pandas (ValueError): {ve}\")\n",
    "            print(\"Check if the HTML string actually contains a <table> tag.\")\n",
    "            return None\n",
    "        except ImportError:\n",
    "            print(\"Error: The 'lxml' library might be required by pd.read_html. Please install it (`pip install lxml`).\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during HTML parsing: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_fuel_correction(lap_time: float, lap_number: int,\n",
    "                             total_laps: int, session_type: str) -> float:\n",
    "        \"\"\"\n",
    "        Apply fuel correction to lap time.\n",
    "\n",
    "        Args:\n",
    "            lap_time: Original lap time in seconds\n",
    "            lap_number: Current lap number\n",
    "            total_laps: Total number of laps in the session\n",
    "            session_type: Type of session (Race, Sprint, etc.)\n",
    "\n",
    "        Returns:\n",
    "            Corrected lap time\n",
    "        \"\"\"\n",
    "        initial_fuel = INITIAL_FUEL_LOAD.get(session_type, 0)\n",
    "        if initial_fuel == 0:\n",
    "            return lap_time\n",
    "\n",
    "        fuel_remaining = initial_fuel * (1 - (lap_number / total_laps))\n",
    "        fuel_penalty = fuel_remaining * FUEL_CORRECTION_FACTOR\n",
    "        return lap_time - fuel_penalty\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dataframe_to_json(df: pd.DataFrame, event_title: str, year: int = 2025, output_dir: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Save DataFrame to a JSON file with a standardized filename.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            event_title: Title of the event (e.g., \"FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\")\n",
    "            year: Year of the event\n",
    "            output_dir: Directory to save the JSON file (defaults to year folder)\n",
    "\n",
    "        Returns:\n",
    "            Path to the saved JSON file\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            print(f\"Error: Cannot save None DataFrame for {event_title}\")\n",
    "            return \"\"\n",
    "\n",
    "        # If no output directory is specified, use the year as the directory name\n",
    "        if output_dir is None:\n",
    "            output_dir = str(year)\n",
    "\n",
    "        # Find the proper race name from the event title\n",
    "        race_name = None\n",
    "\n",
    "        # First, try to match with our predefined race names\n",
    "        for key, proper_name in F1_RACES_2025.items():\n",
    "            if key in event_title.upper():\n",
    "                race_name = proper_name\n",
    "                break\n",
    "\n",
    "        # If no match found, try to extract from the title\n",
    "        if race_name is None:\n",
    "            # Try to extract Grand Prix name using regex\n",
    "            match = re.search(r'([A-Z]+(?:\\s+[A-Z]+)*)\\s+GRAND\\s+PRIX', event_title, re.IGNORECASE)\n",
    "            if match:\n",
    "                location = match.group(1).strip()\n",
    "                race_name = f\"{location} Grand Prix\"\n",
    "            else:\n",
    "                # Fallback to a generic name with the event ID\n",
    "                race_name = \"Unknown Grand Prix\"\n",
    "                print(f\"Warning: Could not determine race name for: {event_title}\")\n",
    "\n",
    "        # # Clean up the filename - remove special characters and replace spaces with underscores\n",
    "        filename = race_name\n",
    "        # filename = race_name.replace(' ', '_')\n",
    "        # filename = re.sub(r'[^\\w\\s_-]', '', filename)  # Remove special characters except underscores and hyphens\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save the DataFrame to a JSON file\n",
    "        file_path = os.path.join(output_dir, f\"{filename}.json\")\n",
    "        df.to_json(file_path, orient=\"records\", indent=4)\n",
    "\n",
    "        print(f\"Saved data to {file_path}\")\n",
    "        return file_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate the usage of the classes.\"\"\"\n",
    "    # Initialize the data fetcher\n",
    "    fetcher = F1DataFetcher()\n",
    "\n",
    "    # Fetch events data\n",
    "    events_data = fetcher.fetch_events_data()\n",
    "    if not events_data:\n",
    "        print(\"No events data found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Fetch specific data for each event\n",
    "    all_event_specific_data = fetcher.fetch_event_specific_data(events_data)\n",
    "\n",
    "    # Create a DataProcessor instance\n",
    "    processor = DataProcessor()\n",
    "\n",
    "    # Process each event and save to JSON\n",
    "    saved_files = []\n",
    "    year = 2025  # Default year\n",
    "    output_dir = str(year)  # Use year as directory name\n",
    "\n",
    "    for event in events_data:\n",
    "        event_id = event.get(\"id\")\n",
    "        event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "        if not event_id or event_id not in all_event_specific_data:\n",
    "            print(f\"Skipping event {event_title}: No data available\")\n",
    "            continue\n",
    "\n",
    "        event_data = all_event_specific_data[event_id]\n",
    "\n",
    "        # Check if there was an error fetching this event's data\n",
    "        if isinstance(event_data, dict) and \"error\" in event_data:\n",
    "            print(f\"Skipping event {event_title}: Error in data - {event_data.get('error')}\")\n",
    "            continue\n",
    "\n",
    "        # Convert HTML table to DataFrame\n",
    "        print(f\"\\nProcessing event: {event_title}\")\n",
    "        event_dataframe = processor.html_table_to_dataframe(event_data)\n",
    "\n",
    "        if event_dataframe is not None:\n",
    "            # Extract year from title or use default\n",
    "            year_match = re.search(r'(\\d{4})', event_title)\n",
    "            if year_match:\n",
    "                year = int(year_match.group(1))\n",
    "                output_dir = str(year)  # Update output directory based on year\n",
    "\n",
    "            # Save DataFrame to JSON\n",
    "            file_path = processor.save_dataframe_to_json(\n",
    "                event_dataframe,\n",
    "                event_title,\n",
    "                year,\n",
    "                output_dir\n",
    "            )\n",
    "\n",
    "            if file_path:\n",
    "                saved_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Failed to create DataFrame for {event_title}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n--- JSON Export Complete ---\")\n",
    "    print(f\"Successfully saved {len(saved_files)} event data files to the '{output_dir}' directory.\")\n",
    "    if saved_files:\n",
    "        print(\"Files saved:\")\n",
    "        for file_path in saved_files:\n",
    "            print(f\"  - {file_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching F1 data for year: 2024\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6276\n",
      "\n",
      "Successfully extracted events data\n",
      "Found 24 events to process.\n",
      "\n",
      "Attempting to fetch data for Event ID: 1016 (Formula 1 Gulf Air Bahrain Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1016\n",
      "Successfully fetched and parsed data for Event ID: 1016\n",
      "\n",
      "Attempting to fetch data for Event ID: 1017 (Formula 1 stc Saudi Arabian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1017\n",
      "Successfully fetched and parsed data for Event ID: 1017\n",
      "\n",
      "Attempting to fetch data for Event ID: 1018 (Formula 1 Rolex Australian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1018\n",
      "Successfully fetched and parsed data for Event ID: 1018\n",
      "\n",
      "Attempting to fetch data for Event ID: 1019 (Formula 1 MSC Cruises Japanese Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1019\n",
      "Successfully fetched and parsed data for Event ID: 1019\n",
      "\n",
      "Attempting to fetch data for Event ID: 1020 (Formula 1 Lenovo Chinese Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1020\n",
      "Successfully fetched and parsed data for Event ID: 1020\n",
      "\n",
      "Attempting to fetch data for Event ID: 1021 (Formula 1 Crypto.com Miami Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1021\n",
      "Successfully fetched and parsed data for Event ID: 1021\n",
      "\n",
      "Attempting to fetch data for Event ID: 1022 (Formula 1 MSC Cruises Gran Premio dell'Emilia-Romagna 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1022\n",
      "Successfully fetched and parsed data for Event ID: 1022\n",
      "\n",
      "Attempting to fetch data for Event ID: 1023 (Formula 1 Grand Prix de Monaco 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1023\n",
      "Successfully fetched and parsed data for Event ID: 1023\n",
      "\n",
      "Attempting to fetch data for Event ID: 1024 (Formula 1 AWS Grand Prix du Canada 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1024\n",
      "Successfully fetched and parsed data for Event ID: 1024\n",
      "\n",
      "Attempting to fetch data for Event ID: 1025 (Formula 1 Aramco Grand Premio de España)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1025\n",
      "Successfully fetched and parsed data for Event ID: 1025\n",
      "\n",
      "Attempting to fetch data for Event ID: 1026 (Formula 1 Qatar Airways Austrian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1026\n",
      "Successfully fetched and parsed data for Event ID: 1026\n",
      "\n",
      "Attempting to fetch data for Event ID: 1027 (Formula 1 Qatar Airways British Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1027\n",
      "Successfully fetched and parsed data for Event ID: 1027\n",
      "\n",
      "Attempting to fetch data for Event ID: 1028 (Formula 1 Hungarian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1028\n",
      "Successfully fetched and parsed data for Event ID: 1028\n",
      "\n",
      "Attempting to fetch data for Event ID: 1029 (Formula 1 Rolex Belgian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1029\n",
      "Successfully fetched and parsed data for Event ID: 1029\n",
      "\n",
      "Attempting to fetch data for Event ID: 1030 (Formula 1 Heineken Dutch Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1030\n",
      "Successfully fetched and parsed data for Event ID: 1030\n",
      "\n",
      "Attempting to fetch data for Event ID: 1031 (Formula 1 Pirelli Gran Premio d'Italia 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1031\n",
      "Successfully fetched and parsed data for Event ID: 1031\n",
      "\n",
      "Attempting to fetch data for Event ID: 1032 (Formula 1 Qatar Airways Azerbaijan Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1032\n",
      "Successfully fetched and parsed data for Event ID: 1032\n",
      "\n",
      "Attempting to fetch data for Event ID: 1033 (Formula 1 Singapore Airlines Singapore Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1033\n",
      "Successfully fetched and parsed data for Event ID: 1033\n",
      "\n",
      "Attempting to fetch data for Event ID: 1034 (Formula 1 Pirelli United States Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1034\n",
      "Successfully fetched and parsed data for Event ID: 1034\n",
      "\n",
      "Attempting to fetch data for Event ID: 1035 (Formula 1 Grand Premio de la Ciudad de Mexico 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1035\n",
      "Successfully fetched and parsed data for Event ID: 1035\n",
      "\n",
      "Attempting to fetch data for Event ID: 1036 (Formula 1 Lenovo Grande Prêmio de São Paolo 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1036\n",
      "Successfully fetched and parsed data for Event ID: 1036\n",
      "\n",
      "Attempting to fetch data for Event ID: 1037 (Formula 1 Heineken Silver Las Vegas Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1037\n",
      "Successfully fetched and parsed data for Event ID: 1037\n",
      "\n",
      "Attempting to fetch data for Event ID: 1038 (Formula 1 Qatar Airways Qatar Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1038\n",
      "Successfully fetched and parsed data for Event ID: 1038\n",
      "\n",
      "Attempting to fetch data for Event ID: 1039 (Formula 1 Etihad Airways Abu Dhabi Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1039\n",
      "Successfully fetched and parsed data for Event ID: 1039\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully fetched data for 24 events.\n",
      "Failed to fetch data for 0 events.\n",
      "\n",
      "Processing event: Formula 1 Gulf Air Bahrain Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Bahrain Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 stc Saudi Arabian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Saudi Arabian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Rolex Australian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Australian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 MSC Cruises Japanese Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Japanese Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Lenovo Chinese Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Chinese Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Crypto.com Miami Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Miami Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 MSC Cruises Gran Premio dell'Emilia-Romagna 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Emilia Romagna Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grand Prix de Monaco 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Monaco Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 AWS Grand Prix du Canada 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Canadian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Aramco Grand Premio de España\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Spanish Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Airways Austrian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Austrian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Airways British Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/British Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Hungarian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Hungarian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Rolex Belgian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Belgian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Heineken Dutch Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Dutch Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Pirelli Gran Premio d'Italia 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Italian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Airways Azerbaijan Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Azerbaijan Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Singapore Airlines Singapore Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Singapore Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Pirelli United States Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/United States Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grand Premio de la Ciudad de Mexico 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Warning: Could not determine race name for: Formula 1 Grand Premio de la Ciudad de Mexico 2024\n",
      "Saved data to 2024/Unknown Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Lenovo Grande Prêmio de São Paolo 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Warning: Could not determine race name for: Formula 1 Lenovo Grande Prêmio de São Paolo 2024\n",
      "Saved data to 2024/Unknown Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Heineken Silver Las Vegas Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Las Vegas Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Airways Qatar Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Qatar Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Etihad Airways Abu Dhabi Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Abu Dhabi Grand Prix.json\n",
      "\n",
      "--- JSON Export Complete ---\n",
      "Successfully saved 24 event data files to the '2024' directory.\n",
      "Files saved:\n",
      "  - 2024/Bahrain Grand Prix.json\n",
      "  - 2024/Saudi Arabian Grand Prix.json\n",
      "  - 2024/Australian Grand Prix.json\n",
      "  - 2024/Japanese Grand Prix.json\n",
      "  - 2024/Chinese Grand Prix.json\n",
      "  - 2024/Miami Grand Prix.json\n",
      "  - 2024/Emilia Romagna Grand Prix.json\n",
      "  - 2024/Monaco Grand Prix.json\n",
      "  - 2024/Canadian Grand Prix.json\n",
      "  - 2024/Spanish Grand Prix.json\n",
      "  - 2024/Austrian Grand Prix.json\n",
      "  - 2024/British Grand Prix.json\n",
      "  - 2024/Hungarian Grand Prix.json\n",
      "  - 2024/Belgian Grand Prix.json\n",
      "  - 2024/Dutch Grand Prix.json\n",
      "  - 2024/Italian Grand Prix.json\n",
      "  - 2024/Azerbaijan Grand Prix.json\n",
      "  - 2024/Singapore Grand Prix.json\n",
      "  - 2024/United States Grand Prix.json\n",
      "  - 2024/Unknown Grand Prix.json\n",
      "  - 2024/Unknown Grand Prix.json\n",
      "  - 2024/Las Vegas Grand Prix.json\n",
      "  - 2024/Qatar Grand Prix.json\n",
      "  - 2024/Abu Dhabi Grand Prix.json\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Configuration constants\n",
    "DEFAULT_TIMEOUT = 10\n",
    "# URLs by year\n",
    "F1_URLS = {\n",
    "    2024: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6276\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6273\"\n",
    "    },\n",
    "    2025: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6367\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6365\"\n",
    "    }\n",
    "}\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DELAY_BETWEEN_REQUESTS = 0.5  # seconds\n",
    "\n",
    "# F1 race names by year\n",
    "F1_RACES = {\n",
    "    2025: {\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"CHINESE\": \"Chinese Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"EMILIA-ROMAGNA\": \"Emilia Romagna Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"MÉXICO\": \"Mexico City Grand Prix\",\n",
    "        \"SÃO PAULO\": \"São Paulo Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "    },\n",
    "    2024: {\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"CHINESE\": \"Chinese Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"EMILIA-ROMAGNA\": \"Emilia Romagna Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"MÉXICO\": \"Mexico City Grand Prix\",\n",
    "        \"SÃO PAULO\": \"São Paulo Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class F1DataFetcher:\n",
    "    \"\"\"Class for fetching and processing Formula 1 data.\"\"\"\n",
    "\n",
    "    def __init__(self, year: int = 2025, timeout: int = DEFAULT_TIMEOUT, headers: Dict = None):\n",
    "        \"\"\"\n",
    "        Initialize the F1DataFetcher.\n",
    "\n",
    "        Args:\n",
    "            year: Year for which to fetch F1 data\n",
    "            timeout: Request timeout in seconds\n",
    "            headers: HTTP headers for requests\n",
    "        \"\"\"\n",
    "        self.year = year\n",
    "        self.timeout = timeout\n",
    "        self.headers = headers or DEFAULT_HEADERS\n",
    "        self.event_data_cache = {}\n",
    "        self.event_specific_data_cache = {}\n",
    "\n",
    "        # Set URLs based on year\n",
    "        self.set_year(year)\n",
    "\n",
    "    def set_year(self, year: int) -> None:\n",
    "        \"\"\"\n",
    "        Set the year and update URLs accordingly.\n",
    "\n",
    "        Args:\n",
    "            year: Year for which to fetch F1 data\n",
    "        \"\"\"\n",
    "        self.year = year\n",
    "        if year in F1_URLS:\n",
    "            self.event_data_url = F1_URLS[year][\"EVENT_DATA_URL\"]\n",
    "            self.event_specific_url = F1_URLS[year][\"EVENT_SPECIFIC_URL\"]\n",
    "        else:\n",
    "            # Default to latest year if requested year is not available\n",
    "            latest_year = max(F1_URLS.keys())\n",
    "            print(f\"Warning: Data for year {year} not available. Using {latest_year} instead.\")\n",
    "            self.year = latest_year\n",
    "            self.event_data_url = F1_URLS[latest_year][\"EVENT_DATA_URL\"]\n",
    "            self.event_specific_url = F1_URLS[latest_year][\"EVENT_SPECIFIC_URL\"]\n",
    "\n",
    "    def fetch_events_data(self, url: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Fetch events data from the specified URL.\n",
    "\n",
    "        Args:\n",
    "            url: URL to fetch data from (defaults to year-specific URL)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing events data\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "            json.JSONDecodeError: If response is not valid JSON\n",
    "        \"\"\"\n",
    "        if url is None:\n",
    "            url = self.event_data_url\n",
    "\n",
    "        print(f\"Attempting to fetch data from: {url}\")\n",
    "\n",
    "        try:\n",
    "            response = self._make_request(url)\n",
    "            parsed_data = response.json()\n",
    "\n",
    "            # Extract events list from the nested structure\n",
    "            data_section = parsed_data.get(\"data\", {})\n",
    "            chart_section = data_section.get(\"chart\", {})\n",
    "            events_data = chart_section.get(\"events\", [])\n",
    "\n",
    "            if events_data:\n",
    "                print(\"\\nSuccessfully extracted events data\")\n",
    "                return events_data\n",
    "            else:\n",
    "                print(\"\\nError: Could not find the 'events' data at the expected path\")\n",
    "                return []\n",
    "\n",
    "        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
    "            print(f\"\\nError fetching events data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_event_specific_data(\n",
    "        self, events_data: List[Dict], base_url: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch specific data for each event.\n",
    "\n",
    "        Args:\n",
    "            events_data: List of event dictionaries\n",
    "            base_url: Base URL for event-specific data (defaults to year-specific URL)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping event IDs to their specific data\n",
    "        \"\"\"\n",
    "        if base_url is None:\n",
    "            base_url = self.event_specific_url\n",
    "\n",
    "        all_event_specific_data = {}\n",
    "\n",
    "        print(f\"Found {len(events_data)} events to process.\")\n",
    "\n",
    "        for event in events_data:\n",
    "            event_id = event.get(\"id\")\n",
    "            event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "            if not event_id:\n",
    "                print(f\"Warning: Skipping event with missing ID: {event_title}\")\n",
    "                continue\n",
    "\n",
    "            # Check cache first\n",
    "            if event_id in self.event_specific_data_cache:\n",
    "                all_event_specific_data[event_id] = self.event_specific_data_cache[\n",
    "                    event_id\n",
    "                ]\n",
    "                print(f\"Using cached data for Event ID: {event_id} ({event_title})\")\n",
    "                continue\n",
    "\n",
    "            specific_url = f\"{base_url}?event={event_id}\"\n",
    "            print(\n",
    "                f\"\\nAttempting to fetch data for Event ID: {event_id} ({event_title})\"\n",
    "            )\n",
    "            print(f\"URL: {specific_url}\")\n",
    "\n",
    "            try:\n",
    "                response = self._make_request(specific_url)\n",
    "                event_specific_data = response.json()\n",
    "\n",
    "                # Cache the result\n",
    "                self.event_specific_data_cache[event_id] = event_specific_data\n",
    "                all_event_specific_data[event_id] = event_specific_data\n",
    "\n",
    "                print(f\"Successfully fetched and parsed data for Event ID: {event_id}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                error_info = {\"error\": str(e)}\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error during request for Event ID: {event_id}: {e}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                error_info = {\n",
    "                    \"error\": \"JSONDecodeError\",\n",
    "                    \"response_text\": (\n",
    "                        response.text[:500]\n",
    "                        if hasattr(response, \"text\")\n",
    "                        else \"No response text\"\n",
    "                    ),\n",
    "                }\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error: Failed to decode JSON for Event ID: {event_id}\")\n",
    "\n",
    "            # Add delay between requests\n",
    "            if DELAY_BETWEEN_REQUESTS > 0:\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        # Print summary\n",
    "        self._print_fetch_summary(events_data, all_event_specific_data)\n",
    "        return all_event_specific_data\n",
    "\n",
    "    def _make_request(self, url: str) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Make an HTTP request with error handling.\n",
    "\n",
    "        Args:\n",
    "            url: URL to request\n",
    "\n",
    "        Returns:\n",
    "            Response object\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Error: The request to {url} timed out.\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Error during request to {url}\")\n",
    "            raise\n",
    "\n",
    "    def _print_fetch_summary(\n",
    "        self, events_data: List[Dict], all_event_specific_data: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Print a summary of the fetch operation.\"\"\"\n",
    "        print(\"\\n--- Processing Complete ---\")\n",
    "        successful_fetches = sum(\n",
    "            1\n",
    "            for data in all_event_specific_data.values()\n",
    "            if isinstance(data, dict) and \"error\" not in data\n",
    "        )\n",
    "        failed_fetches = len(events_data) - successful_fetches\n",
    "        print(f\"Successfully fetched data for {successful_fetches} events.\")\n",
    "        print(f\"Failed to fetch data for {failed_fetches} events.\")\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Class for processing F1 data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def html_table_to_dataframe(event_json_data: Dict) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract HTML table from event JSON data and convert to DataFrame.\n",
    "\n",
    "        Args:\n",
    "            event_json_data: JSON data dictionary for a specific event\n",
    "\n",
    "        Returns:\n",
    "            DataFrame created from HTML table, or None if extraction fails\n",
    "        \"\"\"\n",
    "        if not isinstance(event_json_data, dict):\n",
    "            print(\"Error: Input must be a dictionary.\")\n",
    "            return None\n",
    "\n",
    "        # Extract HTML table string\n",
    "        html_table_str = event_json_data.get(\"htmlList\", {}).get(\"table\")\n",
    "\n",
    "        if not html_table_str:\n",
    "            print(\n",
    "                \"Error: Could not find 'htmlList' -> 'table' in the provided JSON data or it's empty.\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        if not isinstance(html_table_str, str):\n",
    "            print(\"Error: The value at ['htmlList']['table'] is not a string.\")\n",
    "            return None\n",
    "\n",
    "        print(\"Found HTML table string. Attempting to parse...\")\n",
    "        try:\n",
    "            # Parse HTML table into DataFrame\n",
    "            list_of_dfs = pd.read_html(io.StringIO(html_table_str))\n",
    "\n",
    "            if list_of_dfs:\n",
    "                print(\"Successfully parsed HTML table into DataFrame.\")\n",
    "                return list_of_dfs[0]\n",
    "            else:\n",
    "                print(\n",
    "                    \"Warning: No tables found by pd.read_html, although HTML string was present.\"\n",
    "                )\n",
    "                return None\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error parsing HTML with pandas (ValueError): {ve}\")\n",
    "            print(\"Check if the HTML string actually contains a <table> tag.\")\n",
    "            return None\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"Error: The 'lxml' library might be required by pd.read_html. Please install it (`pip install lxml`).\"\n",
    "            )\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during HTML parsing: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dataframe_to_json(\n",
    "        df: pd.DataFrame, event_title: str, year: int, output_dir: str = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save DataFrame to a JSON file with a standardized filename.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            event_title: Title of the event (e.g., \"FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\")\n",
    "            year: Year of the event\n",
    "            output_dir: Directory to save the JSON file (defaults to year folder)\n",
    "\n",
    "        Returns:\n",
    "            Path to the saved JSON file\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            print(f\"Error: Cannot save None DataFrame for {event_title}\")\n",
    "            return \"\"\n",
    "\n",
    "        # If no output directory is specified, use the year as the directory name\n",
    "        if output_dir is None:\n",
    "            output_dir = str(year)\n",
    "\n",
    "        # Find the proper race name from the event title\n",
    "        race_name = None\n",
    "\n",
    "        # First, try to match with our predefined race names for the specific year\n",
    "        if year in F1_RACES:\n",
    "            year_races = F1_RACES[year]\n",
    "            for key, proper_name in year_races.items():\n",
    "                if key in event_title.upper():\n",
    "                    race_name = proper_name\n",
    "                    break\n",
    "        else:\n",
    "            # If year not in F1_RACES, use the latest year's race names\n",
    "            latest_year = max(F1_RACES.keys())\n",
    "            year_races = F1_RACES[latest_year]\n",
    "            for key, proper_name in year_races.items():\n",
    "                if key in event_title.upper():\n",
    "                    race_name = proper_name\n",
    "                    break\n",
    "\n",
    "        # If no match found, try to extract from the title\n",
    "        if race_name is None:\n",
    "            # Try to extract Grand Prix name using regex\n",
    "            match = re.search(\n",
    "                r\"([A-Z]+(?:\\s+[A-Z]+)*)\\s+GRAND\\s+PRIX\", event_title, re.IGNORECASE\n",
    "            )\n",
    "            if match:\n",
    "                location = match.group(1).strip()\n",
    "                race_name = f\"{location} Grand Prix\"\n",
    "            else:\n",
    "                # Fallback to a generic name with the event ID\n",
    "                race_name = \"Unknown Grand Prix\"\n",
    "                print(f\"Warning: Could not determine race name for: {event_title}\")\n",
    "\n",
    "        # Clean up the filename\n",
    "        filename = race_name\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save the DataFrame to a JSON file\n",
    "        file_path = os.path.join(output_dir, f\"{filename}.json\")\n",
    "        df.to_json(file_path, orient=\"records\", indent=4)\n",
    "\n",
    "        print(f\"Saved data to {file_path}\")\n",
    "        return file_path\n",
    "\n",
    "\n",
    "def main(year: int = 2025):\n",
    "    \"\"\"\n",
    "    Main function to fetch and process F1 data for a specific year.\n",
    "\n",
    "    Args:\n",
    "        year: Year to fetch data for (default: 2025)\n",
    "    \"\"\"\n",
    "    print(f\"Fetching F1 data for year: {year}\")\n",
    "\n",
    "    # Initialize the data fetcher with the specified year\n",
    "    fetcher = F1DataFetcher(year=year)\n",
    "\n",
    "    # Fetch events data\n",
    "    events_data = fetcher.fetch_events_data()\n",
    "    if not events_data:\n",
    "        print(\"No events data found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Fetch specific data for each event\n",
    "    all_event_specific_data = fetcher.fetch_event_specific_data(events_data)\n",
    "\n",
    "    # Create a DataProcessor instance\n",
    "    processor = DataProcessor()\n",
    "\n",
    "    # Process each event and save to JSON\n",
    "    saved_files = []\n",
    "    output_dir = str(year)  # Use year as directory name\n",
    "\n",
    "    for event in events_data:\n",
    "        event_id = event.get(\"id\")\n",
    "        event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "        if not event_id or event_id not in all_event_specific_data:\n",
    "            print(f\"Skipping event {event_title}: No data available\")\n",
    "            continue\n",
    "\n",
    "        event_data = all_event_specific_data[event_id]\n",
    "\n",
    "        # Check if there was an error fetching this event's data\n",
    "        if isinstance(event_data, dict) and \"error\" in event_data:\n",
    "            print(\n",
    "                f\"Skipping event {event_title}: Error in data - {event_data.get('error')}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Convert HTML table to DataFrame\n",
    "        print(f\"\\nProcessing event: {event_title}\")\n",
    "        event_dataframe = processor.html_table_to_dataframe(event_data)\n",
    "\n",
    "        if event_dataframe is not None:\n",
    "            # Save DataFrame to JSON\n",
    "            file_path = processor.save_dataframe_to_json(\n",
    "                event_dataframe, event_title, year, output_dir\n",
    "            )\n",
    "\n",
    "            if file_path:\n",
    "                saved_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Failed to create DataFrame for {event_title}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n--- JSON Export Complete ---\")\n",
    "    print(\n",
    "        f\"Successfully saved {len(saved_files)} event data files to the '{output_dir}' directory.\"\n",
    "    )\n",
    "    if saved_files:\n",
    "        print(\"Files saved:\")\n",
    "        for file_path in saved_files:\n",
    "            print(f\"  - {file_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Call main function with desired year\n",
    "    # Example: main(2024) to fetch 2024 data\n",
    "    main(2024)  # Default to 2025 if no year specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching F1 data for year: 2024\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6276\n",
      "\n",
      "Successfully extracted events data\n",
      "Found 24 events to process.\n",
      "\n",
      "Attempting to fetch data for Event ID: 1016 (Formula 1 Gulf Air Bahrain Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1016\n",
      "Successfully fetched and parsed data for Event ID: 1016\n",
      "\n",
      "Attempting to fetch data for Event ID: 1017 (Formula 1 stc Saudi Arabian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1017\n",
      "Successfully fetched and parsed data for Event ID: 1017\n",
      "\n",
      "Attempting to fetch data for Event ID: 1018 (Formula 1 Rolex Australian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1018\n",
      "Successfully fetched and parsed data for Event ID: 1018\n",
      "\n",
      "Attempting to fetch data for Event ID: 1019 (Formula 1 MSC Cruises Japanese Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1019\n",
      "Successfully fetched and parsed data for Event ID: 1019\n",
      "\n",
      "Attempting to fetch data for Event ID: 1020 (Formula 1 Lenovo Chinese Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1020\n",
      "Successfully fetched and parsed data for Event ID: 1020\n",
      "\n",
      "Attempting to fetch data for Event ID: 1021 (Formula 1 Crypto.com Miami Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1021\n",
      "Successfully fetched and parsed data for Event ID: 1021\n",
      "\n",
      "Attempting to fetch data for Event ID: 1022 (Formula 1 MSC Cruises Gran Premio dell'Emilia-Romagna 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1022\n",
      "Successfully fetched and parsed data for Event ID: 1022\n",
      "\n",
      "Attempting to fetch data for Event ID: 1023 (Formula 1 Grand Prix de Monaco 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1023\n",
      "Successfully fetched and parsed data for Event ID: 1023\n",
      "\n",
      "Attempting to fetch data for Event ID: 1024 (Formula 1 AWS Grand Prix du Canada 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1024\n",
      "Successfully fetched and parsed data for Event ID: 1024\n",
      "\n",
      "Attempting to fetch data for Event ID: 1025 (Formula 1 Aramco Grand Premio de España)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1025\n",
      "Successfully fetched and parsed data for Event ID: 1025\n",
      "\n",
      "Attempting to fetch data for Event ID: 1026 (Formula 1 Qatar Airways Austrian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1026\n",
      "Successfully fetched and parsed data for Event ID: 1026\n",
      "\n",
      "Attempting to fetch data for Event ID: 1027 (Formula 1 Qatar Airways British Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1027\n",
      "Successfully fetched and parsed data for Event ID: 1027\n",
      "\n",
      "Attempting to fetch data for Event ID: 1028 (Formula 1 Hungarian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1028\n",
      "Successfully fetched and parsed data for Event ID: 1028\n",
      "\n",
      "Attempting to fetch data for Event ID: 1029 (Formula 1 Rolex Belgian Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1029\n",
      "Successfully fetched and parsed data for Event ID: 1029\n",
      "\n",
      "Attempting to fetch data for Event ID: 1030 (Formula 1 Heineken Dutch Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1030\n",
      "Successfully fetched and parsed data for Event ID: 1030\n",
      "\n",
      "Attempting to fetch data for Event ID: 1031 (Formula 1 Pirelli Gran Premio d'Italia 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1031\n",
      "Successfully fetched and parsed data for Event ID: 1031\n",
      "\n",
      "Attempting to fetch data for Event ID: 1032 (Formula 1 Qatar Airways Azerbaijan Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1032\n",
      "Successfully fetched and parsed data for Event ID: 1032\n",
      "\n",
      "Attempting to fetch data for Event ID: 1033 (Formula 1 Singapore Airlines Singapore Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1033\n",
      "Successfully fetched and parsed data for Event ID: 1033\n",
      "\n",
      "Attempting to fetch data for Event ID: 1034 (Formula 1 Pirelli United States Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1034\n",
      "Successfully fetched and parsed data for Event ID: 1034\n",
      "\n",
      "Attempting to fetch data for Event ID: 1035 (Formula 1 Grand Premio de la Ciudad de Mexico 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1035\n",
      "Successfully fetched and parsed data for Event ID: 1035\n",
      "\n",
      "Attempting to fetch data for Event ID: 1036 (Formula 1 Lenovo Grande Prêmio de São Paolo 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1036\n",
      "Successfully fetched and parsed data for Event ID: 1036\n",
      "\n",
      "Attempting to fetch data for Event ID: 1037 (Formula 1 Heineken Silver Las Vegas Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1037\n",
      "Successfully fetched and parsed data for Event ID: 1037\n",
      "\n",
      "Attempting to fetch data for Event ID: 1038 (Formula 1 Qatar Airways Qatar Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1038\n",
      "Successfully fetched and parsed data for Event ID: 1038\n",
      "\n",
      "Attempting to fetch data for Event ID: 1039 (Formula 1 Etihad Airways Abu Dhabi Grand Prix 2024)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6273?event=1039\n",
      "Successfully fetched and parsed data for Event ID: 1039\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully fetched data for 24 events.\n",
      "Failed to fetch data for 0 events.\n",
      "\n",
      "Processing event: Formula 1 Gulf Air Bahrain Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Bahrain Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 stc Saudi Arabian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Saudi Arabian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Rolex Australian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Australian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 MSC Cruises Japanese Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Japanese Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Lenovo Chinese Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Chinese Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Crypto.com Miami Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Miami Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 MSC Cruises Gran Premio dell'Emilia-Romagna 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Emilia Romagna Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grand Prix de Monaco 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Monaco Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 AWS Grand Prix du Canada 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Canadian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Aramco Grand Premio de España\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Spanish Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Airways Austrian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Austrian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Airways British Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/British Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Hungarian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Hungarian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Rolex Belgian Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Belgian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Heineken Dutch Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Dutch Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Pirelli Gran Premio d'Italia 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Italian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Airways Azerbaijan Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Azerbaijan Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Singapore Airlines Singapore Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Singapore Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Pirelli United States Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/United States Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grand Premio de la Ciudad de Mexico 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Mexico City Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Lenovo Grande Prêmio de São Paolo 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/São Paulo Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Heineken Silver Las Vegas Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Las Vegas Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Airways Qatar Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Qatar Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Etihad Airways Abu Dhabi Grand Prix 2024\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2024/Abu Dhabi Grand Prix.json\n",
      "\n",
      "--- JSON Export Complete ---\n",
      "Successfully saved 24 event data files to the '2024' directory.\n",
      "Files saved:\n",
      "  - 2024/Bahrain Grand Prix.json\n",
      "  - 2024/Saudi Arabian Grand Prix.json\n",
      "  - 2024/Australian Grand Prix.json\n",
      "  - 2024/Japanese Grand Prix.json\n",
      "  - 2024/Chinese Grand Prix.json\n",
      "  - 2024/Miami Grand Prix.json\n",
      "  - 2024/Emilia Romagna Grand Prix.json\n",
      "  - 2024/Monaco Grand Prix.json\n",
      "  - 2024/Canadian Grand Prix.json\n",
      "  - 2024/Spanish Grand Prix.json\n",
      "  - 2024/Austrian Grand Prix.json\n",
      "  - 2024/British Grand Prix.json\n",
      "  - 2024/Hungarian Grand Prix.json\n",
      "  - 2024/Belgian Grand Prix.json\n",
      "  - 2024/Dutch Grand Prix.json\n",
      "  - 2024/Italian Grand Prix.json\n",
      "  - 2024/Azerbaijan Grand Prix.json\n",
      "  - 2024/Singapore Grand Prix.json\n",
      "  - 2024/United States Grand Prix.json\n",
      "  - 2024/Mexico City Grand Prix.json\n",
      "  - 2024/São Paulo Grand Prix.json\n",
      "  - 2024/Las Vegas Grand Prix.json\n",
      "  - 2024/Qatar Grand Prix.json\n",
      "  - 2024/Abu Dhabi Grand Prix.json\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Configuration constants\n",
    "DEFAULT_TIMEOUT = 10\n",
    "# URLs by year\n",
    "F1_URLS = {\n",
    "    2023: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6284\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6282\",\n",
    "    },\n",
    "    2024: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6276\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6273\",\n",
    "    },\n",
    "    2025: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6367\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6365\",\n",
    "    },\n",
    "}\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DELAY_BETWEEN_REQUESTS = 0.5  # seconds\n",
    "\n",
    "# F1 race names by year\n",
    "F1_RACES = {\n",
    "    2025: {\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"CHINESE\": \"Chinese Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"EMILIA-ROMAGNA\": \"Emilia Romagna Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"MÉXICO\": \"Mexico City Grand Prix\",\n",
    "        \"SÃO PAULO\": \"São Paulo Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "    },\n",
    "    2024: {\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"CHINESE\": \"Chinese Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"EMILIA-ROMAGNA\": \"Emilia Romagna Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"MEXICO\": \"Mexico City Grand Prix\",\n",
    "        \"SÃO PAOLO\": \"São Paulo Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class F1DataFetcher:\n",
    "    \"\"\"Class for fetching and processing Formula 1 data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, year: int = 2025, timeout: int = DEFAULT_TIMEOUT, headers: Dict = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the F1DataFetcher.\n",
    "\n",
    "        Args:\n",
    "            year: Year for which to fetch F1 data\n",
    "            timeout: Request timeout in seconds\n",
    "            headers: HTTP headers for requests\n",
    "        \"\"\"\n",
    "        self.year = year\n",
    "        self.timeout = timeout\n",
    "        self.headers = headers or DEFAULT_HEADERS\n",
    "        self.event_data_cache = {}\n",
    "        self.event_specific_data_cache = {}\n",
    "\n",
    "        # Set URLs based on year\n",
    "        self.set_year(year)\n",
    "\n",
    "    def set_year(self, year: int) -> None:\n",
    "        \"\"\"\n",
    "        Set the year and update URLs accordingly.\n",
    "\n",
    "        Args:\n",
    "            year: Year for which to fetch F1 data\n",
    "        \"\"\"\n",
    "        self.year = year\n",
    "        if year in F1_URLS:\n",
    "            self.event_data_url = F1_URLS[year][\"EVENT_DATA_URL\"]\n",
    "            self.event_specific_url = F1_URLS[year][\"EVENT_SPECIFIC_URL\"]\n",
    "        else:\n",
    "            # Default to latest year if requested year is not available\n",
    "            latest_year = max(F1_URLS.keys())\n",
    "            print(\n",
    "                f\"Warning: Data for year {year} not available. Using {latest_year} instead.\"\n",
    "            )\n",
    "            self.year = latest_year\n",
    "            self.event_data_url = F1_URLS[latest_year][\"EVENT_DATA_URL\"]\n",
    "            self.event_specific_url = F1_URLS[latest_year][\"EVENT_SPECIFIC_URL\"]\n",
    "\n",
    "    def fetch_events_data(self, url: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Fetch events data from the specified URL.\n",
    "\n",
    "        Args:\n",
    "            url: URL to fetch data from (defaults to year-specific URL)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing events data\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "            json.JSONDecodeError: If response is not valid JSON\n",
    "        \"\"\"\n",
    "        if url is None:\n",
    "            url = self.event_data_url\n",
    "\n",
    "        print(f\"Attempting to fetch data from: {url}\")\n",
    "\n",
    "        try:\n",
    "            response = self._make_request(url)\n",
    "            parsed_data = response.json()\n",
    "\n",
    "            # Extract events list from the nested structure\n",
    "            data_section = parsed_data.get(\"data\", {})\n",
    "            chart_section = data_section.get(\"chart\", {})\n",
    "            events_data = chart_section.get(\"events\", [])\n",
    "\n",
    "            if events_data:\n",
    "                print(\"\\nSuccessfully extracted events data\")\n",
    "                return events_data\n",
    "            else:\n",
    "                print(\"\\nError: Could not find the 'events' data at the expected path\")\n",
    "                return []\n",
    "\n",
    "        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
    "            print(f\"\\nError fetching events data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_event_specific_data(\n",
    "        self, events_data: List[Dict], base_url: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch specific data for each event.\n",
    "\n",
    "        Args:\n",
    "            events_data: List of event dictionaries\n",
    "            base_url: Base URL for event-specific data (defaults to year-specific URL)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping event IDs to their specific data\n",
    "        \"\"\"\n",
    "        if base_url is None:\n",
    "            base_url = self.event_specific_url\n",
    "\n",
    "        all_event_specific_data = {}\n",
    "\n",
    "        print(f\"Found {len(events_data)} events to process.\")\n",
    "\n",
    "        for event in events_data:\n",
    "            event_id = event.get(\"id\")\n",
    "            event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "            if not event_id:\n",
    "                print(f\"Warning: Skipping event with missing ID: {event_title}\")\n",
    "                continue\n",
    "\n",
    "            # Check cache first\n",
    "            if event_id in self.event_specific_data_cache:\n",
    "                all_event_specific_data[event_id] = self.event_specific_data_cache[\n",
    "                    event_id\n",
    "                ]\n",
    "                print(f\"Using cached data for Event ID: {event_id} ({event_title})\")\n",
    "                continue\n",
    "\n",
    "            specific_url = f\"{base_url}?event={event_id}\"\n",
    "            print(\n",
    "                f\"\\nAttempting to fetch data for Event ID: {event_id} ({event_title})\"\n",
    "            )\n",
    "            print(f\"URL: {specific_url}\")\n",
    "\n",
    "            try:\n",
    "                response = self._make_request(specific_url)\n",
    "                event_specific_data = response.json()\n",
    "\n",
    "                # Cache the result\n",
    "                self.event_specific_data_cache[event_id] = event_specific_data\n",
    "                all_event_specific_data[event_id] = event_specific_data\n",
    "\n",
    "                print(f\"Successfully fetched and parsed data for Event ID: {event_id}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                error_info = {\"error\": str(e)}\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error during request for Event ID: {event_id}: {e}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                error_info = {\n",
    "                    \"error\": \"JSONDecodeError\",\n",
    "                    \"response_text\": (\n",
    "                        response.text[:500]\n",
    "                        if hasattr(response, \"text\")\n",
    "                        else \"No response text\"\n",
    "                    ),\n",
    "                }\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error: Failed to decode JSON for Event ID: {event_id}\")\n",
    "\n",
    "            # Add delay between requests\n",
    "            if DELAY_BETWEEN_REQUESTS > 0:\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        # Print summary\n",
    "        self._print_fetch_summary(events_data, all_event_specific_data)\n",
    "        return all_event_specific_data\n",
    "\n",
    "    def _make_request(self, url: str) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Make an HTTP request with error handling.\n",
    "\n",
    "        Args:\n",
    "            url: URL to request\n",
    "\n",
    "        Returns:\n",
    "            Response object\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Error: The request to {url} timed out.\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Error during request to {url}\")\n",
    "            raise\n",
    "\n",
    "    def _print_fetch_summary(\n",
    "        self, events_data: List[Dict], all_event_specific_data: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Print a summary of the fetch operation.\"\"\"\n",
    "        print(\"\\n--- Processing Complete ---\")\n",
    "        successful_fetches = sum(\n",
    "            1\n",
    "            for data in all_event_specific_data.values()\n",
    "            if isinstance(data, dict) and \"error\" not in data\n",
    "        )\n",
    "        failed_fetches = len(events_data) - successful_fetches\n",
    "        print(f\"Successfully fetched data for {successful_fetches} events.\")\n",
    "        print(f\"Failed to fetch data for {failed_fetches} events.\")\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Class for processing F1 data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def html_table_to_dataframe(event_json_data: Dict) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract HTML table from event JSON data and convert to DataFrame.\n",
    "\n",
    "        Args:\n",
    "            event_json_data: JSON data dictionary for a specific event\n",
    "\n",
    "        Returns:\n",
    "            DataFrame created from HTML table, or None if extraction fails\n",
    "        \"\"\"\n",
    "        if not isinstance(event_json_data, dict):\n",
    "            print(\"Error: Input must be a dictionary.\")\n",
    "            return None\n",
    "\n",
    "        # Extract HTML table string\n",
    "        html_table_str = event_json_data.get(\"htmlList\", {}).get(\"table\")\n",
    "\n",
    "        if not html_table_str:\n",
    "            print(\n",
    "                \"Error: Could not find 'htmlList' -> 'table' in the provided JSON data or it's empty.\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        if not isinstance(html_table_str, str):\n",
    "            print(\"Error: The value at ['htmlList']['table'] is not a string.\")\n",
    "            return None\n",
    "\n",
    "        print(\"Found HTML table string. Attempting to parse...\")\n",
    "        try:\n",
    "            # Parse HTML table into DataFrame\n",
    "            list_of_dfs = pd.read_html(io.StringIO(html_table_str))\n",
    "\n",
    "            if list_of_dfs:\n",
    "                print(\"Successfully parsed HTML table into DataFrame.\")\n",
    "                return list_of_dfs[0]\n",
    "            else:\n",
    "                print(\n",
    "                    \"Warning: No tables found by pd.read_html, although HTML string was present.\"\n",
    "                )\n",
    "                return None\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error parsing HTML with pandas (ValueError): {ve}\")\n",
    "            print(\"Check if the HTML string actually contains a <table> tag.\")\n",
    "            return None\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"Error: The 'lxml' library might be required by pd.read_html. Please install it (`pip install lxml`).\"\n",
    "            )\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during HTML parsing: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dataframe_to_json(\n",
    "        df: pd.DataFrame, event_title: str, year: int, output_dir: str = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save DataFrame to a JSON file with a standardized filename.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            event_title: Title of the event (e.g., \"FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\")\n",
    "            year: Year of the event\n",
    "            output_dir: Directory to save the JSON file (defaults to year folder)\n",
    "\n",
    "        Returns:\n",
    "            Path to the saved JSON file\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            print(f\"Error: Cannot save None DataFrame for {event_title}\")\n",
    "            return \"\"\n",
    "\n",
    "        # If no output directory is specified, use the year as the directory name\n",
    "        if output_dir is None:\n",
    "            output_dir = str(year)\n",
    "\n",
    "        # Find the proper race name from the event title\n",
    "        race_name = None\n",
    "\n",
    "        # First, try to match with our predefined race names for the specific year\n",
    "        if year in F1_RACES:\n",
    "            year_races = F1_RACES[year]\n",
    "            for key, proper_name in year_races.items():\n",
    "                if key in event_title.upper():\n",
    "                    race_name = proper_name\n",
    "                    break\n",
    "        else:\n",
    "            # If year not in F1_RACES, use the latest year's race names\n",
    "            latest_year = max(F1_RACES.keys())\n",
    "            year_races = F1_RACES[latest_year]\n",
    "            for key, proper_name in year_races.items():\n",
    "                if key in event_title.upper():\n",
    "                    race_name = proper_name\n",
    "                    break\n",
    "\n",
    "        # If no match found, try to extract from the title\n",
    "        if race_name is None:\n",
    "            # Try to extract Grand Prix name using regex\n",
    "            match = re.search(\n",
    "                r\"([A-Z]+(?:\\s+[A-Z]+)*)\\s+GRAND\\s+PRIX\", event_title, re.IGNORECASE\n",
    "            )\n",
    "            if match:\n",
    "                location = match.group(1).strip()\n",
    "                race_name = f\"{location} Grand Prix\"\n",
    "            else:\n",
    "                # Fallback to a generic name with the event ID\n",
    "                race_name = \"Unknown Grand Prix\"\n",
    "                print(f\"Warning: Could not determine race name for: {event_title}\")\n",
    "\n",
    "        # Clean up the filename\n",
    "        filename = race_name\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save the DataFrame to a JSON file\n",
    "        file_path = os.path.join(output_dir, f\"{filename}.json\")\n",
    "        df.to_json(file_path, orient=\"records\", indent=4)\n",
    "\n",
    "        print(f\"Saved data to {file_path}\")\n",
    "        return file_path\n",
    "\n",
    "\n",
    "def main(year: int = 2025):\n",
    "    \"\"\"\n",
    "    Main function to fetch and process F1 data for a specific year.\n",
    "\n",
    "    Args:\n",
    "        year: Year to fetch data for (default: 2025)\n",
    "    \"\"\"\n",
    "    print(f\"Fetching F1 data for year: {year}\")\n",
    "\n",
    "    # Initialize the data fetcher with the specified year\n",
    "    fetcher = F1DataFetcher(year=year)\n",
    "\n",
    "    # Fetch events data\n",
    "    events_data = fetcher.fetch_events_data()\n",
    "    if not events_data:\n",
    "        print(\"No events data found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Fetch specific data for each event\n",
    "    all_event_specific_data = fetcher.fetch_event_specific_data(events_data)\n",
    "\n",
    "    # Create a DataProcessor instance\n",
    "    processor = DataProcessor()\n",
    "\n",
    "    # Process each event and save to JSON\n",
    "    saved_files = []\n",
    "    output_dir = str(year)  # Use year as directory name\n",
    "\n",
    "    for event in events_data:\n",
    "        event_id = event.get(\"id\")\n",
    "        event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "        if not event_id or event_id not in all_event_specific_data:\n",
    "            print(f\"Skipping event {event_title}: No data available\")\n",
    "            continue\n",
    "\n",
    "        event_data = all_event_specific_data[event_id]\n",
    "\n",
    "        # Check if there was an error fetching this event's data\n",
    "        if isinstance(event_data, dict) and \"error\" in event_data:\n",
    "            print(\n",
    "                f\"Skipping event {event_title}: Error in data - {event_data.get('error')}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Convert HTML table to DataFrame\n",
    "        print(f\"\\nProcessing event: {event_title}\")\n",
    "        event_dataframe = processor.html_table_to_dataframe(event_data)\n",
    "\n",
    "        if event_dataframe is not None:\n",
    "            # Save DataFrame to JSON\n",
    "            file_path = processor.save_dataframe_to_json(\n",
    "                event_dataframe, event_title, year, output_dir\n",
    "            )\n",
    "\n",
    "            if file_path:\n",
    "                saved_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Failed to create DataFrame for {event_title}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n--- JSON Export Complete ---\")\n",
    "    print(\n",
    "        f\"Successfully saved {len(saved_files)} event data files to the '{output_dir}' directory.\"\n",
    "    )\n",
    "    if saved_files:\n",
    "        print(\"Files saved:\")\n",
    "        for file_path in saved_files:\n",
    "            print(f\"  - {file_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching F1 data for year: 2023\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6284\n",
      "\n",
      "Successfully extracted events data\n",
      "Found 22 events to process.\n",
      "\n",
      "Attempting to fetch data for Event ID: 837 (Formula 1 Gulf Air Bahrain Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=837\n",
      "Successfully fetched and parsed data for Event ID: 837\n",
      "\n",
      "Attempting to fetch data for Event ID: 838 (Formula 1 stc Saudi Arabian Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=838\n",
      "Successfully fetched and parsed data for Event ID: 838\n",
      "\n",
      "Attempting to fetch data for Event ID: 839 (Formula 1 Rolex Australian Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=839\n",
      "Successfully fetched and parsed data for Event ID: 839\n",
      "\n",
      "Attempting to fetch data for Event ID: 841 (Formula 1 Azerbaijan Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=841\n",
      "Successfully fetched and parsed data for Event ID: 841\n",
      "\n",
      "Attempting to fetch data for Event ID: 842 (Formula 1 Crypto.com Miami Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=842\n",
      "Successfully fetched and parsed data for Event ID: 842\n",
      "\n",
      "Attempting to fetch data for Event ID: 844 (Formula 1 Grand Prix de Monaco 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=844\n",
      "Successfully fetched and parsed data for Event ID: 844\n",
      "\n",
      "Attempting to fetch data for Event ID: 845 (Formula 1 Pirelli Gran Premio De España 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=845\n",
      "Successfully fetched and parsed data for Event ID: 845\n",
      "\n",
      "Attempting to fetch data for Event ID: 847 (Formula 1 Grand Prix Du Canada 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=847\n",
      "Successfully fetched and parsed data for Event ID: 847\n",
      "\n",
      "Attempting to fetch data for Event ID: 848 (Formula 1 Grosser Preis von Österreich 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=848\n",
      "Successfully fetched and parsed data for Event ID: 848\n",
      "\n",
      "Attempting to fetch data for Event ID: 846 (Formula 1 Aramco British Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=846\n",
      "Successfully fetched and parsed data for Event ID: 846\n",
      "\n",
      "Attempting to fetch data for Event ID: 849 (Formula 1 Hungarian Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=849\n",
      "Successfully fetched and parsed data for Event ID: 849\n",
      "\n",
      "Attempting to fetch data for Event ID: 850 (Formula 1 Belgian Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=850\n",
      "Successfully fetched and parsed data for Event ID: 850\n",
      "\n",
      "Attempting to fetch data for Event ID: 851 (Formula 1 Heineken Dutch Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=851\n",
      "Successfully fetched and parsed data for Event ID: 851\n",
      "\n",
      "Attempting to fetch data for Event ID: 852 (Formula 1 Gran Premio D’italia 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=852\n",
      "Successfully fetched and parsed data for Event ID: 852\n",
      "\n",
      "Attempting to fetch data for Event ID: 853 (Formula 1 Singapore Airlines Singapore Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=853\n",
      "Successfully fetched and parsed data for Event ID: 853\n",
      "\n",
      "Attempting to fetch data for Event ID: 854 (Formula 1 Lenovo Japanese Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=854\n",
      "Successfully fetched and parsed data for Event ID: 854\n",
      "\n",
      "Attempting to fetch data for Event ID: 855 (Formula 1 Qatar Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=855\n",
      "Successfully fetched and parsed data for Event ID: 855\n",
      "\n",
      "Attempting to fetch data for Event ID: 856 (Formula 1 Lenovo United States Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=856\n",
      "Successfully fetched and parsed data for Event ID: 856\n",
      "\n",
      "Attempting to fetch data for Event ID: 857 (Formula 1 Gran Premio De La Ciudad De México 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=857\n",
      "Successfully fetched and parsed data for Event ID: 857\n",
      "\n",
      "Attempting to fetch data for Event ID: 858 (Formula 1 Rolex Grande Prêmio De São Paulo 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=858\n",
      "Successfully fetched and parsed data for Event ID: 858\n",
      "\n",
      "Attempting to fetch data for Event ID: 859 (Formula 1 Heineken Silver Las Vegas Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=859\n",
      "Successfully fetched and parsed data for Event ID: 859\n",
      "\n",
      "Attempting to fetch data for Event ID: 860 (Formula 1 Etihad Airways Abu Dhabi Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=860\n",
      "Successfully fetched and parsed data for Event ID: 860\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully fetched data for 22 events.\n",
      "Failed to fetch data for 0 events.\n",
      "\n",
      "Processing event: Formula 1 Gulf Air Bahrain Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Bahrain Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 stc Saudi Arabian Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Saudi Arabian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Rolex Australian Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Australian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Azerbaijan Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Azerbaijan Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Crypto.com Miami Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Miami Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grand Prix de Monaco 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Monaco Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Pirelli Gran Premio De España 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Spanish Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grand Prix Du Canada 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Canadian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grosser Preis von Österreich 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Warning: Could not determine race name for: Formula 1 Grosser Preis von Österreich 2023\n",
      "Saved data to 2023/Unknown Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Aramco British Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/British Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Hungarian Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Hungarian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Belgian Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Belgian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Heineken Dutch Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Dutch Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Gran Premio D’italia 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Italian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Singapore Airlines Singapore Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Singapore Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Lenovo Japanese Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Japanese Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Qatar Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Lenovo United States Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/United States Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Gran Premio De La Ciudad De México 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Warning: Could not determine race name for: Formula 1 Gran Premio De La Ciudad De México 2023\n",
      "Saved data to 2023/Unknown Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Rolex Grande Prêmio De São Paulo 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/São Paulo Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Heineken Silver Las Vegas Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Las Vegas Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Etihad Airways Abu Dhabi Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Abu Dhabi Grand Prix.json\n",
      "\n",
      "--- JSON Export Complete ---\n",
      "Successfully saved 22 event data files to the '2023' directory.\n",
      "Files saved:\n",
      "  - 2023/Bahrain Grand Prix.json\n",
      "  - 2023/Saudi Arabian Grand Prix.json\n",
      "  - 2023/Australian Grand Prix.json\n",
      "  - 2023/Azerbaijan Grand Prix.json\n",
      "  - 2023/Miami Grand Prix.json\n",
      "  - 2023/Monaco Grand Prix.json\n",
      "  - 2023/Spanish Grand Prix.json\n",
      "  - 2023/Canadian Grand Prix.json\n",
      "  - 2023/Unknown Grand Prix.json\n",
      "  - 2023/British Grand Prix.json\n",
      "  - 2023/Hungarian Grand Prix.json\n",
      "  - 2023/Belgian Grand Prix.json\n",
      "  - 2023/Dutch Grand Prix.json\n",
      "  - 2023/Italian Grand Prix.json\n",
      "  - 2023/Singapore Grand Prix.json\n",
      "  - 2023/Japanese Grand Prix.json\n",
      "  - 2023/Qatar Grand Prix.json\n",
      "  - 2023/United States Grand Prix.json\n",
      "  - 2023/Unknown Grand Prix.json\n",
      "  - 2023/São Paulo Grand Prix.json\n",
      "  - 2023/Las Vegas Grand Prix.json\n",
      "  - 2023/Abu Dhabi Grand Prix.json\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Configuration constants\n",
    "DEFAULT_TIMEOUT = 10\n",
    "# URLs by year\n",
    "F1_URLS = {\n",
    "    2023: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6284\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6282\",\n",
    "    },\n",
    "    2024: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6276\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6273\",\n",
    "    },\n",
    "    2025: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6367\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6365\",\n",
    "    },\n",
    "}\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DELAY_BETWEEN_REQUESTS = 0.5  # seconds\n",
    "\n",
    "# F1 race names by year\n",
    "F1_RACES = {\n",
    "    2025: {\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"CHINESE\": \"Chinese Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"EMILIA-ROMAGNA\": \"Emilia Romagna Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"MÉXICO\": \"Mexico City Grand Prix\",\n",
    "        \"SÃO PAULO\": \"São Paulo Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "    },\n",
    "    2024: {\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"CHINESE\": \"Chinese Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"EMILIA-ROMAGNA\": \"Emilia Romagna Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"MEXICO\": \"Mexico City Grand Prix\",\n",
    "        \"SÃO PAOLO\": \"São Paulo Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "    },\n",
    "    2023: {\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"SÃO PAULO\": \"São Paulo Grand Prix\",\n",
    "        \"MEXICO\": \"Mexico City Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\"\n",
    "},\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class F1DataFetcher:\n",
    "    \"\"\"Class for fetching and processing Formula 1 data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, year: int = 2025, timeout: int = DEFAULT_TIMEOUT, headers: Dict = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the F1DataFetcher.\n",
    "\n",
    "        Args:\n",
    "            year: Year for which to fetch F1 data\n",
    "            timeout: Request timeout in seconds\n",
    "            headers: HTTP headers for requests\n",
    "        \"\"\"\n",
    "        self.year = year\n",
    "        self.timeout = timeout\n",
    "        self.headers = headers or DEFAULT_HEADERS\n",
    "        self.event_data_cache = {}\n",
    "        self.event_specific_data_cache = {}\n",
    "\n",
    "        # Set URLs based on year\n",
    "        self.set_year(year)\n",
    "\n",
    "    def set_year(self, year: int) -> None:\n",
    "        \"\"\"\n",
    "        Set the year and update URLs accordingly.\n",
    "\n",
    "        Args:\n",
    "            year: Year for which to fetch F1 data\n",
    "        \"\"\"\n",
    "        self.year = year\n",
    "        if year in F1_URLS:\n",
    "            self.event_data_url = F1_URLS[year][\"EVENT_DATA_URL\"]\n",
    "            self.event_specific_url = F1_URLS[year][\"EVENT_SPECIFIC_URL\"]\n",
    "        else:\n",
    "            # Default to latest year if requested year is not available\n",
    "            latest_year = max(F1_URLS.keys())\n",
    "            print(\n",
    "                f\"Warning: Data for year {year} not available. Using {latest_year} instead.\"\n",
    "            )\n",
    "            self.year = latest_year\n",
    "            self.event_data_url = F1_URLS[latest_year][\"EVENT_DATA_URL\"]\n",
    "            self.event_specific_url = F1_URLS[latest_year][\"EVENT_SPECIFIC_URL\"]\n",
    "\n",
    "    def fetch_events_data(self, url: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Fetch events data from the specified URL.\n",
    "\n",
    "        Args:\n",
    "            url: URL to fetch data from (defaults to year-specific URL)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing events data\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "            json.JSONDecodeError: If response is not valid JSON\n",
    "        \"\"\"\n",
    "        if url is None:\n",
    "            url = self.event_data_url\n",
    "\n",
    "        print(f\"Attempting to fetch data from: {url}\")\n",
    "\n",
    "        try:\n",
    "            response = self._make_request(url)\n",
    "            parsed_data = response.json()\n",
    "\n",
    "            # Extract events list from the nested structure\n",
    "            data_section = parsed_data.get(\"data\", {})\n",
    "            chart_section = data_section.get(\"chart\", {})\n",
    "            events_data = chart_section.get(\"events\", [])\n",
    "\n",
    "            if events_data:\n",
    "                print(\"\\nSuccessfully extracted events data\")\n",
    "                return events_data\n",
    "            else:\n",
    "                print(\"\\nError: Could not find the 'events' data at the expected path\")\n",
    "                return []\n",
    "\n",
    "        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
    "            print(f\"\\nError fetching events data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_event_specific_data(\n",
    "        self, events_data: List[Dict], base_url: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch specific data for each event.\n",
    "\n",
    "        Args:\n",
    "            events_data: List of event dictionaries\n",
    "            base_url: Base URL for event-specific data (defaults to year-specific URL)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping event IDs to their specific data\n",
    "        \"\"\"\n",
    "        if base_url is None:\n",
    "            base_url = self.event_specific_url\n",
    "\n",
    "        all_event_specific_data = {}\n",
    "\n",
    "        print(f\"Found {len(events_data)} events to process.\")\n",
    "\n",
    "        for event in events_data:\n",
    "            event_id = event.get(\"id\")\n",
    "            event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "            if not event_id:\n",
    "                print(f\"Warning: Skipping event with missing ID: {event_title}\")\n",
    "                continue\n",
    "\n",
    "            # Check cache first\n",
    "            if event_id in self.event_specific_data_cache:\n",
    "                all_event_specific_data[event_id] = self.event_specific_data_cache[\n",
    "                    event_id\n",
    "                ]\n",
    "                print(f\"Using cached data for Event ID: {event_id} ({event_title})\")\n",
    "                continue\n",
    "\n",
    "            specific_url = f\"{base_url}?event={event_id}\"\n",
    "            print(\n",
    "                f\"\\nAttempting to fetch data for Event ID: {event_id} ({event_title})\"\n",
    "            )\n",
    "            print(f\"URL: {specific_url}\")\n",
    "\n",
    "            try:\n",
    "                response = self._make_request(specific_url)\n",
    "                event_specific_data = response.json()\n",
    "\n",
    "                # Cache the result\n",
    "                self.event_specific_data_cache[event_id] = event_specific_data\n",
    "                all_event_specific_data[event_id] = event_specific_data\n",
    "\n",
    "                print(f\"Successfully fetched and parsed data for Event ID: {event_id}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                error_info = {\"error\": str(e)}\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error during request for Event ID: {event_id}: {e}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                error_info = {\n",
    "                    \"error\": \"JSONDecodeError\",\n",
    "                    \"response_text\": (\n",
    "                        response.text[:500]\n",
    "                        if hasattr(response, \"text\")\n",
    "                        else \"No response text\"\n",
    "                    ),\n",
    "                }\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error: Failed to decode JSON for Event ID: {event_id}\")\n",
    "\n",
    "            # Add delay between requests\n",
    "            if DELAY_BETWEEN_REQUESTS > 0:\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        # Print summary\n",
    "        self._print_fetch_summary(events_data, all_event_specific_data)\n",
    "        return all_event_specific_data\n",
    "\n",
    "    def _make_request(self, url: str) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Make an HTTP request with error handling.\n",
    "\n",
    "        Args:\n",
    "            url: URL to request\n",
    "\n",
    "        Returns:\n",
    "            Response object\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Error: The request to {url} timed out.\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Error during request to {url}\")\n",
    "            raise\n",
    "\n",
    "    def _print_fetch_summary(\n",
    "        self, events_data: List[Dict], all_event_specific_data: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Print a summary of the fetch operation.\"\"\"\n",
    "        print(\"\\n--- Processing Complete ---\")\n",
    "        successful_fetches = sum(\n",
    "            1\n",
    "            for data in all_event_specific_data.values()\n",
    "            if isinstance(data, dict) and \"error\" not in data\n",
    "        )\n",
    "        failed_fetches = len(events_data) - successful_fetches\n",
    "        print(f\"Successfully fetched data for {successful_fetches} events.\")\n",
    "        print(f\"Failed to fetch data for {failed_fetches} events.\")\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Class for processing F1 data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def html_table_to_dataframe(event_json_data: Dict) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract HTML table from event JSON data and convert to DataFrame.\n",
    "\n",
    "        Args:\n",
    "            event_json_data: JSON data dictionary for a specific event\n",
    "\n",
    "        Returns:\n",
    "            DataFrame created from HTML table, or None if extraction fails\n",
    "        \"\"\"\n",
    "        if not isinstance(event_json_data, dict):\n",
    "            print(\"Error: Input must be a dictionary.\")\n",
    "            return None\n",
    "\n",
    "        # Extract HTML table string\n",
    "        html_table_str = event_json_data.get(\"htmlList\", {}).get(\"table\")\n",
    "\n",
    "        if not html_table_str:\n",
    "            print(\n",
    "                \"Error: Could not find 'htmlList' -> 'table' in the provided JSON data or it's empty.\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        if not isinstance(html_table_str, str):\n",
    "            print(\"Error: The value at ['htmlList']['table'] is not a string.\")\n",
    "            return None\n",
    "\n",
    "        print(\"Found HTML table string. Attempting to parse...\")\n",
    "        try:\n",
    "            # Parse HTML table into DataFrame\n",
    "            list_of_dfs = pd.read_html(io.StringIO(html_table_str))\n",
    "\n",
    "            if list_of_dfs:\n",
    "                print(\"Successfully parsed HTML table into DataFrame.\")\n",
    "                return list_of_dfs[0]\n",
    "            else:\n",
    "                print(\n",
    "                    \"Warning: No tables found by pd.read_html, although HTML string was present.\"\n",
    "                )\n",
    "                return None\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error parsing HTML with pandas (ValueError): {ve}\")\n",
    "            print(\"Check if the HTML string actually contains a <table> tag.\")\n",
    "            return None\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"Error: The 'lxml' library might be required by pd.read_html. Please install it (`pip install lxml`).\"\n",
    "            )\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during HTML parsing: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dataframe_to_json(\n",
    "        df: pd.DataFrame, event_title: str, year: int, output_dir: str = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save DataFrame to a JSON file with a standardized filename.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            event_title: Title of the event (e.g., \"FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\")\n",
    "            year: Year of the event\n",
    "            output_dir: Directory to save the JSON file (defaults to year folder)\n",
    "\n",
    "        Returns:\n",
    "            Path to the saved JSON file\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            print(f\"Error: Cannot save None DataFrame for {event_title}\")\n",
    "            return \"\"\n",
    "\n",
    "        # If no output directory is specified, use the year as the directory name\n",
    "        if output_dir is None:\n",
    "            output_dir = str(year)\n",
    "\n",
    "        # Find the proper race name from the event title\n",
    "        race_name = None\n",
    "\n",
    "        # First, try to match with our predefined race names for the specific year\n",
    "        if year in F1_RACES:\n",
    "            year_races = F1_RACES[year]\n",
    "            for key, proper_name in year_races.items():\n",
    "                if key in event_title.upper():\n",
    "                    race_name = proper_name\n",
    "                    break\n",
    "        else:\n",
    "            # If year not in F1_RACES, use the latest year's race names\n",
    "            latest_year = max(F1_RACES.keys())\n",
    "            year_races = F1_RACES[latest_year]\n",
    "            for key, proper_name in year_races.items():\n",
    "                if key in event_title.upper():\n",
    "                    race_name = proper_name\n",
    "                    break\n",
    "\n",
    "        # If no match found, try to extract from the title\n",
    "        if race_name is None:\n",
    "            # Try to extract Grand Prix name using regex\n",
    "            match = re.search(\n",
    "                r\"([A-Z]+(?:\\s+[A-Z]+)*)\\s+GRAND\\s+PRIX\", event_title, re.IGNORECASE\n",
    "            )\n",
    "            if match:\n",
    "                location = match.group(1).strip()\n",
    "                race_name = f\"{location} Grand Prix\"\n",
    "            else:\n",
    "                # Fallback to a generic name with the event ID\n",
    "                race_name = \"Unknown Grand Prix\"\n",
    "                print(f\"Warning: Could not determine race name for: {event_title}\")\n",
    "\n",
    "        # Clean up the filename\n",
    "        filename = race_name\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save the DataFrame to a JSON file\n",
    "        file_path = os.path.join(output_dir, f\"{filename}.json\")\n",
    "        df.to_json(file_path, orient=\"records\", indent=4)\n",
    "\n",
    "        print(f\"Saved data to {file_path}\")\n",
    "        return file_path\n",
    "\n",
    "\n",
    "def main(year: int = 2025):\n",
    "    \"\"\"\n",
    "    Main function to fetch and process F1 data for a specific year.\n",
    "\n",
    "    Args:\n",
    "        year: Year to fetch data for (default: 2025)\n",
    "    \"\"\"\n",
    "    print(f\"Fetching F1 data for year: {year}\")\n",
    "\n",
    "    # Initialize the data fetcher with the specified year\n",
    "    fetcher = F1DataFetcher(year=year)\n",
    "\n",
    "    # Fetch events data\n",
    "    events_data = fetcher.fetch_events_data()\n",
    "    if not events_data:\n",
    "        print(\"No events data found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Fetch specific data for each event\n",
    "    all_event_specific_data = fetcher.fetch_event_specific_data(events_data)\n",
    "\n",
    "    # Create a DataProcessor instance\n",
    "    processor = DataProcessor()\n",
    "\n",
    "    # Process each event and save to JSON\n",
    "    saved_files = []\n",
    "    output_dir = str(year)  # Use year as directory name\n",
    "\n",
    "    for event in events_data:\n",
    "        event_id = event.get(\"id\")\n",
    "        event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "        if not event_id or event_id not in all_event_specific_data:\n",
    "            print(f\"Skipping event {event_title}: No data available\")\n",
    "            continue\n",
    "\n",
    "        event_data = all_event_specific_data[event_id]\n",
    "\n",
    "        # Check if there was an error fetching this event's data\n",
    "        if isinstance(event_data, dict) and \"error\" in event_data:\n",
    "            print(\n",
    "                f\"Skipping event {event_title}: Error in data - {event_data.get('error')}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Convert HTML table to DataFrame\n",
    "        print(f\"\\nProcessing event: {event_title}\")\n",
    "        event_dataframe = processor.html_table_to_dataframe(event_data)\n",
    "\n",
    "        if event_dataframe is not None:\n",
    "            # Save DataFrame to JSON\n",
    "            file_path = processor.save_dataframe_to_json(\n",
    "                event_dataframe, event_title, year, output_dir\n",
    "            )\n",
    "\n",
    "            if file_path:\n",
    "                saved_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Failed to create DataFrame for {event_title}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n--- JSON Export Complete ---\")\n",
    "    print(\n",
    "        f\"Successfully saved {len(saved_files)} event data files to the '{output_dir}' directory.\"\n",
    "    )\n",
    "    if saved_files:\n",
    "        print(\"Files saved:\")\n",
    "        for file_path in saved_files:\n",
    "            print(f\"  - {file_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching F1 data for year: 2023\n",
      "Attempting to fetch data from: https://inmotion.dhl/api/f1-award-element-data/6284\n",
      "\n",
      "Successfully extracted events data\n",
      "Found 22 events to process.\n",
      "\n",
      "Attempting to fetch data for Event ID: 837 (Formula 1 Gulf Air Bahrain Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=837\n",
      "Successfully fetched and parsed data for Event ID: 837\n",
      "\n",
      "Attempting to fetch data for Event ID: 838 (Formula 1 stc Saudi Arabian Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=838\n",
      "Successfully fetched and parsed data for Event ID: 838\n",
      "\n",
      "Attempting to fetch data for Event ID: 839 (Formula 1 Rolex Australian Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=839\n",
      "Successfully fetched and parsed data for Event ID: 839\n",
      "\n",
      "Attempting to fetch data for Event ID: 841 (Formula 1 Azerbaijan Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=841\n",
      "Successfully fetched and parsed data for Event ID: 841\n",
      "\n",
      "Attempting to fetch data for Event ID: 842 (Formula 1 Crypto.com Miami Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=842\n",
      "Successfully fetched and parsed data for Event ID: 842\n",
      "\n",
      "Attempting to fetch data for Event ID: 844 (Formula 1 Grand Prix de Monaco 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=844\n",
      "Successfully fetched and parsed data for Event ID: 844\n",
      "\n",
      "Attempting to fetch data for Event ID: 845 (Formula 1 Pirelli Gran Premio De España 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=845\n",
      "Successfully fetched and parsed data for Event ID: 845\n",
      "\n",
      "Attempting to fetch data for Event ID: 847 (Formula 1 Grand Prix Du Canada 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=847\n",
      "Successfully fetched and parsed data for Event ID: 847\n",
      "\n",
      "Attempting to fetch data for Event ID: 848 (Formula 1 Grosser Preis von Österreich 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=848\n",
      "Successfully fetched and parsed data for Event ID: 848\n",
      "\n",
      "Attempting to fetch data for Event ID: 846 (Formula 1 Aramco British Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=846\n",
      "Successfully fetched and parsed data for Event ID: 846\n",
      "\n",
      "Attempting to fetch data for Event ID: 849 (Formula 1 Hungarian Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=849\n",
      "Successfully fetched and parsed data for Event ID: 849\n",
      "\n",
      "Attempting to fetch data for Event ID: 850 (Formula 1 Belgian Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=850\n",
      "Successfully fetched and parsed data for Event ID: 850\n",
      "\n",
      "Attempting to fetch data for Event ID: 851 (Formula 1 Heineken Dutch Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=851\n",
      "Successfully fetched and parsed data for Event ID: 851\n",
      "\n",
      "Attempting to fetch data for Event ID: 852 (Formula 1 Gran Premio D’italia 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=852\n",
      "Successfully fetched and parsed data for Event ID: 852\n",
      "\n",
      "Attempting to fetch data for Event ID: 853 (Formula 1 Singapore Airlines Singapore Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=853\n",
      "Successfully fetched and parsed data for Event ID: 853\n",
      "\n",
      "Attempting to fetch data for Event ID: 854 (Formula 1 Lenovo Japanese Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=854\n",
      "Successfully fetched and parsed data for Event ID: 854\n",
      "\n",
      "Attempting to fetch data for Event ID: 855 (Formula 1 Qatar Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=855\n",
      "Successfully fetched and parsed data for Event ID: 855\n",
      "\n",
      "Attempting to fetch data for Event ID: 856 (Formula 1 Lenovo United States Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=856\n",
      "Successfully fetched and parsed data for Event ID: 856\n",
      "\n",
      "Attempting to fetch data for Event ID: 857 (Formula 1 Gran Premio De La Ciudad De México 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=857\n",
      "Successfully fetched and parsed data for Event ID: 857\n",
      "\n",
      "Attempting to fetch data for Event ID: 858 (Formula 1 Rolex Grande Prêmio De São Paulo 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=858\n",
      "Successfully fetched and parsed data for Event ID: 858\n",
      "\n",
      "Attempting to fetch data for Event ID: 859 (Formula 1 Heineken Silver Las Vegas Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=859\n",
      "Successfully fetched and parsed data for Event ID: 859\n",
      "\n",
      "Attempting to fetch data for Event ID: 860 (Formula 1 Etihad Airways Abu Dhabi Grand Prix 2023)\n",
      "URL: https://inmotion.dhl/api/f1-award-element-data/6282?event=860\n",
      "Successfully fetched and parsed data for Event ID: 860\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully fetched data for 22 events.\n",
      "Failed to fetch data for 0 events.\n",
      "\n",
      "Processing event: Formula 1 Gulf Air Bahrain Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Bahrain Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 stc Saudi Arabian Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Saudi Arabian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Rolex Australian Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Australian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Azerbaijan Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Azerbaijan Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Crypto.com Miami Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Miami Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grand Prix de Monaco 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Monaco Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Pirelli Gran Premio De España 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Spanish Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grand Prix Du Canada 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Canadian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Grosser Preis von Österreich 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Austrian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Aramco British Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/British Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Hungarian Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Hungarian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Belgian Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Belgian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Heineken Dutch Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Dutch Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Gran Premio D’italia 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Italian Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Singapore Airlines Singapore Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Singapore Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Lenovo Japanese Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Japanese Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Qatar Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Qatar Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Lenovo United States Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/United States Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Gran Premio De La Ciudad De México 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Mexico City Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Rolex Grande Prêmio De São Paulo 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/São Paulo Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Heineken Silver Las Vegas Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Las Vegas Grand Prix.json\n",
      "\n",
      "Processing event: Formula 1 Etihad Airways Abu Dhabi Grand Prix 2023\n",
      "Found HTML table string. Attempting to parse...\n",
      "Successfully parsed HTML table into DataFrame.\n",
      "Saved data to 2023/Abu Dhabi Grand Prix.json\n",
      "\n",
      "--- JSON Export Complete ---\n",
      "Successfully saved 22 event data files to the '2023' directory.\n",
      "Files saved:\n",
      "  - 2023/Bahrain Grand Prix.json\n",
      "  - 2023/Saudi Arabian Grand Prix.json\n",
      "  - 2023/Australian Grand Prix.json\n",
      "  - 2023/Azerbaijan Grand Prix.json\n",
      "  - 2023/Miami Grand Prix.json\n",
      "  - 2023/Monaco Grand Prix.json\n",
      "  - 2023/Spanish Grand Prix.json\n",
      "  - 2023/Canadian Grand Prix.json\n",
      "  - 2023/Austrian Grand Prix.json\n",
      "  - 2023/British Grand Prix.json\n",
      "  - 2023/Hungarian Grand Prix.json\n",
      "  - 2023/Belgian Grand Prix.json\n",
      "  - 2023/Dutch Grand Prix.json\n",
      "  - 2023/Italian Grand Prix.json\n",
      "  - 2023/Singapore Grand Prix.json\n",
      "  - 2023/Japanese Grand Prix.json\n",
      "  - 2023/Qatar Grand Prix.json\n",
      "  - 2023/United States Grand Prix.json\n",
      "  - 2023/Mexico City Grand Prix.json\n",
      "  - 2023/São Paulo Grand Prix.json\n",
      "  - 2023/Las Vegas Grand Prix.json\n",
      "  - 2023/Abu Dhabi Grand Prix.json\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Configuration constants\n",
    "DEFAULT_TIMEOUT = 10\n",
    "# URLs by year\n",
    "F1_URLS = {\n",
    "    2023: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6284\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6282\",\n",
    "    },\n",
    "    2024: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6276\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6273\",\n",
    "    },\n",
    "    2025: {\n",
    "        \"EVENT_DATA_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6367\",\n",
    "        \"EVENT_SPECIFIC_URL\": \"https://inmotion.dhl/api/f1-award-element-data/6365\",\n",
    "    },\n",
    "}\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DELAY_BETWEEN_REQUESTS = 0.5  # seconds\n",
    "\n",
    "# F1 race names by year\n",
    "F1_RACES = {\n",
    "    2025: {\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"CHINESE\": \"Chinese Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"EMILIA-ROMAGNA\": \"Emilia Romagna Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"MÉXICO\": \"Mexico City Grand Prix\",\n",
    "        \"SÃO PAULO\": \"São Paulo Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "    },\n",
    "    2024: {\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"CHINESE\": \"Chinese Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"EMILIA-ROMAGNA\": \"Emilia Romagna Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"AUSTRIAN\": \"Austrian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"MEXICO\": \"Mexico City Grand Prix\",\n",
    "        \"SÃO PAOLO\": \"São Paulo Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "    },\n",
    "    2023: {\n",
    "        \"ABU DHABI\": \"Abu Dhabi Grand Prix\",\n",
    "        \"LAS VEGAS\": \"Las Vegas Grand Prix\",\n",
    "        \"SÃO PAULO\": \"São Paulo Grand Prix\",\n",
    "        \"MÉXICO\": \"Mexico City Grand Prix\",\n",
    "        \"UNITED STATES\": \"United States Grand Prix\",\n",
    "        \"QATAR\": \"Qatar Grand Prix\",\n",
    "        \"JAPANESE\": \"Japanese Grand Prix\",\n",
    "        \"SINGAPORE\": \"Singapore Grand Prix\",\n",
    "        \"ITALIA\": \"Italian Grand Prix\",\n",
    "        \"DUTCH\": \"Dutch Grand Prix\",\n",
    "        \"BELGIAN\": \"Belgian Grand Prix\",\n",
    "        \"HUNGARIAN\": \"Hungarian Grand Prix\",\n",
    "        \"BRITISH\": \"British Grand Prix\",\n",
    "        \"ÖSTERREICH\": \"Austrian Grand Prix\",\n",
    "        \"CANADA\": \"Canadian Grand Prix\",\n",
    "        \"ESPAÑA\": \"Spanish Grand Prix\",\n",
    "        \"MONACO\": \"Monaco Grand Prix\",\n",
    "        \"MIAMI\": \"Miami Grand Prix\",\n",
    "        \"AZERBAIJAN\": \"Azerbaijan Grand Prix\",\n",
    "        \"AUSTRALIAN\": \"Australian Grand Prix\",\n",
    "        \"SAUDI ARABIAN\": \"Saudi Arabian Grand Prix\",\n",
    "        \"BAHRAIN\": \"Bahrain Grand Prix\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class F1DataFetcher:\n",
    "    \"\"\"Class for fetching and processing Formula 1 data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, year: int = 2025, timeout: int = DEFAULT_TIMEOUT, headers: Dict = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the F1DataFetcher.\n",
    "\n",
    "        Args:\n",
    "            year: Year for which to fetch F1 data\n",
    "            timeout: Request timeout in seconds\n",
    "            headers: HTTP headers for requests\n",
    "        \"\"\"\n",
    "        self.year = year\n",
    "        self.timeout = timeout\n",
    "        self.headers = headers or DEFAULT_HEADERS\n",
    "        self.event_data_cache = {}\n",
    "        self.event_specific_data_cache = {}\n",
    "\n",
    "        # Set URLs based on year\n",
    "        self.set_year(year)\n",
    "\n",
    "    def set_year(self, year: int) -> None:\n",
    "        \"\"\"\n",
    "        Set the year and update URLs accordingly.\n",
    "\n",
    "        Args:\n",
    "            year: Year for which to fetch F1 data\n",
    "        \"\"\"\n",
    "        self.year = year\n",
    "        if year in F1_URLS:\n",
    "            self.event_data_url = F1_URLS[year][\"EVENT_DATA_URL\"]\n",
    "            self.event_specific_url = F1_URLS[year][\"EVENT_SPECIFIC_URL\"]\n",
    "        else:\n",
    "            # Default to latest year if requested year is not available\n",
    "            latest_year = max(F1_URLS.keys())\n",
    "            print(\n",
    "                f\"Warning: Data for year {year} not available. Using {latest_year} instead.\"\n",
    "            )\n",
    "            self.year = latest_year\n",
    "            self.event_data_url = F1_URLS[latest_year][\"EVENT_DATA_URL\"]\n",
    "            self.event_specific_url = F1_URLS[latest_year][\"EVENT_SPECIFIC_URL\"]\n",
    "\n",
    "    def fetch_events_data(self, url: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Fetch events data from the specified URL.\n",
    "\n",
    "        Args:\n",
    "            url: URL to fetch data from (defaults to year-specific URL)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing events data\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "            json.JSONDecodeError: If response is not valid JSON\n",
    "        \"\"\"\n",
    "        if url is None:\n",
    "            url = self.event_data_url\n",
    "\n",
    "        print(f\"Attempting to fetch data from: {url}\")\n",
    "\n",
    "        try:\n",
    "            response = self._make_request(url)\n",
    "            parsed_data = response.json()\n",
    "\n",
    "            # Extract events list from the nested structure\n",
    "            data_section = parsed_data.get(\"data\", {})\n",
    "            chart_section = data_section.get(\"chart\", {})\n",
    "            events_data = chart_section.get(\"events\", [])\n",
    "\n",
    "            if events_data:\n",
    "                print(\"\\nSuccessfully extracted events data\")\n",
    "                return events_data\n",
    "            else:\n",
    "                print(\"\\nError: Could not find the 'events' data at the expected path\")\n",
    "                return []\n",
    "\n",
    "        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
    "            print(f\"\\nError fetching events data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_event_specific_data(\n",
    "        self, events_data: List[Dict], base_url: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch specific data for each event.\n",
    "\n",
    "        Args:\n",
    "            events_data: List of event dictionaries\n",
    "            base_url: Base URL for event-specific data (defaults to year-specific URL)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping event IDs to their specific data\n",
    "        \"\"\"\n",
    "        if base_url is None:\n",
    "            base_url = self.event_specific_url\n",
    "\n",
    "        all_event_specific_data = {}\n",
    "\n",
    "        print(f\"Found {len(events_data)} events to process.\")\n",
    "\n",
    "        for event in events_data:\n",
    "            event_id = event.get(\"id\")\n",
    "            event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "            if not event_id:\n",
    "                print(f\"Warning: Skipping event with missing ID: {event_title}\")\n",
    "                continue\n",
    "\n",
    "            # Check cache first\n",
    "            if event_id in self.event_specific_data_cache:\n",
    "                all_event_specific_data[event_id] = self.event_specific_data_cache[\n",
    "                    event_id\n",
    "                ]\n",
    "                print(f\"Using cached data for Event ID: {event_id} ({event_title})\")\n",
    "                continue\n",
    "\n",
    "            specific_url = f\"{base_url}?event={event_id}\"\n",
    "            print(\n",
    "                f\"\\nAttempting to fetch data for Event ID: {event_id} ({event_title})\"\n",
    "            )\n",
    "            print(f\"URL: {specific_url}\")\n",
    "\n",
    "            try:\n",
    "                response = self._make_request(specific_url)\n",
    "                event_specific_data = response.json()\n",
    "\n",
    "                # Cache the result\n",
    "                self.event_specific_data_cache[event_id] = event_specific_data\n",
    "                all_event_specific_data[event_id] = event_specific_data\n",
    "\n",
    "                print(f\"Successfully fetched and parsed data for Event ID: {event_id}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                error_info = {\"error\": str(e)}\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error during request for Event ID: {event_id}: {e}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                error_info = {\n",
    "                    \"error\": \"JSONDecodeError\",\n",
    "                    \"response_text\": (\n",
    "                        response.text[:500]\n",
    "                        if hasattr(response, \"text\")\n",
    "                        else \"No response text\"\n",
    "                    ),\n",
    "                }\n",
    "                all_event_specific_data[event_id] = error_info\n",
    "                print(f\"Error: Failed to decode JSON for Event ID: {event_id}\")\n",
    "\n",
    "            # Add delay between requests\n",
    "            if DELAY_BETWEEN_REQUESTS > 0:\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        # Print summary\n",
    "        self._print_fetch_summary(events_data, all_event_specific_data)\n",
    "        return all_event_specific_data\n",
    "\n",
    "    def _make_request(self, url: str) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Make an HTTP request with error handling.\n",
    "\n",
    "        Args:\n",
    "            url: URL to request\n",
    "\n",
    "        Returns:\n",
    "            Response object\n",
    "\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If request fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Error: The request to {url} timed out.\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Error during request to {url}\")\n",
    "            raise\n",
    "\n",
    "    def _print_fetch_summary(\n",
    "        self, events_data: List[Dict], all_event_specific_data: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Print a summary of the fetch operation.\"\"\"\n",
    "        print(\"\\n--- Processing Complete ---\")\n",
    "        successful_fetches = sum(\n",
    "            1\n",
    "            for data in all_event_specific_data.values()\n",
    "            if isinstance(data, dict) and \"error\" not in data\n",
    "        )\n",
    "        failed_fetches = len(events_data) - successful_fetches\n",
    "        print(f\"Successfully fetched data for {successful_fetches} events.\")\n",
    "        print(f\"Failed to fetch data for {failed_fetches} events.\")\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Class for processing F1 data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def html_table_to_dataframe(event_json_data: Dict) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract HTML table from event JSON data and convert to DataFrame.\n",
    "\n",
    "        Args:\n",
    "            event_json_data: JSON data dictionary for a specific event\n",
    "\n",
    "        Returns:\n",
    "            DataFrame created from HTML table, or None if extraction fails\n",
    "        \"\"\"\n",
    "        if not isinstance(event_json_data, dict):\n",
    "            print(\"Error: Input must be a dictionary.\")\n",
    "            return None\n",
    "\n",
    "        # Extract HTML table string\n",
    "        html_table_str = event_json_data.get(\"htmlList\", {}).get(\"table\")\n",
    "\n",
    "        if not html_table_str:\n",
    "            print(\n",
    "                \"Error: Could not find 'htmlList' -> 'table' in the provided JSON data or it's empty.\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        if not isinstance(html_table_str, str):\n",
    "            print(\"Error: The value at ['htmlList']['table'] is not a string.\")\n",
    "            return None\n",
    "\n",
    "        print(\"Found HTML table string. Attempting to parse...\")\n",
    "        try:\n",
    "            # Parse HTML table into DataFrame\n",
    "            list_of_dfs = pd.read_html(io.StringIO(html_table_str))\n",
    "\n",
    "            if list_of_dfs:\n",
    "                print(\"Successfully parsed HTML table into DataFrame.\")\n",
    "                return list_of_dfs[0]\n",
    "            else:\n",
    "                print(\n",
    "                    \"Warning: No tables found by pd.read_html, although HTML string was present.\"\n",
    "                )\n",
    "                return None\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error parsing HTML with pandas (ValueError): {ve}\")\n",
    "            print(\"Check if the HTML string actually contains a <table> tag.\")\n",
    "            return None\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"Error: The 'lxml' library might be required by pd.read_html. Please install it (`pip install lxml`).\"\n",
    "            )\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during HTML parsing: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dataframe_to_json(\n",
    "        df: pd.DataFrame, event_title: str, year: int, output_dir: str = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save DataFrame to a JSON file with a standardized filename.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            event_title: Title of the event (e.g., \"FORMULA 1 LOUIS VUITTON AUSTRALIAN GRAND PRIX 2025\")\n",
    "            year: Year of the event\n",
    "            output_dir: Directory to save the JSON file (defaults to year folder)\n",
    "\n",
    "        Returns:\n",
    "            Path to the saved JSON file\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            print(f\"Error: Cannot save None DataFrame for {event_title}\")\n",
    "            return \"\"\n",
    "\n",
    "        # If no output directory is specified, use the year as the directory name\n",
    "        if output_dir is None:\n",
    "            output_dir = str(year)\n",
    "\n",
    "        # Find the proper race name from the event title\n",
    "        race_name = None\n",
    "\n",
    "        # First, try to match with our predefined race names for the specific year\n",
    "        if year in F1_RACES:\n",
    "            year_races = F1_RACES[year]\n",
    "            for key, proper_name in year_races.items():\n",
    "                if key in event_title.upper():\n",
    "                    race_name = proper_name\n",
    "                    break\n",
    "        else:\n",
    "            # If year not in F1_RACES, use the latest year's race names\n",
    "            latest_year = max(F1_RACES.keys())\n",
    "            year_races = F1_RACES[latest_year]\n",
    "            for key, proper_name in year_races.items():\n",
    "                if key in event_title.upper():\n",
    "                    race_name = proper_name\n",
    "                    break\n",
    "\n",
    "        # If no match found, try to extract from the title\n",
    "        if race_name is None:\n",
    "            # Try to extract Grand Prix name using regex\n",
    "            match = re.search(\n",
    "                r\"([A-Z]+(?:\\s+[A-Z]+)*)\\s+GRAND\\s+PRIX\", event_title, re.IGNORECASE\n",
    "            )\n",
    "            if match:\n",
    "                location = match.group(1).strip()\n",
    "                race_name = f\"{location} Grand Prix\"\n",
    "            else:\n",
    "                # Fallback to a generic name with the event ID\n",
    "                race_name = \"Unknown Grand Prix\"\n",
    "                print(f\"Warning: Could not determine race name for: {event_title}\")\n",
    "\n",
    "        # Clean up the filename\n",
    "        filename = race_name\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save the DataFrame to a JSON file\n",
    "        file_path = os.path.join(output_dir, f\"{filename}.json\")\n",
    "        df.to_json(file_path, orient=\"records\", indent=4)\n",
    "\n",
    "        print(f\"Saved data to {file_path}\")\n",
    "        return file_path\n",
    "\n",
    "\n",
    "def main(year: int = 2025):\n",
    "    \"\"\"\n",
    "    Main function to fetch and process F1 data for a specific year.\n",
    "\n",
    "    Args:\n",
    "        year: Year to fetch data for (default: 2025)\n",
    "    \"\"\"\n",
    "    print(f\"Fetching F1 data for year: {year}\")\n",
    "\n",
    "    # Initialize the data fetcher with the specified year\n",
    "    fetcher = F1DataFetcher(year=year)\n",
    "\n",
    "    # Fetch events data\n",
    "    events_data = fetcher.fetch_events_data()\n",
    "    if not events_data:\n",
    "        print(\"No events data found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Fetch specific data for each event\n",
    "    all_event_specific_data = fetcher.fetch_event_specific_data(events_data)\n",
    "\n",
    "    # Create a DataProcessor instance\n",
    "    processor = DataProcessor()\n",
    "\n",
    "    # Process each event and save to JSON\n",
    "    saved_files = []\n",
    "    output_dir = str(year)  # Use year as directory name\n",
    "\n",
    "    for event in events_data:\n",
    "        event_id = event.get(\"id\")\n",
    "        event_title = event.get(\"title\", \"Unknown Title\")\n",
    "\n",
    "        if not event_id or event_id not in all_event_specific_data:\n",
    "            print(f\"Skipping event {event_title}: No data available\")\n",
    "            continue\n",
    "\n",
    "        event_data = all_event_specific_data[event_id]\n",
    "\n",
    "        # Check if there was an error fetching this event's data\n",
    "        if isinstance(event_data, dict) and \"error\" in event_data:\n",
    "            print(\n",
    "                f\"Skipping event {event_title}: Error in data - {event_data.get('error')}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Convert HTML table to DataFrame\n",
    "        print(f\"\\nProcessing event: {event_title}\")\n",
    "        event_dataframe = processor.html_table_to_dataframe(event_data)\n",
    "\n",
    "        if event_dataframe is not None:\n",
    "            # Save DataFrame to JSON\n",
    "            file_path = processor.save_dataframe_to_json(\n",
    "                event_dataframe, event_title, year, output_dir\n",
    "            )\n",
    "\n",
    "            if file_path:\n",
    "                saved_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Failed to create DataFrame for {event_title}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n--- JSON Export Complete ---\")\n",
    "    print(\n",
    "        f\"Successfully saved {len(saved_files)} event data files to the '{output_dir}' directory.\"\n",
    "    )\n",
    "    if saved_files:\n",
    "        print(\"Files saved:\")\n",
    "        for file_path in saved_files:\n",
    "            print(f\"  - {file_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 43/285 [01:05<06:08,  1.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid IDs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 36\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Iterate through IDs with progress bar (from 6284 down to 6000)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id_number \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6284\u001b[39m, \u001b[38;5;241m5999\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):  \u001b[38;5;66;03m# 6284 down to 6000 inclusive\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     has_data, data \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_data:\n\u001b[1;32m     39\u001b[0m         valid_ids\u001b[38;5;241m.\u001b[39mappend(id_number)\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mcheck_url\u001b[0;34m(id_number)\u001b[0m\n\u001b[1;32m      8\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://inmotion.dhl/api/f1-award-element-data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Check if response is successful\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Try to parse JSON\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/urllib3/connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 741\u001b[0m     sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    761\u001b[0m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/urllib3/connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/urllib3/util/ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    456\u001b[0m         context\u001b[38;5;241m.\u001b[39mload_cert_chain(certfile, keyfile, key_password)\n\u001b[1;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[0;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/site-packages/urllib3/util/ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    501\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1039\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.9/lib/python3.12/ssl.py:1319\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "def check_url(id_number):\n",
    "    \"\"\"Check if the URL with the given ID returns data\"\"\"\n",
    "    url = f\"https://inmotion.dhl/api/f1-award-element-data/{id_number}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "\n",
    "        # Check if response is successful\n",
    "        if response.status_code == 200:\n",
    "            # Try to parse JSON\n",
    "            try:\n",
    "                data = response.json()\n",
    "                # Check if data is not empty (this might need adjustment based on actual response structure)\n",
    "                if data and not (isinstance(data, dict) and len(data) == 0):\n",
    "                    return True, data\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        return False, None\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False, None\n",
    "\n",
    "def main():\n",
    "    # List to store IDs with data\n",
    "    valid_ids = []\n",
    "\n",
    "    # File to save detailed results\n",
    "    with open(\"valid_dhl_ids.txt\", \"w\") as file:\n",
    "        file.write(\"IDs that return data from https://inmotion.dhl/api/f1-award-element-data/:\\n\\n\")\n",
    "\n",
    "        # Iterate through IDs with progress bar (from 6284 down to 6000)\n",
    "        for id_number in tqdm(range(6284, 5999, -1)):  # 6284 down to 6000 inclusive\n",
    "            has_data, data = check_url(id_number)\n",
    "\n",
    "            if has_data:\n",
    "                valid_ids.append(id_number)\n",
    "                file.write(f\"ID: {id_number}\\n\")\n",
    "                file.write(f\"Data: {json.dumps(data, indent=2)}\\n\\n\")\n",
    "\n",
    "            # Rate limiting - sleep to avoid overwhelming the server\n",
    "            time.sleep(0.2)  # 200ms delay between requests\n",
    "\n",
    "    # Save summary of valid IDs\n",
    "    with open(\"valid_dhl_ids_summary.txt\", \"w\") as file:\n",
    "        file.write(\"Summary of IDs that return data:\\n\")\n",
    "        file.write(f\"Total valid IDs: {len(valid_ids)}\\n\\n\")\n",
    "        file.write(\"Valid IDs: \" + \", \".join(map(str, valid_ids)))\n",
    "\n",
    "    print(f\"Scan complete. Found {len(valid_ids)} valid IDs.\")\n",
    "    print(f\"Valid IDs: {valid_ids}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [00:22<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan complete. Found 12 valid IDs.\n",
      "Valid IDs: [6268, 6269, 6271, 6273, 6275, 6276, 6279, 6280, 6281, 6282, 6283, 6284]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_url(id_number):\n",
    "    \"\"\"Check if the URL with the given ID returns data\"\"\"\n",
    "    url = f\"https://inmotion.dhl/api/f1-award-element-data/{id_number}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "\n",
    "        # Check if response is successful\n",
    "        if response.status_code == 200:\n",
    "            # Try to parse JSON\n",
    "            try:\n",
    "                data = response.json()\n",
    "                # Check if data is not empty\n",
    "                if data and not (isinstance(data, dict) and len(data) == 0):\n",
    "                    return id_number, True, data\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        return id_number, False, None\n",
    "    except requests.exceptions.RequestException:\n",
    "        return id_number, False, None\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    start_id = 6284\n",
    "    end_id = 6000\n",
    "    max_workers = 20  # Number of parallel requests\n",
    "\n",
    "    # Create a list of IDs to check\n",
    "    ids_to_check = list(range(start_id, end_id - 1, -1))\n",
    "    valid_results = []\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel requests\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks and get future objects\n",
    "        future_to_id = {executor.submit(check_url, id_num): id_num for id_num in ids_to_check}\n",
    "\n",
    "        # Process results as they complete with a progress bar\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_id), total=len(ids_to_check)):\n",
    "            id_num, has_data, data = future.result()\n",
    "            if has_data:\n",
    "                valid_results.append((id_num, data))\n",
    "\n",
    "    # Sort results by ID for consistent output\n",
    "    valid_results.sort(key=lambda x: x[0])\n",
    "    valid_ids = [id_num for id_num, _ in valid_results]\n",
    "\n",
    "    # Save detailed results\n",
    "    with open(\"valid_dhl_ids.txt\", \"w\") as file:\n",
    "        file.write(\"IDs that return data from https://inmotion.dhl/api/f1-award-element-data/:\\n\\n\")\n",
    "        for id_num, data in valid_results:\n",
    "            file.write(f\"ID: {id_num}\\n\")\n",
    "            file.write(f\"Data: {json.dumps(data, indent=2)}\\n\\n\")\n",
    "\n",
    "    # Save summary of valid IDs\n",
    "    with open(\"valid_dhl_ids_summary.txt\", \"w\") as file:\n",
    "        file.write(\"Summary of IDs that return data:\\n\")\n",
    "        file.write(f\"Total valid IDs: {len(valid_ids)}\\n\\n\")\n",
    "        file.write(\"Valid IDs: \" + \", \".join(map(str, valid_ids)))\n",
    "\n",
    "    print(f\"Scan complete. Found {len(valid_ids)} valid IDs.\")\n",
    "    print(f\"Valid IDs: {valid_ids}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3716/3716 [00:24<00:00, 151.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan complete. Found 0 valid IDs.\n",
      "Valid IDs: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_url(id_number):\n",
    "    \"\"\"Check if the URL with the given ID returns data\"\"\"\n",
    "    url = f\"https://inmotion.dhl/api/f1-award-element-data/{id_number}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "\n",
    "        # Check if response is successful\n",
    "        if response.status_code == 200:\n",
    "            # Try to parse JSON\n",
    "            try:\n",
    "                data = response.json()\n",
    "                # Check if data is not empty\n",
    "                if data and not (isinstance(data, dict) and len(data) == 0):\n",
    "                    return id_number, True, data\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        return id_number, False, None\n",
    "    except requests.exceptions.RequestException:\n",
    "        return id_number, False, None\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    start_id = 9999\n",
    "    end_id = 6284\n",
    "    max_workers = 200  # Number of parallel requests\n",
    "\n",
    "    # Create a list of IDs to check\n",
    "    ids_to_check = list(range(start_id, end_id - 1, -1))\n",
    "    valid_results = []\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel requests\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks and get future objects\n",
    "        future_to_id = {executor.submit(check_url, id_num): id_num for id_num in ids_to_check}\n",
    "\n",
    "        # Process results as they complete with a progress bar\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_id), total=len(ids_to_check)):\n",
    "            id_num, has_data, data = future.result()\n",
    "            if has_data:\n",
    "                valid_results.append((id_num, data))\n",
    "\n",
    "    # Sort results by ID for consistent output\n",
    "    valid_results.sort(key=lambda x: x[0])\n",
    "    valid_ids = [id_num for id_num, _ in valid_results]\n",
    "\n",
    "    # Save detailed results\n",
    "    with open(\"valid_dhl_ids.txt\", \"w\") as file:\n",
    "        file.write(\"IDs that return data from https://inmotion.dhl/api/f1-award-element-data/:\\n\\n\")\n",
    "        for id_num, data in valid_results:\n",
    "            file.write(f\"ID: {id_num}\\n\")\n",
    "            file.write(f\"Data: {json.dumps(data, indent=2)}\\n\\n\")\n",
    "\n",
    "    # Save summary of valid IDs\n",
    "    with open(\"valid_dhl_ids_summary.txt\", \"w\") as file:\n",
    "        file.write(\"Summary of IDs that return data:\\n\")\n",
    "        file.write(f\"Total valid IDs: {len(valid_ids)}\\n\\n\")\n",
    "        file.write(\"Valid IDs: \" + \", \".join(map(str, valid_ids)))\n",
    "\n",
    "    print(f\"Scan complete. Found {len(valid_ids)} valid IDs.\")\n",
    "    print(f\"Valid IDs: {valid_ids}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
